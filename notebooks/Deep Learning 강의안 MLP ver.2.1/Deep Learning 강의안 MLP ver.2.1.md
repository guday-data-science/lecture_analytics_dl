
# Analysis ê³¼ì • - Deep Learning

## ê°•ì˜ ê°œìš”


* <font size='4'>ì™œ ë”¥ëŸ¬ë‹ì¸ê°€?</font>  
  - í†µê³„, ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ì˜ ì°¨ì´
  - ë”¥ëŸ¬ë‹ìœ¼ë¡œ í’€ ìˆ˜ ìˆëŠ” ë¬¸ì œ


* <font size='4'>ë”¥ëŸ¬ë‹ì˜ ì›ë¦¬</font>
  - í•™ìŠµì´ë€?
  - ì–´ë–»ê²Œ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ”ê°€


* <font size='4'>ë”¥ëŸ¬ë‹ì˜ êµ¬ì¡°</font>
  - <b>ì…ë ¥ $\rightarrow$ ëª¨ë¸ $\rightarrow$ ì¶œë ¥</b>
  - <b>Layer</b> : ëª¨ë¸ì˜ êµ¬ì„±ë‹¨ìœ„
  - <b>Activation</b> : ë¹„ì„ í˜• ëª¨ë¸ë¡œ ê°€ëŠ” ê¸¸
  - <b>Bias</b> : ì˜ì ì¡°ì ˆ



* <font size='4'>í•™ìŠµìœ¼ë¡œ ëŒì•„ì™€ì„œ</font>
  - <b>Train Set</b> : ê¸°ì¶œ ë¬¸ì œì€í–‰
  - <b>Batch</b> : ëª¨ì˜ê³ ì‚¬
  - <b>Epoch</b> : 1íšŒë…
  - <b>Step</b> : ëª¨ì˜ê³ ì‚¬ ì‘ì‹œíšŸìˆ˜
  - <b>Loss</b> : ì‹œí—˜ì ìˆ˜
  - <b>Gradient</b> : ê³µë¶€ë°©í–¥
  - <b>Optimization</b> : ê³µë¶€ë°©ë²•
  - <b>Backpropagation</b> : ì˜¤ë‹µë…¸íŠ¸, ë³µìŠµ


* <font size='4'>ëª¨ë¸ í‰ê°€</font>
  - <b>Accuracy</b> : ë¬¸ì œì€í–‰
  - <b>Batch</b> : ëª¨ì˜ê³ ì‚¬
  - <b>Epoch</b> : 1íšŒë…
  - <b>Step</b> : ëª¨ì˜ê³ ì‚¬ ì‘ì‹œíšŸìˆ˜


* <font size='4'>ë”¥ëŸ¬ë‹ì˜ ë°œì „</font>
  - <font size='3'><b>MLP</b></font> : ë‹¤ ì—®ì–´ë³´ì! ì£„ë‹¤ ì—°ê²°ëœ ì‹ ê²½ë§ (Fully Connected), ë¹¡ë¹¡í•œ ì‹ ê²½ë§ (Dense)
  - <font size='3'><b>CNN</b></font> : Imageë¥¼ ë¶€ë¶„ìœ¼ë¡œ ìª¼ê°œì„œ ì¡°ê¸ˆì”© ë³´ì!
  - <font size='3'><b>RNN</b></font> : Data ìˆœì„œì— ì˜ë¯¸ê°€ ìˆë‹¤ë©´? ê³¼ê±°ë¥¼ ê¸°ì–µí•˜ì  
    - <font size='2'><b>Advanced RNN</b></font> : ê¸°ì–µë ¥ í–¥ìƒ  


### ê°•ì˜ ì§„í–‰ ê³¼ì •

* ê¸°ë³¸
  1. ì‹¤ìŠµ ì§„í–‰ ( Code Review & ì‘ì„± )
    - `keras`
  2. ê°œë… ì„¤ëª…
  3. ê°œë… ì´í•´ ( Code Review )
    - `numpy`


* ì‹¬í™”
  1. ê°œë… ë°˜ë³µ
  1. ì‹¤ìŠµ ì§„í–‰
    - `tensorflow`


---
## ë”¥ëŸ¬ë‹ ê°œìš”

### ì™œ ë”¥ëŸ¬ë‹ì¸ê°€?

<br>
<font size='5'><center><i>"ê¸°ê³„í•™ìŠµ, ì‚¬ëŒì´ íŒë‹¨í•˜ê¸° í˜ë“  ê·œì¹™ë“¤ì„ ë°ì´í„° ì–‘ìœ¼ë¡œ ê·¹ë³µí•´ ë³´ì!"</i></center></font>
<br>

---
* <font size='3'>í†µê³„ ê¸°ë°˜ ë°©ë²•ë¡ </font>  
  - ì›ë¦¬: ëª¨ìˆ˜ê¸°ë°˜ ì ‘ê·¼ë²•ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì •ê·œë¶„í¬ ë˜ëŠ” ê¸°íƒ€ ê²€ì¦ëœ íŠ¹ì§•ì„ ê°–ëŠ” ë¶„í¬ë¡œ ì„¤ëª….  
  (ëŒ€ìˆ˜ì˜ ë²•ì¹™ì— ê¸°ë°˜í•œ ì •ê·œë¶„í¬ ê°€ì •, ì¤‘ì‹¬ê·¹í•œì •ë¦¬ë¥¼ ê¸°ë°˜í•œ í‘œë³¸í‰ê· ì˜ ì •ê·œë¶„í¬ ê·¼ì‚¬ ê°€ì • ë“±)  
  - ëª©ì : ëª¨í˜•ì˜ ë³µì¡ì„± ë³´ë‹¤ëŠ” ë‹¨ìˆœì„±, ì„¤ëª…ê°€ëŠ¥ì„±ì— ë¬´ê²Œë¥¼ ë‘ê³  í‘œë³¸ì¶”ì¶œ ê³¼ì •ì—ì„œì˜ ê°€ì„¤ê²€ì •ì´ ì£¼ ëª©ì .  
  - í‰ê°€: p-value ë“±ì˜ ì§€í‘œë¡œ ê·€ë¬´ê°€ì„¤ ê¸°ê° ì—¬ë¶€ë¡œ í‰ê°€.
  <br>
  <img src="images/normal_dist_example.png" >


---
* <font size='3'>M/L ê¸°ë°˜ ë°©ë²•ë¡ </font>  
  - ì›ë¦¬: ì„ í˜•ê·¼ì‚¬ì™€ ì •ê·œë¶„í¬ ê°€ì •ì„ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í•˜ëŠ” ì„ í˜•ë°©ì •ì‹ì„ ë§Œë“¤ì–´ ìƒˆë¡œìš´ Inputì— ëŒ€ì‘í•˜ëŠ” ê²°ê³¼ë¥¼ ì˜ˆì¸¡.
  - í•™ìŠµ: OLS, GLS ë“± ìµœì†Œì œê³±í•© ë˜ëŠ” ìµœëŒ€ê°€ëŠ¥ë„ì¶”ì •, ë¶„ì‚°ê°ì†Œê¸°ë²• ë“±ì˜ ë‹¤ì†Œ í•™ìŠµì´ ë¹ ë¥¸ ê¸°ë²•ë“¤ì´ ì‚¬ìš©.
  - ëª©ì : ë³€ìˆ˜ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ë‹¤ì†Œ ë³µì¡í•˜ê³  ì„¤ëª…ë ¥ì´ ë–¨ì–´ì§€ë”ë¼ë„ ì¢‹ì€ ì˜ˆì¸¡ë ¥ì„ ê°–ëŠ” ëª¨ë¸ì„ ì§€í–¥í•˜ë©°, ê²½ìš°ì— ë”°ë¼ ì£¼ìš” ì›ì¸ë³€ìˆ˜ ì„ ë³„ë„ ê°€ëŠ¥.
  - í‰ê°€: ì˜¤ì°¨ì˜ í¬ê¸°, ì •í™•ë„(Accuracy) ë° Precision/Recall ë“± ì˜ˆì¸¡ê²°ê³¼ì— ëŒ€í•œ ì§€í‘œë“¤ì´ ì£¼ë¡œ ì‚¬ìš©.
  <br>
  <img src="images/regression_example.png" style="width=100%;height=60%;" >


---
* <font size='3'>D/L ê¸°ë°˜ ë°©ë²•ë¡ </font>  
  - ì›ë¦¬: ë¹„ì„ í˜•ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í•˜ëŠ” ê³ ì°¨ì›ì˜ ë°©ì •ì‹ì„ ë§Œë“¤ê±°ë‚˜ ë°ì´í„° ìì²´ì˜ ë¶„í¬ë¥¼ í•™ìŠµí•˜ì—¬ ìƒˆë¡œìš´ Inputì— ëŒ€ì‘í•˜ëŠ” ê²°ê³¼ë¥¼ ì˜ˆì¸¡.
  - í•™ìŠµ: ê³ ì°¨ì› í•™ìŠµë°©ë²• ì¤‘ Gradient Descent ê¸°ë°˜ì˜ ê·€ë‚©ì  ë°©ë²•ë¡ ì„ ì‚¬ìš©.
  - ëª©ì : ë†’ì€ ì˜ˆì¸¡ë ¥ì„ ê°–ëŠ” ëª¨ë¸ì„ ëª©í‘œë¡œ, ì„ í˜•ê·¼ì‚¬ë¡œëŠ” ì„¤ëª…í•˜ê¸° í˜ë“  ë¹„ì •í˜• ë°ì´í„° ë˜ëŠ” ë³µì¡í•œ ë°ì´í„° ê°„ì˜ ê´€ê³„ ì„¤ëª….
  - í‰ê°€: ì˜¤ì°¨ì˜ í¬ê¸°, ì •í™•ë„(Accuracy) ë° Precision/Recall ë“± ì˜ˆì¸¡ê²°ê³¼ì— ëŒ€í•œ ì§€í‘œë“¤ì´ ì£¼ë¡œ ì‚¬ìš©.
  <br>
  <img src="images/dl_prediction_example.png" style="width=100%;height=60%;" >

<br>
ë”¥ëŸ¬ë‹ì€ ì‚¬ì‹¤ ë‚˜ì˜¨ ì§€ 30ë…„ì´ ë„˜ì—ˆë‹¤.  
ì™œ ë‹¤ì‹œ ë”¥ëŸ¬ë‹ì´ ê°ê´‘ë°›ëŠ” ê²ƒì¼ê¹Œ?

<div class="alert alert-block alert-info"  markdown="1">
<b>$\divideontimes$ ë”¥ëŸ¬ë‹ì˜ ì¬ì¡°ëª…: </b> 
<br>
<br>

&nbsp;<font size='3'><b><i>$\cdot$ ê³¼ì í•© ë¬¸ì œ í•´ê²°</i></b></font>  
&nbsp;  &nbsp;  - Relu ë“± Vanishing Gradient ë¬¸ì œë¥¼ ì»¤ë²„í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ê³¼ íš¨ìœ¨ì ì¸ Optimizerì˜ ë“±ì¥ìœ¼ë¡œ í•™ìŠµ ì„±ëŠ¥ ì¦ê°€  
&nbsp;  &nbsp; - Dropout ë“± íš¨ìœ¨ì ì¸ ê³¼ì í•© ë°©ì§€ ê¸°ë²•ì´ ë“±ì¥í•˜ë©° ê¸°ì¡´ ëª¨ë¸ì˜ í•œê³„ ê·¹ë³µ  
<br>

<font size='3'><b><i>$\cdot$ í•˜ë“œì›¨ì–´ì˜ ë°œì „</i></b></font>  
&nbsp;  &nbsp; - ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì˜ ë†’ì€ ì—°ì‚° ë¶€í•˜ë¥¼ ì»¤ë²„í•˜ëŠ” Computing Powerì˜ ì¦ëŒ€ì™€ GPUë¥¼ í™œìš©í•œ ë³‘ë ¬ ì—°ì‚° ì•„í‚¤í…ì²˜ ë“±ì¥  
<br>

<font size='3'><b><i>$\cdot$ ë¹…ë°ì´í„° ì‹œëŒ€ì˜ ë„ë˜</i></b></font>  
&nbsp;  &nbsp; - ê¸°ì¡´ ì •í˜• ë°ì´í„° ê·œëª¨ì˜ í™•ëŒ€ + ìŒì„±, ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ ë“± ë¹„ì •í˜• ë°ì´í„° ì²˜ë¦¬ ê¸°ìˆ ì˜ ë°œì „  

</center>
<br>
</div>

### ë”¥ëŸ¬ë‹ì˜ ì›ë¦¬

<br>
ìš°ë¦¬ê°€ ë¬´ì–¸ê°€ ë°°ìš¸ ë•Œë¥¼ ìƒìƒí•´ ë³´ì.

> * ì²« ì£¼ì‹íˆ¬ì -> ì‹¤íŒ¨ -> ìƒìƒê³¼ í˜„ì‹¤ì˜ ì°¨ì´ í™•ì¸   
> * ë‰´ìŠ¤ë¥¼ ë³´ë©´ì„œ ë§¤ë§¤ ë…¸í•˜ìš° í•™ìŠµ -> ì‹¤íŒ¨  
> * ìº”ë“¤ì°¨íŠ¸ë„ ë³´ê³ , ì¶”ì„¸ì„ ë„ ë³´ë©´ì„œ ë‚˜ë¦„ì˜ ë§¤ë§¤ ë…¸í•˜ìš° ì—…ë°ì´íŠ¸ -> ì‹¤íŒ¨  
> * ì¬ë¬´ì œí‘œ í™•ì¸í•˜ë©° íšŒì‚¬ì˜ ê°€ì¹˜ë¥¼ ë§¤ë§¤ ë…¸í•˜ìš°ì— ë°˜ì˜ -> ì‹¤íŒ¨
>   ...  
> * ì ì°¨ ì°¨ì´ë¥¼ ì¢í˜€ ë‚˜ê°„ë‹¤...

$$y = f(x)$$
<br>
ë”¥ëŸ¬ë‹ë„ ê¸°ë³¸ ì›ë¦¬ëŠ” ì´ ë°©ì‹ê³¼ ìœ ì‚¬í•˜ë‹¤.  
`Input`ì— í•´ë‹¹í•˜ëŠ” $X$ë¥¼ ê³„ì‚°í•´ì„œ ë‚˜ì˜¤ëŠ” ì˜ˆìƒ $\hat Y$ì™€ ì§„ì§œ ì •ë‹µ $Y$ë¥¼ ë¹„êµí•´ì„œ, ì¡°ê¸ˆì”© ì°¨ì´ë¥¼ ì¤„ì—¬ë‚˜ê°€ê²Œ ëœë‹¤.  
ë‹¤ì–‘í•œ Caseë¥¼ ê²½í—˜í•  ìˆ˜ë¡ ì˜ˆìƒì´ ë¹—ë‚˜ê°ˆ í™•ë¥ ë„ ë‚®ì•„ì§„ë‹¤.  
<br>
<font size='3'>ë”¥ëŸ¬ë‹ì´ë€, <font color='#318CE7'> *__ê³¼ê±° íŒ¨í„´ ì† ê·œì¹™ì„ ì°¾ì•„ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì´ë‹¤.__*</font></font>

<div class="alert alert-block alert-warning">
<b>$\divideontimes$ ë”¥ëŸ¬ë‹ì˜ ì›ë¦¬: </b> ìˆ˜ë§ì€ Caseì—ì„œ ê·œì¹™ì„±ì„ ë°œê²¬í•˜ê³  í•™ìŠµí•œë‹¤.
<br>
$\space$
$\space$
<font size='5'><center>$\hat y = f(x)$ ì¼ ë•Œ,</center>

<center> $Real \space yì™€ \space ìœ ì‚¬í•œ \space \hat y$ë¥¼ ì°¾ëŠ” ê²ƒ.</center></font>
<br>
</div>  


### ë”¥ëŸ¬ë‹ì˜ êµ¬ì¡°

<br>
<font size='5'><center><b><i>"Layerë¼ëŠ” í•¨ìˆ˜ë¥¼ ì—¬ëŸ¬ ê°œ ìŒ“ì€(ì´ì–´ë¶™ì¸) í˜•íƒœ"</i></b></center></font>
<br>


<img src="images/dl_basic_structure.png" style="width=100%;height=60%;" >

<div class="alert alert-block alert-warning">
<b>$\divideontimes$ ë”¥ëŸ¬ë‹ì˜ êµ¬ì¡°: </b> ì—¬ëŸ¬ ê²¹ì˜ Layerë¡œ êµ¬ì„±ëœ Functionê³¼ ê°™ì€ êµ¬ì¡°
<br>
$\space$
$\space$
<font size='4'><center><b>Input $\rightarrow$ Layer(s) $\rightarrow$ Output</b></center></font>

<br>
</div>  

### ë”¥ëŸ¬ë‹ì˜ í•™ìŠµ

<br>
<font size='5'><center><b><i>"ë°˜ë³µí•™ìŠµì„ í†µí•œ ë¬¸ì œìœ í˜• ìµíˆê¸°"</i></b></center></font>

> 1. ë¬¸ì œì€í–‰ì—ì„œ ë¬¸ì œë¥¼ ê³¨ë¼ì„œ
> 2. ë‚˜ë¦„ì˜ ë°©ë²•ìœ¼ë¡œ ë¬¸ì œë¥¼ í’€ê³ 
> 3. ì •ë‹µê³¼ ë§ì¶° ë³´ê³ 
> 4. ì˜¤ë‹µì„ ì •ë¦¬í•´ì„œ
> 5. ë‹¤ì‹œ ë¬¸ì œë¥¼ í’€ê³ ...

<img src="images/dl_training_process.png" >

<div class="alert alert-block alert-warning">
<b>$\divideontimes$ ë”¥ëŸ¬ë‹ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤: </b> 
<br>
$\space$
$\space$
<font size='4'><center><b>Training Dataë¥¼ ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµí•˜ë©´ì„œ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ Update</b></center></font>

<br>
</div>  

### ë”¥ëŸ¬ë‹ì˜ í‰ê°€

<br>
<font size='5'><center><b><i>"ì‹œí—˜ í‰ê· ìœ¼ë¡œ ì„±ì  ë§¤ê¸°ê¸°"</i></b></center></font>

> 1. ì •í•´ì§„ íšŸìˆ˜ë§Œí¼ ì‹œí—˜ì„ ì¹˜ë¥´ê³ 
> 2. ì •ë‹µê³¼ ë§ì¶° ë³´ê³ 
> 3. í‰ê· ì„ ë‚´ì„œ ìµœì¢… ì ìˆ˜ë¡œ íŒì •

<img src="images/dl_testing_process.png" >

<div class="alert alert-block alert-warning">
<b>$\divideontimes$ ë”¥ëŸ¬ë‹ì˜ í‰ê°€ í”„ë¡œì„¸ìŠ¤: </b> 
<br>
$\space$
$\space$
<font size='4'><center><b>Test Dataì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê· í•˜ì—¬ ì„±ëŠ¥ íŒì •</b></center></font>

<br>
</div>  

## Input


```python
import numpy as np
import pandas as pd
from pprint import pprint
from unipy import aprint, lprint

import matplotlib.pyplot as plt

import importlib
from src import examples
from src.utils import lprint, qprint, keras_lossplot, keras_predict_plot
from src.layers import sigmoid

importlib.reload(examples)


from src.load_data import column_range_scaler, load_data
```

### Loading


```python
rawdata = pd.read_csv(
    'data/data_10min.csv',
    parse_dates=['EVT_DTM'],
    dtype={
        'VEND_ID': 'str',
        'ENB_ID': 'str',
        'CELL_ID': 'str',
        'FREQ_TYP_CD': 'str',
    }
)

kpi_list = ['CQI', 'UE_TX_POWER', 'DL_PRB_USAGE_RATE']

rawdata.head(20)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>EVT_DTM</th>
      <th>VEND_ID</th>
      <th>ENB_ID</th>
      <th>CELL_ID</th>
      <th>FREQ_TYP_CD</th>
      <th>CQI</th>
      <th>DL_PRB_USAGE_RATE</th>
      <th>UE_TX_POWER</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2018-07-02 08:00:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>13.990000</td>
      <td>2.738333</td>
      <td>9.182692</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2018-07-02 08:10:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>13.998333</td>
      <td>2.481667</td>
      <td>5.597436</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2018-07-02 08:20:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>14.100000</td>
      <td>3.475000</td>
      <td>7.600000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2018-07-02 08:30:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>13.881667</td>
      <td>3.291667</td>
      <td>10.550000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2018-07-02 08:40:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>13.323333</td>
      <td>4.091667</td>
      <td>11.894444</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2018-07-02 08:50:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>13.863333</td>
      <td>3.393333</td>
      <td>7.707843</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2018-07-02 09:00:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>13.115000</td>
      <td>4.806667</td>
      <td>7.663462</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2018-07-02 09:10:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>13.095000</td>
      <td>5.243333</td>
      <td>8.254386</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2018-07-02 09:20:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>12.905000</td>
      <td>6.326667</td>
      <td>7.367241</td>
    </tr>
    <tr>
      <th>9</th>
      <td>2018-07-02 09:30:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>12.893333</td>
      <td>7.610000</td>
      <td>8.555357</td>
    </tr>
    <tr>
      <th>10</th>
      <td>2018-07-02 09:40:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>13.035000</td>
      <td>8.011667</td>
      <td>7.208772</td>
    </tr>
    <tr>
      <th>11</th>
      <td>2018-07-02 09:50:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>12.000000</td>
      <td>12.236667</td>
      <td>7.740678</td>
    </tr>
    <tr>
      <th>12</th>
      <td>2018-07-02 10:00:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>12.051667</td>
      <td>8.453333</td>
      <td>8.666667</td>
    </tr>
    <tr>
      <th>13</th>
      <td>2018-07-02 10:10:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>12.270000</td>
      <td>8.841667</td>
      <td>8.438333</td>
    </tr>
    <tr>
      <th>14</th>
      <td>2018-07-02 10:20:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>12.983333</td>
      <td>9.063333</td>
      <td>6.443333</td>
    </tr>
    <tr>
      <th>15</th>
      <td>2018-07-02 10:30:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>12.696667</td>
      <td>7.753333</td>
      <td>5.628333</td>
    </tr>
    <tr>
      <th>16</th>
      <td>2018-07-02 10:40:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>13.061667</td>
      <td>13.945000</td>
      <td>6.576667</td>
    </tr>
    <tr>
      <th>17</th>
      <td>2018-07-02 10:50:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>12.703333</td>
      <td>12.908333</td>
      <td>6.480000</td>
    </tr>
    <tr>
      <th>18</th>
      <td>2018-07-02 11:00:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>12.300000</td>
      <td>12.443333</td>
      <td>6.048333</td>
    </tr>
    <tr>
      <th>19</th>
      <td>2018-07-02 11:10:00</td>
      <td>SS</td>
      <td>28380</td>
      <td>24</td>
      <td>10</td>
      <td>11.836667</td>
      <td>15.426667</td>
      <td>5.103333</td>
    </tr>
  </tbody>
</table>
</div>




```python
print(rawdata['CQI'][:500].plot(figsize=(20, 8)))
```

    AxesSubplot(0.125,0.125;0.775x0.755)



![png](output_13_1.png)



```python
print(rawdata[kpi_list][:500].plot(figsize=(20, 8)))
```

    AxesSubplot(0.125,0.125;0.775x0.755)



![png](output_14_1.png)


### Scaling


```python
kpi_range_dict = {
    'CQI': [0, 15],
    'UE_TX_POWER': [-17, 23],
    'DL_PRB_USAGE_RATE': [0, 100],
}

data_scaled, scaler_dict = column_range_scaler(
    rawdata[kpi_list],
    vendor_name='SS',
    col_real_range_dict=kpi_range_dict,
    feature_range=(0., 1.),
)
```


```python
print(data_scaled[:500].plot(figsize=(20, 8)))
```

    AxesSubplot(0.125,0.125;0.775x0.755)



![png](output_17_1.png)



```python
data_scaled.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CQI</th>
      <th>UE_TX_POWER</th>
      <th>DL_PRB_USAGE_RATE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.932667</td>
      <td>0.654567</td>
      <td>0.027383</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.933222</td>
      <td>0.564936</td>
      <td>0.024817</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.940000</td>
      <td>0.615000</td>
      <td>0.034750</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.925444</td>
      <td>0.688750</td>
      <td>0.032917</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.888222</td>
      <td>0.722361</td>
      <td>0.040917</td>
    </tr>
  </tbody>
</table>
</div>



### Sliding Window (`MLP`)

ìš°ì„  ë¬´ìŠ¨ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì˜ˆì¸¡í•  ê²ƒì¸ì§€ ì •í•´ì•¼ í•œë‹¤.

`X`ì™€ `Y`ë¥¼ ì–´ë–»ê²Œ ì •í•˜ë©´ ì¢‹ì„ê¹Œ?
ìš°ë¦¬ê°€ ê°€ì§„ ë°ì´í„°ëŠ” `Time` ì¶•ê³¼, `KPI` ì¶•ì´ ìˆë‹¤.


| Case | ê¸°ì¤€ | `Time`ìˆ˜ | `KPI`ìˆ˜ | ë‚´ìš© |
| :--: | :------: | :------: | :----: | :-- |
| 1 | Row | Multi | 1 | ì—¬ëŸ¬ ì‹œì ì˜ KPIë¡œ ë‹¤ìŒ KPI ì˜ˆì¸¡í•˜ê¸° (1 KPI) |
| 2 | Column | 1 | Multi | í•œ ì‹œì ì— ëŒ€í•œ KPI ì—¬ëŸ¬ ê°œë¡œ ë‹¤ìŒ KPI ì˜ˆì¸¡í•˜ê¸° |
| 3 | Row & Column | Multi | Multi | ì—¬ëŸ¬ ì‹œì ì˜ KPIë¡œ ë‹¤ìŒ KPI ì˜ˆì¸¡í•˜ê¸° (3 KPI) |


ì¼ë‹¨, ì¶•ë³„ë¡œ ê¸°ì¤€ì„ ì¡ì•„ ë³´ì.


<br>
<font size='4'> <b> $\cdot$ Case 1:</b> ì—¬ëŸ¬ ì‹œì ì˜ KPIë¡œ ë‹¤ìŒ KPI ì˜ˆì¸¡í•˜ê¸° (1 KPI)</font>
<br>
<br>
<img src="images/mlp/mlp_graphset01.png">


<br>
<font size='4'> <b> $\cdot$ Case 2:</b> í•œ ì‹œì ì˜ KPI ì—¬ëŸ¬ ê°œë¡œ ë‹¤ìŒ KPI ì˜ˆì¸¡í•˜ê¸°</font>
<br>
<br>
<img src="images/mlp/mlp_graphset02.png">

<br>
<br>
ì´ ì¤‘ Case 2ì— ëŒ€í•´ ì‹¤ìŠµí•œë‹¤. (Case 1ì„ ì ìš©í•˜ëŠ” ê²½ìš°, Case 2ë¥¼ ì¶•ë§Œ ë°”ê¾¸ë©´ ì‰½ê²Œ ì‘ìš©í•  ìˆ˜ ìˆë‹¤.)
<br>

<br>
<font size='4'> <b> $\cdot$ ë°ì´í„° êµ¬ì¡°</b></font>
<br>

<img src="images/sliding_windows_mlp.png" >
<img src="images/sliding_windows_mlp_reshaping.png" >

[Link to MLP Model](#(ì‹¤ìŠµ)-MLP-in-keras)

#### Exercise 1 : Basic


```python
from ipywidgets import interact, interactive, fixed, interact_manual, interactive_output
import ipywidgets as widgets
from IPython.display import display
from pprint import pprint
```


```python
#==== Essential ========================================================#

case_x_list = []
case_y_list = []
for i in range(5):
    case_x_list += [data_scaled.iloc[i:i+1, :3]]
    case_y_list += [data_scaled.iloc[i+1:i+2, :2]]

#=======================================================================#


print('\nX', '=' * 45)
pprint(case_x_list)
print('\nY', '=' * 45)
pprint(case_y_list)
```

    
    X =============================================
    [        CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
    0  0.932667     0.654567           0.027383,
             CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
    1  0.933222     0.564936           0.024817,
         CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
    2  0.94        0.615            0.03475,
             CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
    3  0.925444      0.68875           0.032917,
             CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
    4  0.888222     0.722361           0.040917]
    
    Y =============================================
    [        CQI  UE_TX_POWER
    1  0.933222     0.564936,
         CQI  UE_TX_POWER
    2  0.94        0.615,
             CQI  UE_TX_POWER
    3  0.925444      0.68875,
             CQI  UE_TX_POWER
    4  0.888222     0.722361,
             CQI  UE_TX_POWER
    5  0.924222     0.617696]


#### Exercise 2 : Sliding Window


```python
@interact(
    case_num=widgets.IntSlider(min=1, max=10, step=1, value=2),
    kpi_num=widgets.IntSlider(min=1, max=3, step=1, value=3),
)
def shape_print(case_num, kpi_num):

    data_cnt = case_num
    input_dim = kpi_num

    print(
        '=' * 40 +
        '\n (data_cnt, input_dim)  ' +
        ':' +
        f'  ({data_cnt}, {input_dim})\n' +
        '=' * 40 + 
        '\n',
    )

    
    #==== Essential ========================================================#
    
    case_list = []
    for i in range(data_cnt):
        case_list += [(
            'X ' +  '=' * 40,
            data_scaled.iloc[i:i+1, :input_dim],
        )]

    #=======================================================================#

    
    pprint(case_list)
```


    interactive(children=(IntSlider(value=2, description='case_num', max=10, min=1), IntSlider(value=3, descriptioâ€¦


#### Exercise 3 : Sliding Window `(X & Y)`


```python
@interact(
    case_num=widgets.IntSlider(min=1, max=10, step=1, value=5),
    x_kpi_num=widgets.IntSlider(min=1, max=3, step=1, value=3),
    y_kpi_num=widgets.IntSlider(min=1, max=3, step=1, value=2),
)
def shape_print(case_num, x_kpi_num, y_kpi_num):

    data_cnt = case_num
    input_dim = x_kpi_num
    output_dim = y_kpi_num

    print(
        '=' * 45 +
        '\n [X]: (data_cnt, input_dim )  ' +
        ':' +
        f'  ({data_cnt}, {input_dim})' +
        '\n [Y]: (data_cnt, output_dim)  ' +
        ':' +
        f'  ({data_cnt}, {output_dim})\n' +
        '=' * 45 + 
        '\n',
    )
    
    
    #==== Essential ========================================================#

    case_list = []
    for i in range(data_cnt):
        case_list += [(
            'X ' +  '=' * 40,
            data_scaled.iloc[i:i+1, :input_dim],
            'Y ' +  '-' * 40,
            data_scaled.iloc[i+1:i+2, :output_dim],
        )]
    
    #=======================================================================#
    
 
    pprint(case_list)
```


    interactive(children=(IntSlider(value=5, description='case_num', max=10, min=1), IntSlider(value=3, descriptioâ€¦



```python
def sliding_window_mlp(data, data_cnt, input_dim, output_dim):

    case_x_list = []
    case_y_list = []
    
    data_rownum = data.shape[0]
    max_data_cnt = data_rownum - 1
    
    if data_cnt is None:
        data_cnt = max_data_cnt
    else:
        data_cnt = min(data_cnt, max_data_cnt)
    
    
    #==== Essential ========================================================#
    
    for i in range(data_cnt):
        case_x_list += [data_scaled.iloc[i, :input_dim].values]
        case_y_list += [data_scaled.iloc[i+1, :output_dim].values]

    #=======================================================================#


    print(
        '=' * 45 +
        '\n [X]: (data_cnt, input_dim )  ' +
        ':' +
        f'  ({data_cnt}, {input_dim})' +
        '\n [Y]: (data_cnt, output_dim)  ' +
        ':' +
        f'  ({data_cnt}, {output_dim})\n' +
        '=' * 45 + 
        '\n',
    )
        
    return np.stack(case_x_list), np.stack(case_y_list)


#==== Essential ========================================================#

data_x_mlp, data_y_mlp = sliding_window_mlp(
    data_scaled,
    data_cnt=data_scaled.shape[0],
    input_dim=3,
    output_dim=2,
)

#=======================================================================#


aprint(data_x_mlp[:5], data_y_mlp[:5], name_list=['data_x_mlp[:5]', 'data_y_mlp[:5]'])
```

    =============================================
     [X]: (data_cnt, input_dim )  :  (4289, 3)
     [Y]: (data_cnt, output_dim)  :  (4289, 2)
    =============================================
    
    =========================================================================
    |  data_x_mlp[:5]                        |   data_y_mlp[:5]             |
    |  (5, 3)                                |   (5, 2)                     |
    =========================================================================
    |  [[0.93266667 0.65456731 0.02738333]   |   [[0.93322222 0.5649359 ]   |
    |   [0.93322222 0.5649359  0.02481667]   |    [0.94       0.615     ]   |
    |   [0.94       0.615      0.03475   ]   |    [0.92544444 0.68875   ]   |
    |   [0.92544444 0.68875    0.03291667]   |    [0.88822222 0.72236111]   |
    |   [0.88822222 0.72236111 0.04091667]]  |    [0.92422222 0.61769608]]  |
    =========================================================================


### Sliding Window (`RNN`)

ì´ë²ˆì—ëŠ” `Time`ì¶•ê³¼ `KPI` ì¶•ì„ ëª¨ë‘ ê³ ë ¤í•´ ë³´ì.

<br>
<font size='4'> <b> $\cdot$ Case 3:</b> ì—¬ëŸ¬ ì‹œì ì˜ KPIë¡œ ë‹¤ìŒ KPI ì˜ˆì¸¡í•˜ê¸° (3 KPI)</font>
<br>
<br>

<img src="images/rnn/rnn_graphset01.png" >
<br>

<br>
<font size='4'> <b> $\cdot$ ë°ì´í„° êµ¬ì¡°</b></font>
<br>

<img src="images/sliding_windows_rnn.png" >
<img src="images/sliding_windows_rnn_reshaping.png" >

[Link to RNN Model](#(ì‹¤ìŠµ)-RNN-in-keras:-many-to-many)

#### Exercise 1 : Basic


```python
case_list = []
for i in range(4):
    case_list += [data_scaled[i:i+7]]

case_list
```




    [        CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
     0  0.932667     0.654567           0.027383
     1  0.933222     0.564936           0.024817
     2  0.940000     0.615000           0.034750
     3  0.925444     0.688750           0.032917
     4  0.888222     0.722361           0.040917
     5  0.924222     0.617696           0.033933
     6  0.874333     0.616587           0.048067,
             CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
     1  0.933222     0.564936           0.024817
     2  0.940000     0.615000           0.034750
     3  0.925444     0.688750           0.032917
     4  0.888222     0.722361           0.040917
     5  0.924222     0.617696           0.033933
     6  0.874333     0.616587           0.048067
     7  0.873000     0.631360           0.052433,
             CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
     2  0.940000     0.615000           0.034750
     3  0.925444     0.688750           0.032917
     4  0.888222     0.722361           0.040917
     5  0.924222     0.617696           0.033933
     6  0.874333     0.616587           0.048067
     7  0.873000     0.631360           0.052433
     8  0.860333     0.609181           0.063267,
             CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
     3  0.925444     0.688750           0.032917
     4  0.888222     0.722361           0.040917
     5  0.924222     0.617696           0.033933
     6  0.874333     0.616587           0.048067
     7  0.873000     0.631360           0.052433
     8  0.860333     0.609181           0.063267
     9  0.859556     0.638884           0.076100]



#### Exercise 2 : Sliding Window


```python
from ipywidgets import interact, interactive, fixed, interact_manual, interactive_output
import ipywidgets as widgets
from IPython.display import display
from pprint import pprint
```


```python
@interact(
    case_num=widgets.IntSlider(min=1, max=10, step=1, value=2),
    window_size=widgets.IntSlider(min=2, max=10, step=1, value=7),
    kpi_num=widgets.IntSlider(min=1, max=3, step=1, value=3),
)
def shape_print(case_num, window_size, kpi_num):

    data_cnt = case_num
    input_size = window_size
    input_dim = kpi_num

    print(
        '=' * 51 +
        '\n (data_cnt, input_size, input_dim)  ' +
        ':' +
        f'  ({data_cnt}, {input_size}, {input_dim})\n' +
        '=' * 51 + 
        '\n',
    )

    
    #==== Essential ========================================================#

    case_list = []
    for i in range(data_cnt):
        case_list += [(
            'X ' +  '=' * 40,
            data_scaled.iloc[i:i+input_size, :input_dim],
        )]

    #=======================================================================#


    pprint(case_list)
```


    interactive(children=(IntSlider(value=2, description='case_num', max=10, min=1), IntSlider(value=7, descriptioâ€¦


#### Exercise 3 : Sliding Window `(X & Y)`


```python
!jupyter nbextension enable --py widgetsnbextension
```

    Enabling notebook extension jupyter-js-widgets/extension...
          - Validating: [32mOK[0m



```python
@interact(
    case_num=widgets.IntSlider(min=1, max=10, step=1, value=2),
    window_x=widgets.IntSlider(min=2, max=5, step=1, value=5),
    window_y=widgets.IntSlider(min=1, max=5, step=1, value=2),
    input_kpi_num=widgets.IntSlider(min=1, max=3, step=1, value=3),
    output_kpi_num=widgets.IntSlider(min=1, max=3, step=1, value=3),
)
def shape_print(case_num, window_x, window_y, input_kpi_num, output_kpi_num):

    data_cnt = case_num
    input_size = window_x
    output_size = window_y
    window_size = input_size + output_size
    input_dim = input_kpi_num
    output_dim = output_kpi_num

    print(
        '=' * 60 +
        '\n [X]: (data_cnt, input_size,  input_dim )  ' +
        ':' +
        f'  ({data_cnt}, {input_size}, {input_dim})' +
        '\n [Y]: (data_cnt, output_size, output_dim)  ' +
        ':' +
        f'  ({data_cnt}, {output_size}, {output_dim})\n' +
        '=' * 60 + 
        '\n',
    )
    
    #==== Essential ========================================================#

    case_list = []
    for i in range(data_cnt):
        case_list += [(
            'X ' +  '=' * 40,
            data_scaled.iloc[i:i+input_size, :input_dim],
            'Y ' +  '-' * 40,
            data_scaled.iloc[i+input_size:i+input_size+output_size, :output_dim],
        )]
 
    #=======================================================================#
    

    pprint(case_list)
```


    interactive(children=(IntSlider(value=2, description='case_num', max=10, min=1), IntSlider(value=5, descriptioâ€¦



```python
def sliding_window_rnn(data, data_cnt, input_size, output_size, input_dim, output_dim):

    case_x_list = []
    case_y_list = []
    
    data_rownum = data.shape[0]
    max_data_cnt = data_rownum - (input_size + output_size) + 1
    
    if data_cnt is None:
        data_cnt = max_data_cnt
    else:
        data_cnt = min(data_cnt, max_data_cnt)
    
    
    #==== Essential ========================================================#
    
    for i in range(data_cnt):
        case_x_list += [data_scaled.iloc[i:i+input_size, :input_dim].values]
        case_y_list += [data_scaled.iloc[i+input_size:i+input_size+output_size, :output_dim].values]

    #=======================================================================#


    print(
        '=' * 60 +
        '\n [X]: (data_cnt, input_size,  input_dim )  ' +
        ':' +
        f'  ({data_cnt}, {input_size}, {input_dim})' +
        '\n [Y]: (data_cnt, output_size, output_dim)  ' +
        ':' +
        f'  ({data_cnt}, {output_size}, {output_dim})\n' +
        '=' * 60 + 
        '\n',
    )
        
    return np.stack(case_x_list), np.stack(case_y_list)


#==== Essential ========================================================#

data_x_rnn, data_y_rnn = sliding_window_rnn(
    data_scaled,
    data_cnt=data_scaled.shape[0],
    input_size=5,
    output_size=2,
    input_dim=3,
    output_dim=2,
)

#=======================================================================#


aprint(data_x_rnn[:5], data_y_rnn[:5], name_list=['data_x_rnn[:5]', 'data_y_rnn[:5]'])
```

    ============================================================
     [X]: (data_cnt, input_size,  input_dim )  :  (4284, 5, 3)
     [Y]: (data_cnt, output_size, output_dim)  :  (4284, 2, 2)
    ============================================================
    
    =============================================================================
    |  data_x_rnn[:5]                          |   data_y_rnn[:5]               |
    |  (5, 5, 3)                               |   (5, 2, 2)                    |
    =============================================================================
    |  [[[0.93266667 0.65456731 0.02738333]    |   [[[0.92422222 0.61769608]    |
    |    [0.93322222 0.5649359  0.02481667]    |     [0.87433333 0.61658654]]   |
    |    [0.94       0.615      0.03475   ]    |                                |
    |    [0.92544444 0.68875    0.03291667]    |    [[0.87433333 0.61658654]    |
    |    [0.88822222 0.72236111 0.04091667]]   |     [0.873      0.63135965]]   |
    |                                          |                                |
    |   [[0.93322222 0.5649359  0.02481667]    |    [[0.873      0.63135965]    |
    |    [0.94       0.615      0.03475   ]    |     [0.86033333 0.60918103]]   |
    |    [0.92544444 0.68875    0.03291667]    |                                |
    |    [0.88822222 0.72236111 0.04091667]    |    [[0.86033333 0.60918103]    |
    |    [0.92422222 0.61769608 0.03393333]]   |     [0.85955556 0.63888393]]   |
    |                                          |                                |
    |   [[0.94       0.615      0.03475   ]    |    [[0.85955556 0.63888393]    |
    |    [0.92544444 0.68875    0.03291667]    |     [0.869      0.6052193 ]]]  |
    |    [0.88822222 0.72236111 0.04091667]    |                                |
    |    [0.92422222 0.61769608 0.03393333]    |                                |
    |    [0.87433333 0.61658654 0.04806667]]   |                                |
    |                                          |                                |
    |   [[0.92544444 0.68875    0.03291667]    |                                |
    |    [0.88822222 0.72236111 0.04091667]    |                                |
    |    [0.92422222 0.61769608 0.03393333]    |                                |
    |    [0.87433333 0.61658654 0.04806667]    |                                |
    |    [0.873      0.63135965 0.05243333]]   |                                |
    |                                          |                                |
    |   [[0.88822222 0.72236111 0.04091667]    |                                |
    |    [0.92422222 0.61769608 0.03393333]    |                                |
    |    [0.87433333 0.61658654 0.04806667]    |                                |
    |    [0.873      0.63135965 0.05243333]    |                                |
    |    [0.86033333 0.60918103 0.06326667]]]  |                                |
    =============================================================================


### Splitting data : Train & Test


```python
train_x_mlp, test_x_mlp = data_x_mlp[:4000], data_x_mlp[4000:]
train_y_mlp, test_y_mlp = data_y_mlp[:4000], data_y_mlp[4000:]


train_x_rnn, test_x_rnn = data_x_rnn[:4000], data_x_rnn[4000:]
train_y_rnn, test_y_rnn = data_y_rnn[:4000], data_y_rnn[4000:]
```

* Saving the data


```python
data_list = [
    train_x_mlp,
    train_y_mlp,
    train_x_rnn,
    train_y_rnn,
    test_x_mlp,
    test_y_mlp,
    test_x_rnn,
    test_y_rnn,
]


data_str_list = [
    'train_x_mlp',
    'train_y_mlp',
    'train_x_rnn',
    'train_y_rnn',
    'test_x_mlp',
    'test_y_mlp',
    'test_x_rnn',
    'test_y_rnn',
]


for _, __ in zip(data_list, data_str_list):
    _tmp = _.astype(np.float32)
    print(f'{__:10} : {_tmp.shape}')
    np.save(f'data/{__}.npy', _tmp)
```

    train_x_mlp : (4000, 3)
    train_y_mlp : (4000, 2)
    train_x_rnn : (4000, 5, 3)
    train_y_rnn : (4000, 2, 2)
    test_x_mlp : (289, 3)
    test_y_mlp : (289, 2)
    test_x_rnn : (284, 5, 3)
    test_y_rnn : (284, 2, 2)


* Loading the data


```python
data_dict = load_data()

train_x_mlp = data_dict['train_x_mlp']
train_y_mlp = data_dict['train_y_mlp']
test_x_mlp = data_dict['test_x_mlp']
test_y_mlp = data_dict['test_y_mlp']
```

    Loading Data...
    train_x_mlp : (4000, 3)
    train_y_mlp : (4000, 2)
    train_x_rnn : (4000, 5, 3)
    train_y_rnn : (4000, 2, 2)
    test_x_mlp : (289, 3)
    test_y_mlp : (289, 2)
    test_x_rnn : (284, 5, 3)
    test_y_rnn : (284, 2, 2)
    Complete.



```python
load_data
```




    <function src.load_data.load_data()>



## (ì‹¤ìŠµ) MLP in `keras`

<br>
<font size='4'> <b> $\cdot$ Case 2:</b> í•œ ì‹œì ì˜ KPI ì—¬ëŸ¬ ê°œë¡œ ë‹¤ìŒ KPI ì˜ˆì¸¡í•˜ê¸°</font>
<br>
<br>
<img src="images/mlp/mlp_graphset02.png">

<br>
<font size='4'> <b> $\cdot$ ê³„ì‚° íë¦„ : MLP</b></font>
<img src='images/mlp/mlp_compute_flow.gif' width='100%'>

<br>
<font size='4'> <b> $\cdot$ ì½”ë“œ ë¦¬ë·° : MLP</b></font>
<br><img src="images/mlp/mlp_keras.png" >

[Link to MLP Input](#Sliding-Window-(MLP))

<div class="alert alert-block alert-info">
<b>$\divideontimes$ Unit_num ì´ë€ : </b> 

ë”¥ëŸ¬ë‹ Frameworkì—ì„œ í”íˆ ë³¼ ìˆ˜ ìˆëŠ” `unit_num`ì€ `output_shape`ë¥¼ ë§í•œë‹¤.


ë‡Œì™€ ë‰´ëŸ°(ë‡Œì‹ ê²½)ì„ ìƒê°í•´ ë³´ì.  
ìš°ë¦¬ê°€ ì˜í™”ë¥¼ ë³¼ ë•Œ, ë‡Œì—ì„œ ì–´ë–¤ ë‰´ëŸ°ì€ ì˜ìƒ ì´ë¯¸ì§€ë¥¼, ì–´ë–¤ ë‰´ëŸ°ì€ ì†Œë¦¬ë¥¼ ë°›ì•„ë“¤ì¸ë‹¤.  
í•˜ë‚˜ì˜ í™”ë©´ì€ ìˆ˜ë§ì€ ë‰´ëŸ°ë“¤ì„ í†µí•´ ê°ê°ì˜ ê²°ê³¼ë¥¼ ë‚´ë†“ê²Œ ë˜ê³   
ìµœì¢…ì ìœ¼ë¡œëŠ” ì´ ê²°ê³¼ë“¤ì„ ì¢…í•©í•˜ì—¬ ì˜í™”ê°€ ì¬ë¯¸ìˆëŠ”ì§€ íŒë‹¨í•œë‹¤.  

`unit_num`ì€ ì…ë ¥ì„ ì²˜ë¦¬í•˜ëŠ” ë‰´ëŸ°ì˜ ê°œìˆ˜ì´ë©°  
í•œ í™”ë©´ì— ëŒ€í•´ ë‰´ëŸ°ì˜ ê°œìˆ˜ë§Œí¼ ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ” ê²ƒ ì²˜ëŸ¼,  
Inputì„ ë°”ë¼ë³´ëŠ” `unit`ì˜ ê°œìˆ˜ë§Œí¼ Hidden Dimensionì˜ í¬ê¸°ë„ ì»¤ì§€ê²Œ ëœë‹¤.  


<br>
<br>
<font size='5'><center><b><i>"`unit_num`ìœ¼ë¡œ ë‹¤ìŒ ë²ˆ `Hidden Dimension`ì˜ í¬ê¸°ë¥¼ ê²°ì •í•œë‹¤."</i></b></center></font>
<br>
<br>
<br>
$\cdot$ <font size='3'>Input Dimension : $m = 3$</font><br>

$\cdot$ <font size='3'>Output Dimension : $l = 4$</font>  


<font size='4'>
\begin{equation}
\begin{bmatrix} n\times m \end{bmatrix}
\overbrace{ \begin{bmatrix} m\times l \end{bmatrix} }^{Layer} 
= \begin{bmatrix} n\times l \end{bmatrix}
\end{equation}
</font>

$\space$
$\space$

<font size='4'>
\begin{equation}
\overbrace{
\left({\begin{array}{cc}
\color{red}a & \color{red}b & \color{red}c \\
\color{red}d & \color{red}e & \color{red}f 
\end{array}}\right)
}^{input \_ dim \space = \space 3}
\overbrace{
\left( \begin{array}{cc}
    \color{blue}  A & \color{blue} D & \color{blue} G & \color{blue} J \\
    \color{blue} B & \color{blue} E & \color{blue} H & \color{blue} K \\
    \color{blue} C & \color{blue} F & \color{blue} I & \color{blue} L \\
\end{array} \right) }^{unit \_ num \space = \space 4}
= \overbrace{
\left(\begin{array}{cc} 
    \color{red}a \color{blue}A + \color{red}b \color{blue}B + \color{red}c \color{blue}C & 
    \color{red}a \color{blue}D + \color{red}b \color{blue}E + \color{red}c \color{blue}F &
    \color{red}a \color{blue}G + \color{red}b \color{blue}H + \color{red}c \color{blue}I &
    \color{red}a \color{blue}J + \color{red}b \color{blue}K + \color{red}c \color{blue}L
    \\
    \color{red}d \color{blue}A + \color{red}e \color{blue}B + \color{red}f \color{blue}C & 
    \color{red}d \color{blue}D + \color{red}e \color{blue}E + \color{red}f \color{blue}F & 
    \color{red}d \color{blue}G + \color{red}e \color{blue}H + \color{red}f \color{blue}I &
    \color{red}d \color{blue}J + \color{red}e \color{blue}K + \color{red}f \color{blue}L
    \\
\end{array}\right)
}^{output \_ dim \space = \space unit \_ num \space = \space 4}
\end{equation}
</font>

</div>
<br>


```python
import keras
import keras.backend as K
from keras import Model, Sequential
from keras.layers import Input, Dense, SimpleRNN, LSTM, GRU, TimeDistributed
```

    Using TensorFlow backend.


### MLP Modeling


```python
input_layer = Input(shape=(3, ))
layer1 = Dense(4, input_dim=3, activation='sigmoid')
layer2 = Dense(2, input_dim=4, activation='sigmoid')

hidden_layer1 = layer1(input_layer)
output_layer = layer2(hidden_layer1)

model = Model(inputs=[input_layer], outputs=[output_layer])

model.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_1 (InputLayer)         (None, 3)                 0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 4)                 16        
    _________________________________________________________________
    dense_2 (Dense)              (None, 2)                 10        
    =================================================================
    Total params: 26
    Trainable params: 26
    Non-trainable params: 0
    _________________________________________________________________



```python
input_layer = Input(shape=(3, ))
layer1 = Dense(16, input_dim=3, activation='sigmoid')
layer2 = Dense(64, input_dim=16, activation='sigmoid')
layer3 = Dense(16, input_dim=64, activation='sigmoid')
layer4 = Dense(8, input_dim=16, activation='sigmoid')
layer5 = Dense(2, input_dim=8, activation='sigmoid')

hidden_layer1 = layer1(input_layer)
hidden_layer2 = layer2(hidden_layer1)
hidden_layer3 = layer3(hidden_layer2)
hidden_layer4 = layer4(hidden_layer3)
output_layer = layer5(hidden_layer4)

model = Model(inputs=[input_layer], outputs=[output_layer])

model.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_4 (InputLayer)         (None, 3)                 0         
    _________________________________________________________________
    dense_13 (Dense)             (None, 16)                64        
    _________________________________________________________________
    dense_14 (Dense)             (None, 64)                1088      
    _________________________________________________________________
    dense_15 (Dense)             (None, 16)                1040      
    _________________________________________________________________
    dense_16 (Dense)             (None, 8)                 136       
    _________________________________________________________________
    dense_17 (Dense)             (None, 2)                 18        
    =================================================================
    Total params: 2,346
    Trainable params: 2,346
    Non-trainable params: 0
    _________________________________________________________________


### MLP Building

* `optimizer` : ë³µìŠµ ë…¸í•˜ìš°
* `loss` : ëª¨ì˜ê³ ì‚¬ ë•Œ ì‚¬ìš©í•  ì ìˆ˜ê³„ì‚°ë²• (`ì´ì `, `í‰ê· `)
* `metrics` : ì‹¤ì „ì—ì„œ ì‚¬ìš©í•  ì ìˆ˜ê³„ì‚°ë²• (`1 ~ 7ë“±ê¸‰`, `A ~ F`, `ìˆ˜ìš°ë¯¸ì–‘ê°€`)


```python
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])
```

### MLP Training

<div class="alert alert-block alert-success">
<b>Q.</b> ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ë³´ì.
<n>

<b><i>`batch_size`</i></b>ì™€ <b><i>`epoch`</i></b>ë¥¼ ì¡°ì ˆí•´ ë³´ê³ ,
`loss` ê°’ì´ ì¤„ì–´ë“¤ê³  ìˆëŠ” ì§€ Plotìœ¼ë¡œ í™•ì¸í•´ ë³´ì.
</div>


```python
fitted = model.fit(
    train_x_mlp,
    train_y_mlp,
    batch_size=64,
    epochs=200,
    validation_split=.2,
    verbose=1,            # [verbose] 1: progress bar, 2: one line per epoch
    shuffle=True,
)
```

    Train on 3200 samples, validate on 800 samples
    Epoch 1/200
    3200/3200 [==============================] - 0s 50us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0029 - val_mean_absolute_error: 0.0438
    Epoch 2/200
    3200/3200 [==============================] - 0s 44us/step - loss: 0.0017 - mean_absolute_error: 0.0295 - val_loss: 0.0029 - val_mean_absolute_error: 0.0441
    Epoch 3/200
    3200/3200 [==============================] - 0s 46us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0027 - val_mean_absolute_error: 0.0423
    Epoch 4/200
    3200/3200 [==============================] - 0s 41us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0028 - val_mean_absolute_error: 0.0423
    Epoch 5/200
    3200/3200 [==============================] - 0s 41us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0027 - val_mean_absolute_error: 0.0415
    Epoch 6/200
    3200/3200 [==============================] - 0s 41us/step - loss: 0.0017 - mean_absolute_error: 0.0295 - val_loss: 0.0028 - val_mean_absolute_error: 0.0427
    Epoch 7/200
    3200/3200 [==============================] - 0s 41us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0027 - val_mean_absolute_error: 0.0415
    Epoch 8/200
    3200/3200 [==============================] - 0s 45us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0029 - val_mean_absolute_error: 0.0436
    Epoch 9/200
    3200/3200 [==============================] - 0s 44us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0030 - val_mean_absolute_error: 0.0450
    Epoch 10/200
    3200/3200 [==============================] - 0s 43us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0028 - val_mean_absolute_error: 0.0422
    Epoch 11/200
    3200/3200 [==============================] - 0s 44us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0028 - val_mean_absolute_error: 0.0418
    Epoch 12/200
    3200/3200 [==============================] - 0s 45us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0027 - val_mean_absolute_error: 0.0413
    Epoch 13/200
    3200/3200 [==============================] - 0s 46us/step - loss: 0.0017 - mean_absolute_error: 0.0295 - val_loss: 0.0028 - val_mean_absolute_error: 0.0420
    Epoch 14/200
      64/3200 [..............................] - ETA: 0s - loss: 0.0017 - mean_absolute_error: 0.03


<b>limit_output extension: Maximum message size of 10000 exceeded with 10159 characters</b>



```python
keras_lossplot(model)
```




![png](output_59_0.png)



<div class="alert alert-block alert-danger">
<b>Tip:</b>  
<n>

`batch_size`ê°€ ë„ˆë¬´ ì»¤ë„ í•™ìŠµì´ ì˜ ë˜ì§€ ì•Šìœ¼ë©°, ë¬´ì‘ì • `epoch` ìˆ˜ë¡œ ë°˜ë³µì„ ë§ì´ ì‹œì¼œë„ í•™ìŠµì„±ëŠ¥ì— í° ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤.
<n>

`batch_size`ë¥¼ ì‘ê²Œ í•˜ë©´ ì–´ë–¨ê¹Œ?
</div>

<br>ìš°ë¦¬ê°€ í•œêµ­ì‚¬ ì‹œí—˜ì„ ë³¸ë‹¤ê³  ê°€ì •í•˜ì.

<img src="images/dl_training_process.png" >


* `batch_size` : ëª¨ì˜ê³ ì‚¬ 1íšŒë¶„ ì¶œì œ ë²”ìœ„(8=ê³ ì¡°ì„ ~ì‚¼êµ­ì‹œëŒ€, 256=ê³ ì¡°ì„ ~ê·¼í˜„ëŒ€ì‚¬)
* `epoch` : ëª¨ì˜ê³ ì‚¬ ì‘ì‹œ íšŸìˆ˜
* `shuffle` : ë¬¸í•­/ë³´ê¸° ì„ê¸°
* ëª¨ë¸ì˜ ê¹Šì´(`layer`ì˜ ê°œìˆ˜, `hidden space`ì˜ í¬ê¸°) : í•™ì—…ìˆ˜ì¤€, IQ


í•™ìƒì˜ í•™ì—…ìˆ˜ì¤€ì— ë”°ë¼ ê° í•­ëª©ë“¤ì„ ì ì ˆíˆ ë°°ë¶„í•´ì•¼ ì‹¤ë ¥ì´ ì‘¥ì‘¥ ëŠ˜ë“¯ì´,   
<font size='4'><font color='#318CE7'><b><i>ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ Parameter ê°’ì„ ì§€ì •í•´ ì£¼ì–´ì•¼ í•œë‹¤.</i></b></font></font>

<br>
</div>  

### MLP Evaluation

ìœ„ì—ì„œëŠ” ëª¨ì˜ê³ ì‚¬ ì ìˆ˜ë¡œ í•™ìŠµì˜ ì§„í–‰ìƒí™©ë§Œì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.  
ì´ì œ ì‹¤ì „ìœ¼ë¡œ ê·¸ë™ì•ˆì˜ ë…¸ë ¥ì— ëŒ€í•œ ê²°ê³¼ë¥¼ í‰ê°€í•´ ë³´ì.

<br>
<br>
<font size='5'><center><b><i>"Training setìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì„ Test setìœ¼ë¡œ í‰ê°€í•œë‹¤."</i></b></center></font>
<br><n>

`model.compile`ì—ì„œ ì„¤ì •í•œ `loss`ì™€ `metrics` ê°’ì´ ê°ê° ì¶œë ¥ëœë‹¤.



|  í•­ëª©      |  ë‚´ìš©                   |  ì˜ˆì‹œ        |
|:----------|-----------------------:|:------------|
|  `loss`   | í•™ìŠµí•  ë•Œ ì‚¬ìš©í•œ ì ìˆ˜ê³„ì‚°ë²•   | ìˆ˜ëŠ¥ì ìˆ˜(ì´ì ) |
| `metrics` | í‰ê°€í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ì ìˆ˜ê³„ì‚°ë²• |  ìˆ˜ëŠ¥ ë“±ê¸‰ì œ   |
<br>

Result: 
```
<finished>/<total test num> [====== <progress bar> ========] - <elapsed time>/step
[<loss>, <metrics>]
```


```python
model.evaluate(test_x_mlp, test_y_mlp)
```

    289/289 [==============================] - 0s 34us/step





    [0.001410347225859916, 0.030117209498032566]



### MLP Prediction

ì´ì œ í•™ìŠµë„ í–ˆê³ , ì‹¤ì „í‰ê°€ë„ ì¹˜ë¤˜ë‹¤.  
í•˜ì§€ë§Œ ìˆ˜ëŠ¥ì„ ì˜ í‘¸ëŠ” ê²ƒê³¼ ì¼ì„ ì˜ í•˜ëŠ” ê±´ ë‹¤ë¥¸ ì´ì•¼ê¸°ì´ë‹¤. ì§„ì •í•œ ì˜ë¯¸ì˜ ì‹¤ì „ì— íˆ¬ì…í•´ ë³´ì.

*__X__* Case 1ê°œë¥¼ ê³¨ë¼ *__Y__*ê°’ì„ ì˜ˆì¸¡í•´ ë³´ì.

<div class="alert alert-block alert-danger">
<b>Tip:</b>  
<n>


```py
model.predict(test_x_mlp[0])
```

ìœ„ì˜ ì½”ë“œëŠ” ëŒì•„ê°€ì§€ ì•ŠëŠ”ë‹¤. ì™œì¼ê¹Œ?
<n>

```
ValueError: Error when checking input: expected input_2 to have shape (3,) but got array with shape (1,)
```

ê·¸ë™ì•ˆ ì‚¬ìš©í–ˆë˜ Inputì€ `(N, 3)` í˜•íƒœì˜ 2ì°¨ì› arrayì˜€ë‹¤.  
1ê°œë§Œ ì„ íƒí•´ ë²„ë¦¬ë‹ˆ `N`ì— í•´ë‹¹í•˜ëŠ” ì°¨ì›ì„ ìƒëµí•˜ê³  `3`ë§Œ ë‚¨ì€ ê²ƒì´ë‹¤.  
2ì°¨ì›ì„ ìœ ì§€í•˜ë©´ì„œ 1ê°œë§Œ ì„ íƒí•˜ë ¤ë©´ ì–´ë–»ê²Œ ë„£ì–´ì•¼ í• ê¹Œ?

</div>


```python
model.predict(test_x_mlp[:1])
```




    array([[0.8122951 , 0.63150454]], dtype=float32)



ì‹¤ì œ ë‚˜ì™€ì•¼ í•˜ëŠ” ê°’ì€:


```python
test_y_mlp[:1]
```




    array([[0.8277778, 0.6791667]], dtype=float32)




```python
keras_predict_plot(model, test_x_mlp, test_y_mlp, method='mlp')
```

    /opt/conda/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:457: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure
      "matplotlib is currently using a non-GUI backend, "



![png](output_69_1.png)


## (ê°œë…) MLP: Multi Layer Perceptron

<br>
ì´ì œ Layerë¥¼ í–‰ë ¬ë¡œ í‘œí˜„í•´ ë³´ì.  
<img src="images/mlp/mlp_vector.png" width='80%'>
$\space$
<center><font size='4'><b>ë³µì¡í•´ ë³´ì—¬ë„ ë‹¨ì§€ í–‰ë ¬ì—°ì‚°ì˜ ì‘ìš©ì¼ ë¿!</b></font></center>
    
$\space$
\begin{equation}
\overbrace{ \begin{bmatrix} \color{red}3 \end{bmatrix} }^{INPUT}
\overbrace{ \begin{bmatrix} \color{red}3 \times \color{blue}4 \end{bmatrix} }^{1st \space Layer} 
\overbrace{ \begin{bmatrix} \color{blue}4 \times 2 \end{bmatrix} }^{2nd \space Layer} 
= \begin{bmatrix} 2 \end{bmatrix}
\end{equation}
$\space$

\begin{equation}
\overbrace{ \begin{bmatrix} Batch \times \color{red}3 \end{bmatrix} }^{INPUT}
\overbrace{ \begin{bmatrix} \color{red}3 \times \color{blue}4 \end{bmatrix} }^{1st \space Layer} 
\overbrace{ \begin{bmatrix} \color{blue}4 \times 2 \end{bmatrix} }^{2nd \space Layer} 
= \begin{bmatrix} Batch \times 2 \end{bmatrix}
\end{equation}  
$\space$
<div class="alert alert-block alert-warning">
<b>$\divideontimes$ í–‰ë ¬ê³¼ ì‚¬ìƒ: </b> $[m \times n]$ì€ $n$ ì°¨ì› ê³µê°„ì—ì„œ $m$ ì°¨ì› ê³µê°„ìœ¼ë¡œì˜ mapping ( $L:N \rightarrow M$ )    
$\space$
$\space$
<font size='5'>$$\underbrace{Y}_{ [m] } = \underbrace{A}_{ [m \times n] } \underbrace{X}_{ [n] }$$  </font>
</div>  

$m$ ì°¨ì›ì—ì„œ $n$ ì°¨ì›ìœ¼ë¡œì˜ ì‚¬ìƒì€,
<font size='5'>$$\underbrace{Y}_{ [n] } = \underbrace{X}_{ [m] } \underbrace{A}_{ [m \times n] }$$  </font>

<div align="right"><font color='red'><b>* $y=ax$ì˜ ìˆœì„œì™€ ë‹¤ë¥¸ ì ì— ì£¼ì˜.</b></font></div>


### Input

<br>
$m$ ì°¨ì›ì—ì„œ $n$ ì°¨ì›ìœ¼ë¡œì˜ ì‚¬ìƒ = í–‰ë ¬ = Layerì˜ ì˜ë¯¸ë¥¼ ì•Œì•˜ìœ¼ë‹ˆ, 
ì´ì œëŠ” `Input` $m$ ê³¼ `Output` $n$ ì— ëŒ€í•´ ì•Œì•„ë³´ì.
<br>
<br>
ìš°ë¦¬ì˜ ì²« Layer <b>$A$</b> ëŠ” í•œ ë²ˆì— $\color{red}3$ì°¨ì›ì˜ Data $X$ë¥¼ ì†Œí™”í•˜ëŠ” í–‰ë ¬ì´ë‹¤.  
í•˜ì§€ë§Œ, $[2 \times \color{red}3]$ ì²˜ëŸ¼ <font color='#318CE7'><b><i>í–‰ë ¬ì˜ í˜•íƒœë¡œ í•œ ë²ˆì— ë³‘ë ¬ì²˜ë¦¬ í•  ìˆ˜ë„ ìˆë‹¤.</i></b></font></font>

ì´ ë•Œì˜ Data ê°œìˆ˜ëŠ” `Output` ê¹Œì§€ ìœ ì§€ëœë‹¤.

$\space$
\begin{equation}
\overbrace{ \begin{bmatrix} Batch \times \color{red}3 \end{bmatrix} }^{INPUT}
\begin{bmatrix} \color{red}3 \times \color{blue}4 \end{bmatrix}
\begin{bmatrix} \color{blue}4 \times 2 \end{bmatrix}
= \overbrace{ \begin{bmatrix} Batch \times 2 \end{bmatrix} }^{OUTPUT}
\end{equation}  
$\space$


<font size='5'>$$\underbrace{Y}_{ [B \times \color{blue}n] } = \underbrace{X}_{ [B \times \color{red}m] } \underbrace{A}_{ [\color{red}m \times \color{blue}n] }$$  </font>

<br>
<font size='3'>ì´ì œ ìš°ë¦¬ëŠ” ì—´ë²¡í„° 1ê°œì”©ì„ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, <font color='#318CE7'><b><i>Batchë¡œ ì—¬ëŸ¬ Inputì„ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤</i></b></font></font>

<div class="alert alert-block alert-warning">
<b>$\divideontimes$ Inputì˜ í˜•íƒœ: </b> $[2 \times 3]$ì€ $3$ì°¨ì›ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„° $2$ê°œë¥¼ ì˜ë¯¸í•œë‹¤.
<br>
<br>
<font size='5'>$$[2 \times 3]=[Data \space ê°œìˆ˜, Data \space ì°¨ì›]$$  </font>

</div>  


```python
arr_x = train_x_mlp[:2].round(2)
arr_x1 = train_x_mlp[:1].round(2)

aprint(arr_x, arr_x1, name_list=['arr_x', 'arr_x_one'])
```

    ================================================
    |  arr_x               |   arr_x_one           |
    |  (2, 3)              |   (1, 3)              |
    ================================================
    |  [[0.93 0.65 0.03]   |   [[0.93 0.65 0.03]]  |
    |   [0.93 0.56 0.02]]  |                       |
    ================================================


### Output


```python
arr_y = train_y_mlp[:2].round(2)
arr_y1 = train_y_mlp[:1].round(2)

aprint(arr_y, arr_y1, name_list=['arr_y', 'arr_y_one'])
```

    ======================================
    |  arr_y          |   arr_y_one      |
    |  (2, 2)         |   (1, 2)         |
    ======================================
    |  [[0.93 0.56]   |   [[0.93 0.56]]  |
    |   [0.94 0.62]]  |                  |
    ======================================


### Layers

<br>
ì´ì œ Dataë„ í–‰ë ¬ë¡œ í‘œí˜„í•´ ë³´ì.  
<img src="images/mlp/mlp_batch.png" style="width:80%;">
ì²«ì§¸ LayerëŠ” $\color{red}3$ì°¨ì›ì˜ Data $X$ë¥¼ $\color{blue}4$ì°¨ì›ìœ¼ë¡œ í™•ì¥í•˜ê³ ,  
ë‘˜ì§¸ LayerëŠ” $\color{blue}4$ì°¨ì›ì„ $\color{black}2$ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ê²Œ ëœë‹¤.

$\space$
\begin{equation}
\begin{bmatrix} Batch \times \color{red}3 \end{bmatrix}
\overbrace{ \begin{bmatrix} \color{red}3 \times \color{blue}4 \end{bmatrix} }^{1st \space Layer}
\overbrace{ \begin{bmatrix} \color{blue}4 \times 2 \end{bmatrix} }^{2nd \space Layer}
= \begin{bmatrix} Batch \times 2 \end{bmatrix}
\end{equation}  
$\space$

<font size='5'>$$\underbrace{Y}_{ [B \times \color{blue}n] } = \underbrace{X}_{ [B \times \color{red}m] } \underbrace{A}_{ [\color{red}m \times \color{blue}n] }$$  </font>

<br>
ì´ ê¸°ë³¸ Layerì˜ í˜•íƒœë¥¼ `Fully Connected Layer` ë˜ëŠ” `Dense Layer`ë¼ê³  ë¶€ë¥¸ë‹¤.  
<b>Layerë¥¼ êµ¬ì„±í•˜ê¸° ìœ„í•´ì„œëŠ” $\color{red}m$ê³¼ $\color{blue}n$ì„ ì •í•´ ì£¼ì–´ì•¼ í•œë‹¤.</b>  
ê°’ì„ ì •í•˜ê³  ë‚˜ë©´, ì„ì˜ì˜ ì´ˆê¸°ê°’ìœ¼ë¡œ í–‰ë ¬ì„ ë§Œë“¤ì–´ Layerë¥¼ ì²˜ìŒ êµ¬ì„±í•˜ê²Œ ëœë‹¤.

ì´ ë•Œ, $\color{blue}n$ì— í•´ë‹¹í•˜ëŠ” $ \color{blue}4$ì°¨ì›ì„ Hidden Layerì˜ ì°¨ì›ì´ë¼ê³  í•œë‹¤.  
ì¦‰ LayerëŠ” <b>$m$ ì°¨ì› Inputìœ¼ë¡œ $n$ ì°¨ì› Outputì„ ë§Œë“¤ì–´ ë‚´ëŠ” í–‰ë ¬ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.</b>
<font size='3'>ì´ì œ ìš°ë¦¬ëŠ”, <font color='#318CE7'> <b><i> ì ì´ ì•„ë‹Œ ì„ ì„ Layerë¡œ ë³¼ ìˆ˜ ìˆë‹¤.</i></b></font></font>

<div class="alert alert-block alert-warning">
<b>$\divideontimes$ Layerì˜ í˜•íƒœ: </b> <b>$m$ ì°¨ì› Inputìœ¼ë¡œ $n$ ì°¨ì› Outputì„ ë§Œë“¤ì–´ ë‚´ëŠ” í–‰ë ¬</b>

<br>
<br>
<font size='5'>$$[3 \times 4]=[Input \space Data \space ì°¨ì›, Hidden \space ì°¨ì›]$$  </font>

</div>  

ê¸°ë³¸ LayerëŠ” `Fully Connected Layer` ë˜ëŠ” `Dense Layer`ì´ë©°,
`input`ê³¼ `output shape`ë¥¼ ë³€ìˆ˜ë¡œ ë°›ì•„ ìƒì„±.

* <font size='3'>Input Shape : $m$</font>
* <font size='3'>Output Shape : $l$</font>


<font size='4'>
\begin{equation}
\begin{bmatrix} n\times m \end{bmatrix}
\overbrace{ \begin{bmatrix} m\times l \end{bmatrix} }^{Layer} 
= \begin{bmatrix} n\times l \end{bmatrix}
\end{equation}
</font>

$\space$
$\space$

<font size='4'>
\begin{equation}
\left({\begin{array}{cc} \color{red}a & \color{red}b & \color{red}c\\\color{red}d & \color{red}e & \color{red}f \end{array}}\right)
\overbrace{ \left( \begin{array}{cc} \color{blue}  A & \color{blue} D & \color{blue} G\\ \color{blue} B & \color{blue} E & \color{blue} H \\ \color{blue} C & \color{blue} F & \color{blue} I \end{array} \right) }^{Layer}
= \left(\begin{array}{cc} 
\color{red}a \color{blue}A + \color{red}b \color{blue}B + \color{red}c \color{blue}C & 
\color{red}a \color{blue}D + \color{red}b \color{blue}E + \color{red}c \color{blue}F &
\color{red}a \color{blue}G + \color{red}b \color{blue}H + \color{red}c \color{blue}I
\\
\color{red}d \color{blue}A + \color{red}e \color{blue}B + \color{red}f \color{blue}C & 
\color{red}d \color{blue}D + \color{red}e \color{blue}E + \color{red}f \color{blue}F & 
\color{red}d \color{blue}G + \color{red}e \color{blue}H + \color{red}f \color{blue}I
\\
\end{array}\right)
\end{equation}
</font>

#### Dense Layer (Fully Connected Layer)


```python
def dense_layer(
    input_x,
    output_dim=None,
    name=None,
    seed=1,
    ):
    input_dim = input_x.shape[-1]
    np.random.seed(seed)
    
    
    #==== Essential ========================================================#

    weight = np.random.random((input_dim, output_dim)).round(2)

    output = input_x @ weight
    
    #=======================================================================#
    
    
    print(name)
    aprint(input_x, weight, output, name_list=['Input', 'Weight', 'Output'])

    return output, weight
```


```python
dense_layer(arr_x, output_dim=4)
```

    None
    ==========================================================================================================
    |  Input               |   Weight                   |   Output                                           |
    |  (2, 3)              |   (3, 4)                   |   (2, 4)                                           |
    ==========================================================================================================
    |  [[0.93 0.65 0.03]   |   [[0.42 0.72 0.   0.3 ]   |   [[0.5001     0.7443     0.1361     0.52719999]   |
    |   [0.93 0.56 0.02]]  |    [0.15 0.09 0.19 0.35]   |    [0.4826     0.73080001 0.1148     0.4888    ]]  |
    |                      |    [0.4  0.54 0.42 0.69]]  |                                                    |
    ==========================================================================================================





    (array([[0.5001    , 0.7443    , 0.1361    , 0.52719999],
            [0.4826    , 0.73080001, 0.1148    , 0.4888    ]]),
     array([[0.42, 0.72, 0.  , 0.3 ],
            [0.15, 0.09, 0.19, 0.35],
            [0.4 , 0.54, 0.42, 0.69]]))



#### Initialization

Layerë¥¼ ìƒì„±í•œë‹¤ëŠ” ê²ƒì€ í–‰ë ¬ì„ ìƒì„±í•œë‹¤ëŠ” ê²ƒê³¼ ê°™ë‹¤.  
ì´ ë•Œ ì„ì˜ì˜ ê°’ì„ ì´ìš©í•´ í–‰ë ¬ì„ ìƒì„±í•˜ì—¬ì•¼ í•˜ëŠ”ë°, ì´ ì´ˆê¸°ê°’ì„ ë¶€ì—¬í•˜ëŠ” ê³¼ì •ì„ `Initialization` ì´ë¼ê³  í•¨.


```python
np.random.seed(1)
np.random.random((3, 4)).round(2)
```




    array([[0.42, 0.72, 0.  , 0.3 ],
           [0.15, 0.09, 0.19, 0.35],
           [0.4 , 0.54, 0.42, 0.69]])



### Activation Function

Layer ê°„ì˜ ì„ í˜• ê²°í•©ì€ ì„ í˜• í•¨ìˆ˜(1ì°¨ í•¨ìˆ˜)ì˜ í˜•íƒœë¡œë§Œ ì¡´ì¬í•œë‹¤.

\begin{equation}
y = f(x) = ax,\\  
z = g(y) = by,\\
 \\
z = g(f(x)) = b(ax) = abx
\end{equation}

$c = ab$ ë¼ê³  í•˜ë©´
<font size='4'>$$z = h(x) = cx$$</font>

ê°€ ë˜ì–´ ì‚¬ì‹¤ìƒ 2ê°œë¥¼ ê²¹ì¹œ íš¨ê³¼ê°€ ì—†ì–´ì§„ë‹¤.  
ê·¸ëŸ¬ë¯€ë¡œ, Layer ì‚¬ì´ì— ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ë¼ì›Œ ë„£ì–´ 
$y=f(x)$ ë¥¼ ë¹„ì„ í˜• ê´€ê³„ë¡œ ë§Œë“¤ê²Œ ëœë‹¤.  

ë˜í•œ, Output ê°’ì˜ ë²”ìœ„ë¥¼ $[0, 1]$ ì‚¬ì´ë¡œ ì œí•œí•˜ëŠ” íš¨ê³¼ë„ ìˆë‹¤. (`sigmoid`)

#### Activations & Its Derivatives


```python
examples.activation_plot()
```

    /opt/conda/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:457: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure
      "matplotlib is currently using a non-GUI backend, "



![png](output_86_1.png)


`sigmoid` í•¨ìˆ˜ëŠ” `tanh`ì— ë¹„í•´ ê¸°ìš¸ê¸°ê°€ ì‘ë‹¤.  
ë¯¸ë¶„ê°’ì˜ ìµœëŒ€ì¹˜ê°€ 0.25ì´ë©°, ë¯¸ë¶„ê°’ì´ 1ì¸ `tanh` í•¨ìˆ˜ì— ë¹„í•´ $\frac {1}{4}$ ìˆ˜ì¤€ì´ë‹¤.

<font color='#318CE7'> <b><i> ì´ ê¸°ìš¸ê¸°ëŠ” $x$ê°’ì˜ ì°¨ì´ì— ë¹„í•´ $y$ê°’ì˜ ì°¨ì´ê°€ ì‘ë‹¤ëŠ” ë§ì´ë©°, í•™ìŠµ ì§„ë„ê°€ ëŠë¦° ì›ì¸ì´ ëœë‹¤.  </i></b></font>

ë•Œë¬¸ì—, ë¯¸ë¶„ê°’ì´ ì»¤ì„œ í•™ìŠµ Impactê°€ í° `tanh`ë‚˜ `relu`ê°€ ë“±ì¥í•˜ê²Œ ëœë‹¤.


```python
examples.d_activation_plot()
```

    /opt/conda/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:457: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure
      "matplotlib is currently using a non-GUI backend, "



![png](output_88_1.png)


#### Biases

<div class="alert alert-block alert-success">
<b>$\divideontimes$ `bias`ê°€ ê°–ëŠ” ì˜ë¯¸ </b>  
<n></n>  

`weight`ê°€ $x$ ê°„ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•œë‹¤ë©´, `bias`ëŠ” ì˜ì ì¡°ì ˆì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.  
`bias`ê°€ ì—†ìœ¼ë©´, $x$ì˜ ì°¨ì›ì— ê´€ê³„ì—†ì´ ëª¨ë“  í•¨ìˆ˜ëŠ” ì›ì  <font color="black">$(0, 0)$</font> ì„ ì§€ë‚˜ê²Œ ë˜ì–´ ë°ì´í„° í•™ìŠµì— ì§€ì¥ì„ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤.
</div>

![mifeel_gunfeel](images/bias_1.png)

---
* $y = 2x$


```python
examples.draw_without_bias()
```


![png](output_91_0.png)


---
* $y=2x-4$


```python
examples.draw_with_bias()
```


![png](output_93_0.png)


### Complete Network

`Input`ê³¼ `Layer`, `Activation`ì„ í™œìš©í•˜ì—¬ ê°„ë‹¨í•œ `Multi Layer Perceptron` ëª¨ë¸ì„ ìƒì„±í•´ ë³´ì.

<img src="images/mlp/mlp_base.png" style="width:400px;height:400px;align:left;">


```python
# Input Layer
input_layer = arr_x1


# Hidden Layer
hidden_layer_1, w1 = dense_layer(
    input_layer,
    output_dim=4,
    seed=4,
    name='fc1_layer',
)
activated_1 = sigmoid(hidden_layer_1)


# Output Layer
hidden_layer_2, w2 = dense_layer(
    hidden_layer_1,
    output_dim=2,
    seed=3,
    name='fc2_layer',
)
activated_2 = sigmoid(hidden_layer_2)
```

    fc1_layer
    ==========================================================================================================
    |  Input               |   Weight                   |   Output                                           |
    |  (1, 3)              |   (3, 4)                   |   (1, 4)                                           |
    ==========================================================================================================
    |  [[0.93 0.65 0.03]]  |   [[0.97 0.55 0.97 0.71]   |   [[1.36459999 0.6674     1.56249998 0.6728    ]]  |
    |                      |    [0.7  0.22 0.98 0.01]   |                                                    |
    |                      |    [0.25 0.43 0.78 0.2 ]]  |                                                    |
    ==========================================================================================================
    sigmoid
    ==========================================================================================================
    |  Raw                                              |   Activated                                        |
    |  (1, 4)                                           |   (1, 4)                                           |
    ==========================================================================================================
    |  [[1.36459999 0.6674     1.56249998 0.6728    ]]  |   [[0.7965063  0.66092073 0.82671179 0.66212984]]  |
    ==========================================================================================================
    fc2_layer
    =======================================================================================================
    |  Input                                            |   Weight         |   Output                     |
    |  (1, 4)                                           |   (4, 2)         |   (1, 2)                     |
    =======================================================================================================
    |  [[1.36459999 0.6674     1.56249998 0.6728    ]]  |   [[0.55 0.71]   |   [[2.42216498 2.85677798]]  |
    |                                                   |    [0.29 0.51]   |                              |
    |                                                   |    [0.89 0.9 ]   |                              |
    |                                                   |    [0.13 0.21]]  |                              |
    =======================================================================================================
    sigmoid
    ==============================================================
    |  Raw                        |   Activated                  |
    |  (1, 2)                     |   (1, 2)                     |
    ==============================================================
    |  [[2.42216498 2.85677798]]  |   [[0.91850195 0.94566799]]  |
    ==============================================================



```python
hidden_layer_2
```




    array([[2.42216498, 2.85677798]])




```python
activated_2
```




    array([[0.91850195, 0.94566799]])



### Batch

<br>
ì´ì œ Batchì— ëŒ€í•´ ì•Œì•„ë³´ì.
<img src="images/mlp/mlp_batch_full.png" style="width:80%">
<br>
<font size="3">BatchëŠ” í•œë²ˆ í•™ìŠµì— ì´ìš©í•˜ëŠ” ìƒ˜í”Œ ë‹¨ìœ„ë¥¼ ë§í•œë‹¤.</font><br>
<br>
ì „ì²´ Inputì˜ ê°œìˆ˜ê°€ 10ì´ê³  `batch_size`ê°€ 2ì¼ ê²½ìš°, $\space\space$<font size='4'><b> $\leftarrow$ input_size: 10, batch_size: 2</b></font><br>
ë¶€ë¶„ì§‘í•©ì¸ 2ê°œì”©ì„ êº¼ë‚´ì–´ 1ë²ˆ í•™ìŠµì— í™œìš©í•˜ê²Œ ë˜ê³  $\space\space$<font size='4'><b> $\leftarrow$ step_num: 1 (ëˆ„ì í•™ìŠµíšŸìˆ˜)</b></font><br> 
ì´ 5ë²ˆ í•™ìŠµí•˜ë©´ ì „ì²´ Inputì„ ë‹¤ í•™ìŠµí•˜ê²Œ ëœë‹¤. $\space\space$<font size='4'><b> $\leftarrow$ batch_nu: 5 (í•™ìŠµíšŸìˆ˜)</b></font><br>
ì´ë ‡ê²Œ Inputì„ í•œë²ˆ ë‹¤ ë³´ëŠ” ê²½ìš°ë¥¼ 1 epochì´ë¼ í•˜ë©°, $\space\space$<font size='4'><b> $\leftarrow$ epoch_num: 1</b></font><br>
<br>

<font size="3">10 epochë¡œ í•™ìŠµí•˜ëŠ” ê²½ìš°  
`batch_num`ì€ 5, `step_num`ì€ 50ì´ ëœë‹¤.</font>

<br>
<br>
$\space$
$\space 1 \space Epoch,$
\begin{equation}
\begin{bmatrix} 1st \space Batch \times \color{red}3 \end{bmatrix}
\overbrace{ \begin{bmatrix} \color{red}3 \times \color{blue}4 \end{bmatrix} }^{1st Layer}
\overbrace{ \begin{bmatrix} \color{blue}4 \times 2 \end{bmatrix} }^{2nd Layer}
= \begin{bmatrix} 1st \space Batch \times 2 \end{bmatrix}
\Rightarrow 1st \space Backpropagation
\end{equation}  
\begin{equation}
\begin{bmatrix} 2nd \space Batch \times \color{red}3 \end{bmatrix}
\overbrace{ \begin{bmatrix} \color{red}3 \times \color{blue}4 \end{bmatrix} }^{1st Layer}
\overbrace{ \begin{bmatrix} \color{blue}4 \times 2 \end{bmatrix} }^{2nd Layer}
= \begin{bmatrix} 2nd \space Batch \times 2 \end{bmatrix}
\Rightarrow 2nd \space Backpropagation
\end{equation}  
$$\vdots$$
\begin{equation}
\begin{bmatrix} 5th \space Batch \times \color{red}3 \end{bmatrix}
\overbrace{ \begin{bmatrix} \color{red}3 \times \color{blue}4 \end{bmatrix} }^{1st Layer}
\overbrace{ \begin{bmatrix} \color{blue}4 \times 2 \end{bmatrix} }^{2nd Layer}
= \begin{bmatrix} 5th \space Batch \times 2 \end{bmatrix}
\Rightarrow 5th \space Backpropagation
\end{equation} 
$\space$

<br>
<br>

í•™ìŠµí•  ë•ŒëŠ” ì£¼ì–´ì§„ `input_size`ì— ëŒ€í•´ `batch_size`ì™€ `epoch_num`ì„ ì§€ì •í•œë‹¤.<br>
<font size='3'>ì´ì œ, <font color='#318CE7'> <b><i> ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ Inputì„ ì–´ë–»ê²Œ ì½ì–´ë“¤ì´ëŠ” ì§€ ì•Œ ìˆ˜ ìˆë‹¤.</i></b></font></font>
<br>

<div class="alert alert-block alert-warning">
<b>$\divideontimes$ Batchë€:  </b> <b>1ë²ˆ í•™ìŠµì— í™œìš©í•˜ëŠ” Inputì˜ ë¶€ë¶„ì§‘í•© ë‹¨ìœ„</b>

<br>
<br>
$$Batch \space num = \frac {Input \space size}{Batch \space size}$$  

$$1 \space Epoch = Batch \space size \times Batch \space num$$  

$$ Step \space num = Epoch \space num \times Batch \space num$$  
<br>
</div>  


```python
# Input Layer
input_layer = arr_x1


# Hidden Layer
hidden_layer_1, w1 = dense_layer(
    input_layer,
    output_dim=4,
    seed=4,
    name='fc1_layer',
)
activated_1 = sigmoid(hidden_layer_1)


# Output Layer
hidden_layer_2, w2 = dense_layer(
    hidden_layer_1,
    output_dim=2,
    seed=3,
    name='fc2_layer',
)
activated_2 = sigmoid(hidden_layer_2)
```

    fc1_layer
    ==========================================================================================================
    |  Input               |   Weight                   |   Output                                           |
    |  (1, 3)              |   (3, 4)                   |   (1, 4)                                           |
    ==========================================================================================================
    |  [[0.93 0.65 0.03]]  |   [[0.97 0.55 0.97 0.71]   |   [[1.36459999 0.6674     1.56249998 0.6728    ]]  |
    |                      |    [0.7  0.22 0.98 0.01]   |                                                    |
    |                      |    [0.25 0.43 0.78 0.2 ]]  |                                                    |
    ==========================================================================================================
    sigmoid
    ==========================================================================================================
    |  Raw                                              |   Activated                                        |
    |  (1, 4)                                           |   (1, 4)                                           |
    ==========================================================================================================
    |  [[1.36459999 0.6674     1.56249998 0.6728    ]]  |   [[0.7965063  0.66092073 0.82671179 0.66212984]]  |
    ==========================================================================================================
    fc2_layer
    =======================================================================================================
    |  Input                                            |   Weight         |   Output                     |
    |  (1, 4)                                           |   (4, 2)         |   (1, 2)                     |
    =======================================================================================================
    |  [[1.36459999 0.6674     1.56249998 0.6728    ]]  |   [[0.55 0.71]   |   [[2.42216498 2.85677798]]  |
    |                                                   |    [0.29 0.51]   |                              |
    |                                                   |    [0.89 0.9 ]   |                              |
    |                                                   |    [0.13 0.21]]  |                              |
    =======================================================================================================
    sigmoid
    ==============================================================
    |  Raw                        |   Activated                  |
    |  (1, 2)                     |   (1, 2)                     |
    ==============================================================
    |  [[2.42216498 2.85677798]]  |   [[0.91850195 0.94566799]]  |
    ==============================================================


### Loss Function

`Cost Function` í˜¹ì€ `Objective Function`ìœ¼ë¡œë„ ë¶ˆë¦°ë‹¤.  
ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ `Error` ë˜ëŠ” `Cost`ë¼ê³  ë¶€ë¥´ëŠ”ë° ì´ `Error`ì˜ ì¸¡ì •ë°©ë²•ì´ ê³§ `Loss Function`ì´ë‹¤.  
'ëª¨ë¸ì´ í•™ìŠµí•œë‹¤'ëŠ” ì˜ë¯¸ëŠ”, <font color='#318CE7'> <b><i>ì´ ê°’ì„ ìµœì†Œí™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ $w$ ì™€ $b$ ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤ëŠ” ëœ»ì´ë‹¤. </i></b></font>  
<br>
<div align="right"><font color='red'><b>* `accuracy`ë¥¼ ëª©ì í•¨ìˆ˜ë¡œ ì‚¬ìš©í•  ê²½ìš°, ì—…ë°ì´íŠ¸í•¨ì— ë”°ë¼ ê°’ì´ ë¶ˆì—°ì†ì ìœ¼ë¡œ ë³€í•˜ê¸° ë•Œë¬¸ì— ë¶€ì í•©.</b></font></div>

#### Mean Square Error

ì œì¼ ê¸°ë³¸ì ì¸ `Loss Function`ì´ë©°,  
ê°’ì˜ ì°¨ì´ë¥¼ ì œê³±í•˜ì—¬ ê·¸ í¬ê¸° ê°’ì˜ í‰ê· ì„ ê³„ì‚°í•œë‹¤.

<br>
<font size='4'>$$loss = \mathcal{L}(\hat{y}, y) = (\hat y^{(i)} - y^{(i)})^2$$</font>
<br>
<font size='4'>$$MSE = \frac {1}{m} \sum_{i=1}^{m}(\hat y_{i} - y_{i})^2 $$</font>



```python
def mean_squared_error(logits, real):
    
    
    #==== Essential ========================================================#
    
    err = ((logits - real) ** 2).mean()
    
    #=======================================================================#


    return err
```


```python
mean_squared_error(activated_2, arr_y1)
```




    0.0744360007255992



### Training

<img src='images/mlp/mlp_full_propagation.png' width='80%' align='center'>

#### Gradient Descent

`Gradient`ëŠ” 'ê²½ì‚¬ë„' ë¼ëŠ” ëœ»ì´ë©°, `Gradient Descent`ë¥¼ ì‚°ì„ ë‚´ë ¤ê°€ëŠ” ê³¼ì •ì— ë¹„ìœ í•  ìˆ˜ ìˆë‹¤.

ì‚° ì†ì—ì„œ ê¸¸ì„ ìƒì—ˆë‹¤ê³  ìƒê°í•´ ë³´ì.  

1. <font size='3'><b>ê¸¸ì°¾ê¸°</b></font>  
  ì‚°ì„ ë¹ ì ¸ë‚˜ì˜¤ê¸° ìœ„í•´ì„œëŠ” ì‚°ì˜ ê²½ì‚¬ì™€ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ë‚´ë ¤ì™€ì•¼ í•œë‹¤.  
  <font size='3'>ì´ë ‡ê²Œ ê¸¸ì„ ì°¾ëŠ” ë°©ë²•ì´ <font color='#318CE7'> <b><i> 'Gradient Descent'</i></b></font> ì´ë‹¤.</font>


2. <font size='3'><b>ì¤‘ê°„ ê³¨ì§œê¸°</b></font>  
  ì‹œì•¼ê°€ ì¢ê±°ë‚˜ ì§€ë„ê°€ ì¡°ê¸ˆë°–ì— ì—†ì„ ê²½ìš°,  
  ê²½ì‚¬ì— ì˜ì¡´í•´ì„œ ë‚´ë ¤ê°ˆ ìˆ˜ëŠ” ìˆì§€ë§Œ ì´ëŒ€ë¡œ ê°€ë©´ ëª©ì ì§€ê°€ ë‚˜ì˜¬ ì§€ í™•ì‹ í•  ìˆ˜ ì—†ë‹¤.  
  ê·€í‰ì´ ê³¨ì§œê¸°ì— ë„ì°©í•´ì„œ ì˜ì›íˆ í—¤ë§¬ ìˆ˜ë„ ìˆë‹¤.  
  <font size='3'>ì´ ë•Œ ìš°ë¦¬ëŠ” <font color='#318CE7'> <b><i> 'êµ­ì†Œ ìµœì í•´(Local Minima)'</i></b></font>ë¥¼ ì°¾ì•˜ë‹¤ê³  í•œë‹¤.</font>


3. <font size='3'><b>ìµœì¢… ëª©ì ì§€</b></font>  
  ì‹œì•¼ê°€ ë„“ê±°ë‚˜ ì‚°ê¸¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì˜ ì•Œê³  ìˆëŠ” ê²½ìš°, ë˜ëŠ” ìš´ì´ ì¢‹ìœ¼ë©´  
  ê°€ì•¼í•  ê³³ì„ ì˜ ì•Œê³  ë•Œë¡œëŠ” ë‹¤ì‹œ ë´‰ìš°ë¦¬ë¥¼ ì˜¤ë¥´ê¸°ë„ í•˜ë©´ì„œ ëª©ì ì§€ì— ë„ì°©í•  ìˆ˜ ìˆë‹¤.  
  <font size='3'>ì´ ë•Œ ìš°ë¦¬ëŠ” <font color='#318CE7'> <b><i> 'ì „ì—­ ìµœì í•´(Global Minimum)'</i></b></font>ë¥¼ ì°¾ì•˜ë‹¤ê³  í•œë‹¤.</font>


4. <font size='3'><b>ë³´í­</b></font>  
  ë³´í­ì´ ë„“ìœ¼ë©´, ì„±í¼ì„±í¼ ê°ˆ ìˆ˜ëŠ” ìˆê² ì§€ë§Œ ë„ˆë¬´ ë¹¨ë¼ì„œ ê¸¸ì„ ì§€ë‚˜ì¹  ìˆ˜ ìˆë‹¤.  
  ë³´í­ì´ ì¢ìœ¼ë©´, ëª©ì ì§€ë¥¼ ì˜ ì•Œì•„ë„ ë„ˆë¬´ ëŠë ¤ì„œ ê°€ë‹¤ ì§€ì¹  ìˆ˜ ìˆë‹¤.  
  ê°€ì•¼í•  ê³³ì„ ì˜ ì•Œê³  ë•Œë¡œëŠ” ë‹¤ì‹œ ë´‰ìš°ë¦¬ë¥¼ ì˜¤ë¥´ê¸°ë„ í•˜ë©´ì„œ ëª©ì ì§€ì— ë„ì°©í•  ìˆ˜ ìˆë‹¤.  
  <font size='3'>ì´ ë³´í­ì´ <font color='#318CE7'> <b><i> 'í•™ìŠµë¥ (Learning Rate)'</i></b></font>ì´ë‹¤.</font>



<img src='images/gradient_minima.png' width='100%' align='center'>




```python
examples.gradient_plot_a()
examples.gradient_plot_b()
```

    /opt/conda/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:457: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure
      "matplotlib is currently using a non-GUI backend, "
    /opt/conda/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:457: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure
      "matplotlib is currently using a non-GUI backend, "



![png](output_109_1.png)



![png](output_109_2.png)


#### Forward-propagation

<img src='images/mlp/mlp_forward.png' width='80%'>

* Dense Layer

* Activation Layer

##### Forward-propagation


```python
from src.layers import dense_layer, sigmoid, tanh


def forward_propagation(
    input_x,
    weight_dict=None,
    print_ok=True,
    ):

    if weight_dict is None:
        weight_dict = {
            'w1': None,
            'b1': None,
            'w2': None,
            'b2': None,
        }
    
    w1 = weight_dict['w1']
    b1 = weight_dict['b1']
    w2 = weight_dict['w2']
    b2 = weight_dict['b2']

    
    #==== Essential ========================================================#

    ##### LAYER (Forward-propagation) #####

    # Input Layer
    input_layer = input_x

    # Hidden Layer
    hidden_layer_1, w1, b1 = dense_layer(
        input_layer,
        output_dim=4,
        weight=w1,
        bias=b1,
        seed=4,
        name='fc1_layer',
        print_ok=print_ok,
    )
    activated_1 = sigmoid(hidden_layer_1, print_ok=print_ok)

    # Output Layer
    hidden_layer_2, w2, b2 = dense_layer(
        hidden_layer_1,
        output_dim=2,
        weight=w2,
        bias=b2,
        seed=3,
        name='fc2_layer',
        print_ok=print_ok,
    )
    activated_2 = sigmoid(hidden_layer_2, print_ok=print_ok)
    
    #=======================================================================#
    
    
    if print_ok:
        print(f'\n= Input =\n{input_layer}')
        print(f'\n= 1 Layer =\n{hidden_layer_1}\n{activated_1}')
        print(f'\n= 2 Layer =\n{hidden_layer_2}')
        print(f'\n= Output =\n{activated_2}')


    weight_dict['w1'] = w1
    weight_dict['b1'] = b1
    weight_dict['l1'] = hidden_layer_1
    weight_dict['a1'] = activated_1
    
    
    weight_dict['w2'] = w2
    weight_dict['b2'] = b2
    weight_dict['l2'] = hidden_layer_2
    weight_dict['a2'] = activated_2

    return activated_2, weight_dict
```


```python
output, w_dict = forward_propagation(arr_x1)
```

    fc1_layer:	(1, 3) -> (1, 4)
    sigmoid
    ==========================================================================================================
    |  Raw                                              |   Activated                                        |
    |  (1, 4)                                           |   (1, 4)                                           |
    ==========================================================================================================
    |  [[2.22459999 1.6474     1.72249998 1.2728    ]]  |   [[0.90243695 0.83853934 0.84845057 0.78122169]]  |
    ==========================================================================================================
    fc2_layer:	(1, 4) -> (1, 2)
    sigmoid
    ==============================================================
    |  Raw                        |   Activated                  |
    |  (1, 2)                     |   (1, 2)                     |
    ==============================================================
    |  [[3.44976498 4.67717798]]  |   [[0.96922413 0.99078055]]  |
    ==============================================================
    
    = Input =
    [[0.93 0.65 0.03]]
    
    = 1 Layer =
    [[2.22459999 1.6474     1.72249998 1.2728    ]]
    [[0.90243695 0.83853934 0.84845057 0.78122169]]
    
    = 2 Layer =
    [[3.44976498 4.67717798]]
    
    = Output =
    [[0.96922413 0.99078055]]


#### Backpropagation

<img src='images/mlp/mlp_backward.png' width='80%'>

<br>
<div class="alert alert-block alert-warning">
<b>$\divideontimes$ ì—­ì „íŒŒ(Backpropagation): </b> 

<br>
<br>
<font size='3'><center>ë§ˆì§€ë§‰ Outputê³¼ ì‹¤ì œ ê°’ê³¼ì˜ ì°¨ì´( $Loss$ )ë¡œ ë°°ìš´ ê²½í—˜ì„  
ë‚´ë¶€ $Weight$ì— ê³¨ê³ ë£¨ Feedbackí•˜ëŠ” ê³¼ì •.</center></font>
<br>
</div>  

<br>

<div class="alert alert-block alert-success">
<b>$\divideontimes$ Chain Rule: </b> í•©ì„±í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜ì— ëŒ€í•œ ê³µì‹ì´ë‹¤.
$$
z = g(f((x)) \space ì¼ \space ë•Œ \\
\space \\
\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y}\frac{\partial y}{\partial x}
$$

ëª¨ë¸ì€ Layerë“¤ ê°„ì˜ í•©ì„±ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.  
ì¦‰, ëª¨ë¸ ì „ì²´ë¥¼ ë¯¸ë¶„í•œ ê°’ì€ Layer ê°ê° ë¯¸ë¶„ê°’ì˜ ê³±ê³¼ ê°™ë‹¤.

$$
\frac{\partial z}{\partial w} = \frac{\partial z}{\partial y}\frac{\partial y}{\partial x}\frac{\partial x}{\partial w}
$$
<br>

</div>

<br>

<br>
<font size='3'>$$ \frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}$$</font>

$J$ëŠ” $Error$ë¥¼, $\theta$ëŠ” $Weight$ë¥¼ ì˜ë¯¸í•œë‹¤.  
ì¦‰,  <font size='3'>$\frac{\partial J}{\partial \theta}$</font>ëŠ” $Weight$ ë³€í™”ëŸ‰ ëŒ€ë¹„ $Error$ ê°’ì˜ ë³€í™”ëŸ‰ì„ ì˜ë¯¸í•œë‹¤.

<font size='3'>
$$
J = MSE = \frac{1}{m}\sum\limits_{i=1}^m {(\hat{y} - y)^2}
$$

<br>

<img src="images/backprop.png" style=";align:left;">

<div class="alert alert-block alert-success">
<b>$\divideontimes$ Derivative of Sigmoid </b>
<br>
<br>
ë¹„ì„ í˜•í•¨ìˆ˜ë¡œ <b><i>'sigmoid'</i></b>ê°€ ìì£¼ ì“°ì´ëŠ” ì´ìœ  ì¤‘ í•˜ë‚˜ëŠ” ë„í•¨ìˆ˜ê°€ ê°„ë‹¨í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

<br>
<br>
\begin{array}{ll} \hline
Name
&
Formula
&
Derivative
\\ \hline
sigmoid
&
\sigma(x) = \dfrac{1}{1+e^{-x}}
&
\sigma(x)(1 - \sigma(x))
\\ \hline
tanh
&
2\sigma(2x) - 1
&
1-\tanh(x)^2
\\ \hline
\end{array}



```python
def d_sigmoid(y):
    return y * (1 - y)
```

##### Backpropagation (Scratch)


```python
from src.layers import d_sigmoid, d_tanh


def backward_propagation_scratch(
    input_x,
    input_y,
    weight_dict=None,
    learning_rate=5e-2,
    print_ok=True,
    ):
    
    m = input_dim = input_x.shape[-1]

    w1 = weight_dict['w1']
    b1 = weight_dict['b1']
    l1 = weight_dict['l1']
    a1 = weight_dict['a1']
    
    w2 = weight_dict['w2']
    b2 = weight_dict['b2']
    l2 = weight_dict['l2']
    a2 = weight_dict['a2']
    
    
    #==== Essential ========================================================#

    ##### LAYER (Back-propagation) #####

    # Calculate Gradients
    err = a2 - input_y
    d_a2 = err / d_sigmoid(a2)
    d_l2 = a2 - input_y
    d_b2 = 1./m * np.sum(d_l2.T, axis=1, keepdims=False)
    d_o2 = d_l2

    d_w2 = 1./m * (a1.T @ d_o2)
    d_a1 = d_o2 @ w2.T
    d_l1 = d_sigmoid(a1)
    d_b1 = 1./m * np.sum(d_l1.T, axis=1, keepdims=False)
    d_o1 = d_l1
    d_w1 = 1./m * (input_x.T @ d_o1)

    
    # Update
    new_w1 = w1 - (learning_rate * d_w1)
    new_b1 = b1 - (learning_rate * d_b1)
    new_w2 = w2 - (learning_rate * d_w2)
    new_b2 = b2 - (learning_rate * d_b2)

    #=======================================================================#

    
    if print_ok:
        print('\n\nResult: Backpropagation')
        aprint(w2, new_w2, name_list=['w2', 'new_w2'])
        aprint(b2, new_b2, name_list=['b2', 'new_b2'])
        aprint(w1, new_w1, name_list=['w1', 'new_w1'])
        aprint(b1, new_b1, name_list=['b1', 'new_b1'])

    w1 = new_w1
    b1 = new_b1
    w2 = new_w2
    b2 = new_b2


    gradient_dict = {
        'd_w1': d_w1,
        'd_b1': d_b1,
        'd_l1': d_l1,
        'd_a1': d_a1,
        
        'd_w2': d_w2,
        'd_b2': d_b2,
        'd_l2': d_l2,
        'd_a2': d_a2,
    }

    weight_dict['w1'] = new_w1
    weight_dict['b1'] = new_b1
    
    weight_dict['w2'] = new_w2
    weight_dict['b2'] = new_b2
    
    return gradient_dict, weight_dict
```


```python
output, w_dict = forward_propagation(arr_x1)
grad_dict, new_w_dict = backward_propagation_scratch(arr_x1, arr_y1, weight_dict=w_dict)
```

    fc1_layer:	(1, 3) -> (1, 4)
    sigmoid
    ==========================================================================================================
    |  Raw                                              |   Activated                                        |
    |  (1, 4)                                           |   (1, 4)                                           |
    ==========================================================================================================
    |  [[2.22459999 1.6474     1.72249998 1.2728    ]]  |   [[0.90243695 0.83853934 0.84845057 0.78122169]]  |
    ==========================================================================================================
    fc2_layer:	(1, 4) -> (1, 2)
    sigmoid
    ==============================================================
    |  Raw                        |   Activated                  |
    |  (1, 2)                     |   (1, 2)                     |
    ==============================================================
    |  [[3.44976498 4.67717798]]  |   [[0.96922413 0.99078055]]  |
    ==============================================================
    
    = Input =
    [[0.93 0.65 0.03]]
    
    = 1 Layer =
    [[2.22459999 1.6474     1.72249998 1.2728    ]]
    [[0.90243695 0.83853934 0.84845057 0.78122169]]
    
    = 2 Layer =
    [[3.44976498 4.67717798]]
    
    = Output =
    [[0.96922413 0.99078055]]
    
    
    Result: Backpropagation
    ==================================================
    |  w2             |   new_w2                     |
    |  (4, 2)         |   (4, 2)                     |
    ==================================================
    |  [[0.55 0.71]   |   [[0.54941005 0.7035208 ]   |
    |   [0.29 0.51]   |    [0.28945182 0.50397956]   |
    |   [0.89 0.9 ]   |    [0.88944534 0.8939084 ]   |
    |   [0.13 0.21]]  |    [0.12948929 0.20439108]]  |
    ==================================================
    ==============================================
    |  b2           |   new_b2                   |
    |  (2,)         |   (2,)                     |
    ==============================================
    |  [0.05 0.44]  |   [0.04934626 0.43282032]  |
    ==============================================
    ==================================================================================
    |  w1                       |   new_w1                                           |
    |  (3, 4)                   |   (3, 4)                                           |
    ==================================================================================
    |  [[0.97 0.55 0.97 0.71]   |   [[0.96681802 0.54673363 0.9667465  0.70666091]   |
    |   [0.7  0.22 0.98 0.01]   |    [0.69777604 0.21771705 0.97772605 0.00766623]   |
    |   [0.25 0.43 0.78 0.2 ]]  |    [0.24989736 0.42989463 0.77989505 0.19989229]]  |
    ==================================================================================
    ==============================================================================
    |  b1                     |   new_b1                                         |
    |  (4,)                   |   (4,)                                           |
    ==============================================================================
    |  [0.86 0.98 0.16 0.6 ]  |   [0.85657852 0.97648777 0.15650161 0.59640958]  |
    ==============================================================================


#### Optimization

<br>
<div class="alert alert-block alert-warning">
<b>$\divideontimes$ Optimizer: </b> <b>ì§€ë¦„ê¸¸ì„ ì˜ ì°¾ëŠ” ë°©ë²• </b>
<br>
<br>
<font size='3'><center><b> ëˆˆì•ì˜ ì´ë“(Local Minima)ì— ë¹ ì§€ì§€ ì•Šê³ , ìµœì ì˜ ë‹µ(Global Minima)ì„ ì˜ ì°¾ì•„ì•¼ í•œë‹¤!</b></center></font>

</div>
<br>
<br>

í•¨ìˆ˜ì˜ ê·¹ëŒ€ê°’ ë˜ëŠ” ê·¹ì†Œê°’ì„ êµ¬í•˜ê¸° ìœ„í•´ í˜„ì¬ ìœ„ì¹˜ì—ì„œ í•¨ìˆ˜ê°’ì˜ ë³€í™”ê°€ ê°€ì¥ í° ë°©í–¥ìœ¼ë¡œ ì´ë™í•œë‹¤.  
í•¨ìˆ˜ê°’ì˜ ë³€í™”ê°€ ê°€ì¥ í° ë°©í–¥ì„ êµ¬í•  ìˆ˜ë§Œ ìˆë‹¤ë©´ ë‹¤ì–‘í•œ ë¬¸ì œì— ë˜‘ê°™ì€ ê°œë…ì„ ì ìš©í•  ìˆ˜ ìˆë‹¤.

<img src="images/gradient_descent_algorithm.gif" style=";align:left;">
<img src="images/gradient_descent_algorithm2.gif" style=";align:left;">

#### Exercise : Backprop + Optimizer

ì´ì œ ë‹¤ì–‘í•œ Optimizerë¥¼ ì ìš©í•´ì„œ Backpropagationì„ ìˆ˜í–‰í•´ ë³´ì.


```python
from src.optimizer import sgd, sgd_momentum, adagrad, rmsprop, adam


def backward_propagation(
    input_x,
    input_y,
    weight_dict=None,
    param_dict=None,
    loss_func=mean_squared_error,
    optimizer=sgd,
    print_ok=True,
    ):

    if param_dict is None:
        param_dict = {}
    learning_rate = param_dict['learning_rate']
    step_num = param_dict['step_num']
    
    m = input_dim = input_x.shape[-1]

    w1 = weight_dict['w1']
    b1 = weight_dict['b1']
    l1 = weight_dict['l1']
    a1 = weight_dict['a1']
    
    w2 = weight_dict['w2']
    b2 = weight_dict['b2']
    l2 = weight_dict['l2']
    a2 = weight_dict['a2']
    
    #==== Essential 1 ======================================================#

    ##### LAYER (Back-propagation) #####

    # Calculate Gradients
    err = loss_func(a2, input_y)
    d_a2 = err / d_sigmoid(a2)
    d_l2 = a2 - input_y
    d_b2 = 1./m * np.sum(d_l2.T, axis=1, keepdims=False)
    d_o2 = d_l2

    d_w2 = 1./m * (a1.T @ d_o2)
    d_a1 = d_o2 @ w2.T
    d_l1 = d_sigmoid(a1)
    d_b1 = 1./m * np.sum(d_l1.T, axis=1, keepdims=False)
    d_o1 = d_l1
    d_w1 = 1./m * (input_x.T @ d_o1)

    #=======================================================================#

    delta_dict = {
        'w2': d_w2,
        'b2': d_b2,
        'w1': d_w1,
        'b1': d_b1,
    }

    step_num += 1

    for w_name in delta_dict:
        param_dict[w_name] = {}
        param_dict[w_name]['learning_rate'] = learning_rate
        param_dict[w_name]['step_num'] = step_num


    #==== Essential 2 ======================================================#

    # Update
    new_w_list = []
    for w_name in delta_dict:
        w, dw = w_dict[w_name], delta_dict[w_name]

        new_w, param_dict[w_name] = optimizer(w, dw, param_dict=param_dict[w_name])
        new_w_list += [new_w]

    new_w2, new_b2, new_w1, new_b1 = new_w_list

    #=======================================================================#
    
    
    if print_ok:
        print(f"\n\nResult: Backpropagation {step_num}, Optimizer = '{optimizer.__name__}'")
        aprint(w2, new_w2, name_list=['w2', 'new_w2'])
        aprint(b2, new_b2, name_list=['b2', 'new_b2'])
        aprint(w1, new_w1, name_list=['w1', 'new_w1'])
        aprint(b1, new_b1, name_list=['b1', 'new_b1'])

    weight_dict['w1'] = new_w1
    weight_dict['b1'] = new_b1
    
    weight_dict['w2'] = new_w2
    weight_dict['b2'] = new_b2
    
    param_dict['step_num'] = step_num
    param_dict['learning_rate'] = learning_rate
    

    return weight_dict, param_dict
```

#### Exercise : Training


```python
#--------------------#

"""Parameter Setting.
"""

# OPTIMIZER = sgd
# OPTIMIZER = sgd_momentum
OPTIMIZER = adagrad
# OPTIMIZER = rmsprop
# OPTIMIZER = adam

MAX_STEP = 100
LEARNING_RATE = 0.005

#--------------------#


print_ok = False
p_dict = {}
p_dict['learning_rate'] = LEARNING_RATE
p_dict['step_num'] = 0
w_dict = None
output_list = []


#==== Essential ========================================================#

for _ in range(MAX_STEP):
    output, w_dict = forward_propagation(
        arr_x1,
        weight_dict=w_dict,
        print_ok=print_ok,
    )
    w_dict, p_dict = backward_propagation(
        arr_x1,
        arr_y1,
        loss_func=mean_squared_error,
        weight_dict=w_dict,
        param_dict=p_dict,
        optimizer=OPTIMIZER,
        print_ok=print_ok,
    )
    output_list += [output]
    
#=======================================================================#


    if _ % (MAX_STEP // 10) >= 9:
        print('Step_num :', p_dict['step_num'])
        aprint(
            output_list[0],
            output,
            arr_y1,
            name_list=['1st OUTPUT', 'LAST OUTPUT', 'GROUND TRUTH'],
            decimals=3,
        )
        print('')

last_output, _ = forward_propagation(
    arr_x1,
    weight_dict=w_dict,
    print_ok=False,
)
```

    Step_num : 10
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.948 0.983]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 20
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.927 0.969]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 30
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.93  0.947]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 40
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.93  0.913]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 50
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.928 0.868]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 60
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.926 0.813]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 70
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.92  0.752]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 80
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.909 0.689]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 90
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.892 0.631]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 100
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.866 0.582]]  |   [[0.93 0.56]]  |
    =============================================================
    


### MLP Summary


RNNìœ¼ë¡œ ë„˜ì–´ê°€ê¸° ì „ì—, MLP ì‘ë™ í”„ë¡œì„¸ìŠ¤ë¥¼ í•œë²ˆ ë” í™•ì¸í•´ ë³´ì.
<br>

<font size='4'> <b> $\cdot$ ê³„ì‚° íë¦„ : MLP</b></font>
<img src='images/mlp/mlp_compute_flow.gif' width='100%'>

# References

https://wiseodd.github.io/techblog/2016/06/22/nn-optimization/  
https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21
