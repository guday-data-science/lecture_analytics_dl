
Analysis ê³¼ì • - Deep Learning
=============================

ê°•ì˜ ê°œìš”
---------

-  ì™œ ë”¥ëŸ¬ë‹ì¸ê°€?

   -  í†µê³„, ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ì˜ ì°¨ì´
   -  ë”¥ëŸ¬ë‹ìœ¼ë¡œ í’€ ìˆ˜ ìˆëŠ” ë¬¸ì œ

-  ë”¥ëŸ¬ë‹ì˜ ì›ë¦¬

   -  í•™ìŠµì´ë€?
   -  ì–´ë–»ê²Œ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ”ê°€

-  ë”¥ëŸ¬ë‹ì˜ êµ¬ì¡°

   -  ì…ë ¥ :math:`\rightarrow` ëª¨ë¸ :math:`\rightarrow` ì¶œë ¥
   -  Layer : ëª¨ë¸ì˜ êµ¬ì„±ë‹¨ìœ„
   -  Activation : ë¹„ì„ í˜• ëª¨ë¸ë¡œ ê°€ëŠ” ê¸¸
   -  Bias : ì˜ì ì¡°ì ˆ

-  í•™ìŠµìœ¼ë¡œ ëŒì•„ì™€ì„œ

   -  Train Set : ê¸°ì¶œ ë¬¸ì œì€í–‰
   -  Batch : ëª¨ì˜ê³ ì‚¬
   -  Epoch : 1íšŒë…
   -  Step : ëª¨ì˜ê³ ì‚¬ ì‘ì‹œíšŸìˆ˜
   -  Loss : ì‹œí—˜ì ìˆ˜
   -  Gradient : ê³µë¶€ë°©í–¥
   -  Optimization : ê³µë¶€ë°©ë²•
   -  Backpropagation : ì˜¤ë‹µë…¸íŠ¸, ë³µìŠµ

-  ëª¨ë¸ í‰ê°€

   -  Accuracy : ë¬¸ì œì€í–‰
   -  Batch : ëª¨ì˜ê³ ì‚¬
   -  Epoch : 1íšŒë…
   -  Step : ëª¨ì˜ê³ ì‚¬ ì‘ì‹œíšŸìˆ˜

-  ë”¥ëŸ¬ë‹ì˜ ë°œì „

   -  MLP : ë‹¤ ì—®ì–´ë³´ì! ì£„ë‹¤ ì—°ê²°ëœ ì‹ ê²½ë§ (Fully Connected), ë¹¡ë¹¡í•œ
      ì‹ ê²½ë§ (Dense)
   -  CNN : Imageë¥¼ ë¶€ë¶„ìœ¼ë¡œ ìª¼ê°œì„œ ì¡°ê¸ˆì”© ë³´ì!
   -  RNN : Data ìˆœì„œì— ì˜ë¯¸ê°€ ìˆë‹¤ë©´? ê³¼ê±°ë¥¼ ê¸°ì–µí•˜ì

      -  Advanced RNN : ê¸°ì–µë ¥ í–¥ìƒ

ê°•ì˜ ì§„í–‰ ê³¼ì •
~~~~~~~~~~~~~~

-  ê¸°ë³¸

   1. ì‹¤ìŠµ ì§„í–‰ ( Code Review & ì‘ì„± )

   -  ``keras``

   2. ê°œë… ì„¤ëª…
   3. ê°œë… ì´í•´ ( Code Review )

   -  ``numpy``

-  ì‹¬í™”

   1. ê°œë… ë°˜ë³µ
   2. ì‹¤ìŠµ ì§„í–‰

   -  ``tensorflow``

--------------

ë”¥ëŸ¬ë‹ ê°œìš”
-----------

ì™œ ë”¥ëŸ¬ë‹ì¸ê°€?
~~~~~~~~~~~~~~

.. raw:: html

   <center>

â€œê¸°ê³„í•™ìŠµ, ì‚¬ëŒì´ íŒë‹¨í•˜ê¸° í˜ë“  ê·œì¹™ë“¤ì„ ë°ì´í„° ì–‘ìœ¼ë¡œ ê·¹ë³µí•´ ë³´ì!â€

.. raw:: html

   </center>

-  M/L ê¸°ë°˜ ë°©ë²•ë¡ 

   -  ì›ë¦¬: ì„ í˜•ê·¼ì‚¬ì™€ ì •ê·œë¶„í¬ ê°€ì •ì„ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í•˜ëŠ”
      ì„ í˜•ë°©ì •ì‹ì„ ë§Œë“¤ì–´ ìƒˆë¡œìš´ Inputì— ëŒ€ì‘í•˜ëŠ” ê²°ê³¼ë¥¼ ì˜ˆì¸¡.
   -  í•™ìŠµ: OLS, GLS ë“± ìµœì†Œì œê³±í•© ë˜ëŠ” ìµœëŒ€ê°€ëŠ¥ë„ì¶”ì •, ë¶„ì‚°ê°ì†Œê¸°ë²•
      ë“±ì˜ ë‹¤ì†Œ í•™ìŠµì´ ë¹ ë¥¸ ê¸°ë²•ë“¤ì´ ì‚¬ìš©.
   -  ëª©ì : ë³€ìˆ˜ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ë‹¤ì†Œ ë³µì¡í•˜ê³  ì„¤ëª…ë ¥ì´ ë–¨ì–´ì§€ë”ë¼ë„
      ì¢‹ì€ ì˜ˆì¸¡ë ¥ì„ ê°–ëŠ” ëª¨ë¸ì„ ì§€í–¥í•˜ë©°, ê²½ìš°ì— ë”°ë¼ ì£¼ìš” ì›ì¸ë³€ìˆ˜
      ì„ ë³„ë„ ê°€ëŠ¥.
   -  í‰ê°€: ì˜¤ì°¨ì˜ í¬ê¸°, ì •í™•ë„(Accuracy) ë° Precision/Recall ë“±
      ì˜ˆì¸¡ê²°ê³¼ì— ëŒ€í•œ ì§€í‘œë“¤ì´ ì£¼ë¡œ ì‚¬ìš©.

--------------

-  D/L ê¸°ë°˜ ë°©ë²•ë¡ 

   -  ì›ë¦¬: ë¹„ì„ í˜•ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í•˜ëŠ” ê³ ì°¨ì›ì˜ ë°©ì •ì‹ì„
      ë§Œë“¤ê±°ë‚˜ ë°ì´í„° ìì²´ì˜ ë¶„í¬ë¥¼ í•™ìŠµí•˜ì—¬ ìƒˆë¡œìš´ Inputì— ëŒ€ì‘í•˜ëŠ”
      ê²°ê³¼ë¥¼ ì˜ˆì¸¡.
   -  í•™ìŠµ: ê³ ì°¨ì› í•™ìŠµë°©ë²• ì¤‘ Gradient Descent ê¸°ë°˜ì˜ ê·€ë‚©ì  ë°©ë²•ë¡ ì„
      ì‚¬ìš©.
   -  ëª©ì : ë†’ì€ ì˜ˆì¸¡ë ¥ì„ ê°–ëŠ” ëª¨ë¸ì„ ëª©í‘œë¡œ, ì„ í˜•ê·¼ì‚¬ë¡œëŠ” ì„¤ëª…í•˜ê¸° í˜ë“ 
      ë¹„ì •í˜• ë°ì´í„° ë˜ëŠ” ë³µì¡í•œ ë°ì´í„° ê°„ì˜ ê´€ê³„ ì„¤ëª….
   -  í‰ê°€: ì˜¤ì°¨ì˜ í¬ê¸°, ì •í™•ë„(Accuracy) ë° Precision/Recall ë“±
      ì˜ˆì¸¡ê²°ê³¼ì— ëŒ€í•œ ì§€í‘œë“¤ì´ ì£¼ë¡œ ì‚¬ìš©.

|  ë”¥ëŸ¬ë‹ì€ ì‚¬ì‹¤ ë‚˜ì˜¨ ì§€ 30ë…„ì´ ë„˜ì—ˆë‹¤.
| ì™œ ë‹¤ì‹œ ë”¥ëŸ¬ë‹ì´ ê°ê´‘ë°›ëŠ” ê²ƒì¼ê¹Œ?

.. container:: alert alert-block alert-info

   \ :math:`\divideontimes` ë”¥ëŸ¬ë‹ì˜ ì¬ì¡°ëª…:

   | Â \ :math:`\cdot` ê³¼ì í•© ë¬¸ì œ í•´ê²°
   | Â  Â  - Relu ë“± Vanishing Gradient ë¬¸ì œë¥¼ ì»¤ë²„í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ê³¼
     íš¨ìœ¨ì ì¸ Optimizerì˜ ë“±ì¥ìœ¼ë¡œ í•™ìŠµ ì„±ëŠ¥ ì¦ê°€
   | Â  Â  - Dropout ë“± íš¨ìœ¨ì ì¸ ê³¼ì í•© ë°©ì§€ ê¸°ë²•ì´ ë“±ì¥í•˜ë©° ê¸°ì¡´ ëª¨ë¸ì˜
     í•œê³„ ê·¹ë³µ
   | 

   | \ :math:`\cdot` í•˜ë“œì›¨ì–´ì˜ ë°œì „
   | Â  Â  - ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì˜ ë†’ì€ ì—°ì‚° ë¶€í•˜ë¥¼ ì»¤ë²„í•˜ëŠ” Computing Powerì˜
     ì¦ëŒ€ì™€ GPUë¥¼ í™œìš©í•œ ë³‘ë ¬ ì—°ì‚° ì•„í‚¤í…ì²˜ ë“±ì¥
   | 

   | \ :math:`\cdot` ë¹…ë°ì´í„° ì‹œëŒ€ì˜ ë„ë˜
   | Â  Â  - ê¸°ì¡´ ì •í˜• ë°ì´í„° ê·œëª¨ì˜ í™•ëŒ€ + ìŒì„±, ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ ë“± ë¹„ì •í˜•
     ë°ì´í„° ì²˜ë¦¬ ê¸°ìˆ ì˜ ë°œì „

   .. raw:: html

      </center>

ë”¥ëŸ¬ë‹ì˜ ì›ë¦¬
~~~~~~~~~~~~~

 ìš°ë¦¬ê°€ ë¬´ì–¸ê°€ ë°°ìš¸ ë•Œë¥¼ ìƒìƒí•´ ë³´ì.

   -  ì²« ì£¼ì‹íˆ¬ì -> ì‹¤íŒ¨ -> ìƒìƒê³¼ í˜„ì‹¤ì˜ ì°¨ì´ í™•ì¸
   -  ë‰´ìŠ¤ë¥¼ ë³´ë©´ì„œ ë§¤ë§¤ ë…¸í•˜ìš° í•™ìŠµ -> ì‹¤íŒ¨
   -  ìº”ë“¤ì°¨íŠ¸ë„ ë³´ê³ , ì¶”ì„¸ì„ ë„ ë³´ë©´ì„œ ë‚˜ë¦„ì˜ ë§¤ë§¤ ë…¸í•˜ìš° ì—…ë°ì´íŠ¸ ->
      ì‹¤íŒ¨
   -  ì¬ë¬´ì œí‘œ í™•ì¸í•˜ë©° íšŒì‚¬ì˜ ê°€ì¹˜ë¥¼ ë§¤ë§¤ ë…¸í•˜ìš°ì— ë°˜ì˜ -> ì‹¤íŒ¨ â€¦
   -  ì ì°¨ ì°¨ì´ë¥¼ ì¢í˜€ ë‚˜ê°„ë‹¤â€¦

| 

  .. math:: y = f(x)

   ë”¥ëŸ¬ë‹ë„ ê¸°ë³¸ ì›ë¦¬ëŠ” ì´ ë°©ì‹ê³¼ ìœ ì‚¬í•˜ë‹¤.
| ``Input``\ ì— í•´ë‹¹í•˜ëŠ” :math:`X`\ ë¥¼ ê³„ì‚°í•´ì„œ ë‚˜ì˜¤ëŠ” ì˜ˆìƒ
  :math:`\hat Y`\ ì™€ ì§„ì§œ ì •ë‹µ :math:`Y`\ ë¥¼ ë¹„êµí•´ì„œ, ì¡°ê¸ˆì”© ì°¨ì´ë¥¼
  ì¤„ì—¬ë‚˜ê°€ê²Œ ëœë‹¤.
| ë‹¤ì–‘í•œ Caseë¥¼ ê²½í—˜í•  ìˆ˜ë¡ ì˜ˆìƒì´ ë¹—ë‚˜ê°ˆ í™•ë¥ ë„ ë‚®ì•„ì§„ë‹¤.
|  ë”¥ëŸ¬ë‹ì´ë€, **ê³¼ê±° íŒ¨í„´ ì† ê·œì¹™ì„ ì°¾ì•„ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì´ë‹¤.**\ 

.. container:: alert alert-block alert-warning

   \ :math:`\divideontimes` ë”¥ëŸ¬ë‹ì˜ ì›ë¦¬: ìˆ˜ë§ì€ Caseì—ì„œ ê·œì¹™ì„±ì„
   ë°œê²¬í•˜ê³  í•™ìŠµí•œë‹¤. :math:`\space` :math:`\space`

   .. raw:: html

      <center>

   :math:`\hat y = f(x)` ì¼ ë•Œ,

   .. raw:: html

      </center>

   .. raw:: html

      <center>

   :math:`Real \space yì™€ \space ìœ ì‚¬í•œ \space \hat y`\ ë¥¼ ì°¾ëŠ” ê²ƒ.

   .. raw:: html

      </center>

ë”¥ëŸ¬ë‹ì˜ êµ¬ì¡°
~~~~~~~~~~~~~

.. raw:: html

   <center>

â€œLayerë¼ëŠ” í•¨ìˆ˜ë¥¼ ì—¬ëŸ¬ ê°œ ìŒ“ì€(ì´ì–´ë¶™ì¸) í˜•íƒœâ€

.. raw:: html

   </center>

.. container:: alert alert-block alert-warning

   \ :math:`\divideontimes` ë”¥ëŸ¬ë‹ì˜ êµ¬ì¡°: ì—¬ëŸ¬ ê²¹ì˜ Layerë¡œ êµ¬ì„±ëœ
   Functionê³¼ ê°™ì€ êµ¬ì¡° :math:`\space` :math:`\space`

   .. raw:: html

      <center>

   Input :math:`\rightarrow` Layer(s) :math:`\rightarrow` Output

   .. raw:: html

      </center>

ë”¥ëŸ¬ë‹ì˜ í•™ìŠµ
~~~~~~~~~~~~~

.. raw:: html

   <center>

â€œë°˜ë³µí•™ìŠµì„ í†µí•œ ë¬¸ì œìœ í˜• ìµíˆê¸°â€

.. raw:: html

   </center>

   1. ë¬¸ì œì€í–‰ì—ì„œ ë¬¸ì œë¥¼ ê³¨ë¼ì„œ
   2. ë‚˜ë¦„ì˜ ë°©ë²•ìœ¼ë¡œ ë¬¸ì œë¥¼ í’€ê³ 
   3. ì •ë‹µê³¼ ë§ì¶° ë³´ê³ 
   4. ì˜¤ë‹µì„ ì •ë¦¬í•´ì„œ
   5. ë‹¤ì‹œ ë¬¸ì œë¥¼ í’€ê³ â€¦

.. container:: alert alert-block alert-warning

   \ :math:`\divideontimes` ë”¥ëŸ¬ë‹ì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤: :math:`\space`
   :math:`\space`

   .. raw:: html

      <center>

   Training Dataë¥¼ ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµí•˜ë©´ì„œ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ Update

   .. raw:: html

      </center>

ë”¥ëŸ¬ë‹ì˜ í‰ê°€
~~~~~~~~~~~~~

.. raw:: html

   <center>

â€œì‹œí—˜ í‰ê· ìœ¼ë¡œ ì„±ì  ë§¤ê¸°ê¸°â€

.. raw:: html

   </center>

   1. ì •í•´ì§„ íšŸìˆ˜ë§Œí¼ ì‹œí—˜ì„ ì¹˜ë¥´ê³ 
   2. ì •ë‹µê³¼ ë§ì¶° ë³´ê³ 
   3. í‰ê· ì„ ë‚´ì„œ ìµœì¢… ì ìˆ˜ë¡œ íŒì •

.. container:: alert alert-block alert-warning

   \ :math:`\divideontimes` ë”¥ëŸ¬ë‹ì˜ í‰ê°€ í”„ë¡œì„¸ìŠ¤: :math:`\space`
   :math:`\space`

   .. raw:: html

      <center>

   Test Dataì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê· í•˜ì—¬ ì„±ëŠ¥ íŒì •

   .. raw:: html

      </center>

Input
-----

.. code:: ipython3

    import numpy as np
    import pandas as pd
    from pprint import pprint
    from unipy import aprint, lprint
    
    import matplotlib.pyplot as plt
    
    import importlib
    from src import examples
    from src.utils import lprint, qprint, keras_lossplot, keras_predict_plot
    from src.layers import sigmoid
    
    importlib.reload(examples)
    
    
    from src.load_data import column_range_scaler, load_data

Loading
~~~~~~~

.. code:: ipython3

    rawdata = pd.read_csv(
        'data/data_10min.csv',
        parse_dates=['EVT_DTM'],
        dtype={
            'VEND_ID': 'str',
            'ENB_ID': 'str',
            'CELL_ID': 'str',
            'FREQ_TYP_CD': 'str',
        }
    )
    
    kpi_list = ['CQI', 'UE_TX_POWER', 'DL_PRB_USAGE_RATE']
    
    rawdata.head(20)




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>EVT_DTM</th>
          <th>VEND_ID</th>
          <th>ENB_ID</th>
          <th>CELL_ID</th>
          <th>FREQ_TYP_CD</th>
          <th>CQI</th>
          <th>DL_PRB_USAGE_RATE</th>
          <th>UE_TX_POWER</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>2018-07-02 08:00:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>13.990000</td>
          <td>2.738333</td>
          <td>9.182692</td>
        </tr>
        <tr>
          <th>1</th>
          <td>2018-07-02 08:10:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>13.998333</td>
          <td>2.481667</td>
          <td>5.597436</td>
        </tr>
        <tr>
          <th>2</th>
          <td>2018-07-02 08:20:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>14.100000</td>
          <td>3.475000</td>
          <td>7.600000</td>
        </tr>
        <tr>
          <th>3</th>
          <td>2018-07-02 08:30:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>13.881667</td>
          <td>3.291667</td>
          <td>10.550000</td>
        </tr>
        <tr>
          <th>4</th>
          <td>2018-07-02 08:40:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>13.323333</td>
          <td>4.091667</td>
          <td>11.894444</td>
        </tr>
        <tr>
          <th>5</th>
          <td>2018-07-02 08:50:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>13.863333</td>
          <td>3.393333</td>
          <td>7.707843</td>
        </tr>
        <tr>
          <th>6</th>
          <td>2018-07-02 09:00:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>13.115000</td>
          <td>4.806667</td>
          <td>7.663462</td>
        </tr>
        <tr>
          <th>7</th>
          <td>2018-07-02 09:10:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>13.095000</td>
          <td>5.243333</td>
          <td>8.254386</td>
        </tr>
        <tr>
          <th>8</th>
          <td>2018-07-02 09:20:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>12.905000</td>
          <td>6.326667</td>
          <td>7.367241</td>
        </tr>
        <tr>
          <th>9</th>
          <td>2018-07-02 09:30:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>12.893333</td>
          <td>7.610000</td>
          <td>8.555357</td>
        </tr>
        <tr>
          <th>10</th>
          <td>2018-07-02 09:40:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>13.035000</td>
          <td>8.011667</td>
          <td>7.208772</td>
        </tr>
        <tr>
          <th>11</th>
          <td>2018-07-02 09:50:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>12.000000</td>
          <td>12.236667</td>
          <td>7.740678</td>
        </tr>
        <tr>
          <th>12</th>
          <td>2018-07-02 10:00:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>12.051667</td>
          <td>8.453333</td>
          <td>8.666667</td>
        </tr>
        <tr>
          <th>13</th>
          <td>2018-07-02 10:10:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>12.270000</td>
          <td>8.841667</td>
          <td>8.438333</td>
        </tr>
        <tr>
          <th>14</th>
          <td>2018-07-02 10:20:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>12.983333</td>
          <td>9.063333</td>
          <td>6.443333</td>
        </tr>
        <tr>
          <th>15</th>
          <td>2018-07-02 10:30:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>12.696667</td>
          <td>7.753333</td>
          <td>5.628333</td>
        </tr>
        <tr>
          <th>16</th>
          <td>2018-07-02 10:40:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>13.061667</td>
          <td>13.945000</td>
          <td>6.576667</td>
        </tr>
        <tr>
          <th>17</th>
          <td>2018-07-02 10:50:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>12.703333</td>
          <td>12.908333</td>
          <td>6.480000</td>
        </tr>
        <tr>
          <th>18</th>
          <td>2018-07-02 11:00:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>12.300000</td>
          <td>12.443333</td>
          <td>6.048333</td>
        </tr>
        <tr>
          <th>19</th>
          <td>2018-07-02 11:10:00</td>
          <td>SS</td>
          <td>28380</td>
          <td>24</td>
          <td>10</td>
          <td>11.836667</td>
          <td>15.426667</td>
          <td>5.103333</td>
        </tr>
      </tbody>
    </table>
    </div>



.. code:: ipython3

    print(rawdata['CQI'][:500].plot(figsize=(20, 8)))


.. parsed-literal::

    AxesSubplot(0.125,0.125;0.775x0.755)



.. image:: output_13_1.png


.. code:: ipython3

    print(rawdata[kpi_list][:500].plot(figsize=(20, 8)))


.. parsed-literal::

    AxesSubplot(0.125,0.125;0.775x0.755)



.. image:: output_14_1.png


Scaling
~~~~~~~

.. code:: ipython3

    kpi_range_dict = {
        'CQI': [0, 15],
        'UE_TX_POWER': [-17, 23],
        'DL_PRB_USAGE_RATE': [0, 100],
    }
    
    data_scaled, scaler_dict = column_range_scaler(
        rawdata[kpi_list],
        vendor_name='SS',
        col_real_range_dict=kpi_range_dict,
        feature_range=(0., 1.),
    )

.. code:: ipython3

    print(data_scaled[:500].plot(figsize=(20, 8)))


.. parsed-literal::

    AxesSubplot(0.125,0.125;0.775x0.755)



.. image:: output_17_1.png


.. code:: ipython3

    data_scaled.head()




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>CQI</th>
          <th>UE_TX_POWER</th>
          <th>DL_PRB_USAGE_RATE</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0.932667</td>
          <td>0.654567</td>
          <td>0.027383</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0.933222</td>
          <td>0.564936</td>
          <td>0.024817</td>
        </tr>
        <tr>
          <th>2</th>
          <td>0.940000</td>
          <td>0.615000</td>
          <td>0.034750</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0.925444</td>
          <td>0.688750</td>
          <td>0.032917</td>
        </tr>
        <tr>
          <th>4</th>
          <td>0.888222</td>
          <td>0.722361</td>
          <td>0.040917</td>
        </tr>
      </tbody>
    </table>
    </div>



Sliding Window (``MLP``)
~~~~~~~~~~~~~~~~~~~~~~~~

ìš°ì„  ë¬´ìŠ¨ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì˜ˆì¸¡í•  ê²ƒì¸ì§€ ì •í•´ì•¼ í•œë‹¤.

``X``\ ì™€ ``Y``\ ë¥¼ ì–´ë–»ê²Œ ì •í•˜ë©´ ì¢‹ì„ê¹Œ? ìš°ë¦¬ê°€ ê°€ì§„ ë°ì´í„°ëŠ” ``Time``
ì¶•ê³¼, ``KPI`` ì¶•ì´ ìˆë‹¤.

+-------------+-------------+-------------+-------------+-------------+
| Case        | ê¸°ì¤€        | ``Time``\ ìˆ˜ | ``KPI``\ ìˆ˜ | ë‚´ìš©       |
+=============+=============+=============+=============+=============+
| 1           | Row         | Multi       | 1           | ì—¬ëŸ¬ ì‹œì ì˜ |
|             |             |             |             | KPIë¡œ ë‹¤ìŒ  |
|             |             |             |             | KPI         |
|             |             |             |             | ì˜ˆì¸¡í•˜ê¸° (1 |
|             |             |             |             | KPI)        |
+-------------+-------------+-------------+-------------+-------------+
| 2           | Column      | 1           | Multi       | í•œ ì‹œì ì—   |
|             |             |             |             | ëŒ€í•œ KPI    |
|             |             |             |             | ì—¬ëŸ¬ ê°œë¡œ   |
|             |             |             |             | ë‹¤ìŒ KPI    |
|             |             |             |             | ì˜ˆì¸¡í•˜ê¸°    |
+-------------+-------------+-------------+-------------+-------------+
| 3           | Row &       | Multi       | Multi       | ì—¬ëŸ¬ ì‹œì ì˜ |
|             | Column      |             |             | KPIë¡œ ë‹¤ìŒ  |
|             |             |             |             | KPI         |
|             |             |             |             | ì˜ˆì¸¡í•˜ê¸° (3 |
|             |             |             |             | KPI)        |
+-------------+-------------+-------------+-------------+-------------+

ì¼ë‹¨, ì¶•ë³„ë¡œ ê¸°ì¤€ì„ ì¡ì•„ ë³´ì.

 :math:`\cdot` Case 1: ì—¬ëŸ¬ ì‹œì ì˜ KPIë¡œ ë‹¤ìŒ KPI ì˜ˆì¸¡í•˜ê¸° (1 KPI)

 :math:`\cdot` Case 2: í•œ ì‹œì ì˜ KPI ì—¬ëŸ¬ ê°œë¡œ ë‹¤ìŒ KPI ì˜ˆì¸¡í•˜ê¸°

 ì´ ì¤‘ Case 2ì— ëŒ€í•´ ì‹¤ìŠµí•œë‹¤. (Case 1ì„ ì ìš©í•˜ëŠ” ê²½ìš°, Case 2ë¥¼ ì¶•ë§Œ
ë°”ê¾¸ë©´ ì‰½ê²Œ ì‘ìš©í•  ìˆ˜ ìˆë‹¤.)

 :math:`\cdot` ë°ì´í„° êµ¬ì¡°

`Link to MLP Model <#(ì‹¤ìŠµ)-MLP-in-keras>`__

Exercise 1 : Basic
^^^^^^^^^^^^^^^^^^

.. code:: ipython3

    from ipywidgets import interact, interactive, fixed, interact_manual, interactive_output
    import ipywidgets as widgets
    from IPython.display import display
    from pprint import pprint

.. code:: ipython3

    #==== Essential ========================================================#
    
    case_x_list = []
    case_y_list = []
    for i in range(5):
        case_x_list += [data_scaled.iloc[i:i+1, :3]]
        case_y_list += [data_scaled.iloc[i+1:i+2, :2]]
    
    #=======================================================================#
    
    
    print('\nX', '=' * 45)
    pprint(case_x_list)
    print('\nY', '=' * 45)
    pprint(case_y_list)


.. parsed-literal::

    
    X =============================================
    [        CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
    0  0.932667     0.654567           0.027383,
             CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
    1  0.933222     0.564936           0.024817,
         CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
    2  0.94        0.615            0.03475,
             CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
    3  0.925444      0.68875           0.032917,
             CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
    4  0.888222     0.722361           0.040917]
    
    Y =============================================
    [        CQI  UE_TX_POWER
    1  0.933222     0.564936,
         CQI  UE_TX_POWER
    2  0.94        0.615,
             CQI  UE_TX_POWER
    3  0.925444      0.68875,
             CQI  UE_TX_POWER
    4  0.888222     0.722361,
             CQI  UE_TX_POWER
    5  0.924222     0.617696]


Exercise 2 : Sliding Window
^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code:: ipython3

    @interact(
        case_num=widgets.IntSlider(min=1, max=10, step=1, value=2),
        kpi_num=widgets.IntSlider(min=1, max=3, step=1, value=3),
    )
    def shape_print(case_num, kpi_num):
    
        data_cnt = case_num
        input_dim = kpi_num
    
        print(
            '=' * 40 +
            '\n (data_cnt, input_dim)  ' +
            ':' +
            f'  ({data_cnt}, {input_dim})\n' +
            '=' * 40 + 
            '\n',
        )
    
        
        #==== Essential ========================================================#
        
        case_list = []
        for i in range(data_cnt):
            case_list += [(
                'X ' +  '=' * 40,
                data_scaled.iloc[i:i+1, :input_dim],
            )]
    
        #=======================================================================#
    
        
        pprint(case_list)



.. parsed-literal::

    interactive(children=(IntSlider(value=2, description='case_num', max=10, min=1), IntSlider(value=3, descriptioâ€¦


Exercise 3 : Sliding Window ``(X & Y)``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code:: ipython3

    @interact(
        case_num=widgets.IntSlider(min=1, max=10, step=1, value=5),
        x_kpi_num=widgets.IntSlider(min=1, max=3, step=1, value=3),
        y_kpi_num=widgets.IntSlider(min=1, max=3, step=1, value=2),
    )
    def shape_print(case_num, x_kpi_num, y_kpi_num):
    
        data_cnt = case_num
        input_dim = x_kpi_num
        output_dim = y_kpi_num
    
        print(
            '=' * 45 +
            '\n [X]: (data_cnt, input_dim )  ' +
            ':' +
            f'  ({data_cnt}, {input_dim})' +
            '\n [Y]: (data_cnt, output_dim)  ' +
            ':' +
            f'  ({data_cnt}, {output_dim})\n' +
            '=' * 45 + 
            '\n',
        )
        
        
        #==== Essential ========================================================#
    
        case_list = []
        for i in range(data_cnt):
            case_list += [(
                'X ' +  '=' * 40,
                data_scaled.iloc[i:i+1, :input_dim],
                'Y ' +  '-' * 40,
                data_scaled.iloc[i+1:i+2, :output_dim],
            )]
        
        #=======================================================================#
        
     
        pprint(case_list)



.. parsed-literal::

    interactive(children=(IntSlider(value=5, description='case_num', max=10, min=1), IntSlider(value=3, descriptioâ€¦


.. code:: ipython3

    def sliding_window_mlp(data, data_cnt, input_dim, output_dim):
    
        case_x_list = []
        case_y_list = []
        
        data_rownum = data.shape[0]
        max_data_cnt = data_rownum - 1
        
        if data_cnt is None:
            data_cnt = max_data_cnt
        else:
            data_cnt = min(data_cnt, max_data_cnt)
        
        
        #==== Essential ========================================================#
        
        for i in range(data_cnt):
            case_x_list += [data_scaled.iloc[i, :input_dim].values]
            case_y_list += [data_scaled.iloc[i+1, :output_dim].values]
    
        #=======================================================================#
    
    
        print(
            '=' * 45 +
            '\n [X]: (data_cnt, input_dim )  ' +
            ':' +
            f'  ({data_cnt}, {input_dim})' +
            '\n [Y]: (data_cnt, output_dim)  ' +
            ':' +
            f'  ({data_cnt}, {output_dim})\n' +
            '=' * 45 + 
            '\n',
        )
            
        return np.stack(case_x_list), np.stack(case_y_list)
    
    
    #==== Essential ========================================================#
    
    data_x_mlp, data_y_mlp = sliding_window_mlp(
        data_scaled,
        data_cnt=data_scaled.shape[0],
        input_dim=3,
        output_dim=2,
    )
    
    #=======================================================================#
    
    
    aprint(data_x_mlp[:5], data_y_mlp[:5], name_list=['data_x_mlp[:5]', 'data_y_mlp[:5]'])


.. parsed-literal::

    =============================================
     [X]: (data_cnt, input_dim )  :  (4289, 3)
     [Y]: (data_cnt, output_dim)  :  (4289, 2)
    =============================================
    
    =========================================================================
    |  data_x_mlp[:5]                        |   data_y_mlp[:5]             |
    |  (5, 3)                                |   (5, 2)                     |
    =========================================================================
    |  [[0.93266667 0.65456731 0.02738333]   |   [[0.93322222 0.5649359 ]   |
    |   [0.93322222 0.5649359  0.02481667]   |    [0.94       0.615     ]   |
    |   [0.94       0.615      0.03475   ]   |    [0.92544444 0.68875   ]   |
    |   [0.92544444 0.68875    0.03291667]   |    [0.88822222 0.72236111]   |
    |   [0.88822222 0.72236111 0.04091667]]  |    [0.92422222 0.61769608]]  |
    =========================================================================


Sliding Window (``RNN``)
~~~~~~~~~~~~~~~~~~~~~~~~

ì´ë²ˆì—ëŠ” ``Time``\ ì¶•ê³¼ ``KPI`` ì¶•ì„ ëª¨ë‘ ê³ ë ¤í•´ ë³´ì.

 :math:`\cdot` Case 3: ì—¬ëŸ¬ ì‹œì ì˜ KPIë¡œ ë‹¤ìŒ KPI ì˜ˆì¸¡í•˜ê¸° (3 KPI)

 :math:`\cdot` ë°ì´í„° êµ¬ì¡°

`Link to RNN Model <#(ì‹¤ìŠµ)-RNN-in-keras:-many-to-many>`__

Exercise 1 : Basic
^^^^^^^^^^^^^^^^^^

.. code:: ipython3

    case_list = []
    for i in range(4):
        case_list += [data_scaled[i:i+7]]
    
    case_list




.. parsed-literal::

    [        CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
     0  0.932667     0.654567           0.027383
     1  0.933222     0.564936           0.024817
     2  0.940000     0.615000           0.034750
     3  0.925444     0.688750           0.032917
     4  0.888222     0.722361           0.040917
     5  0.924222     0.617696           0.033933
     6  0.874333     0.616587           0.048067,
             CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
     1  0.933222     0.564936           0.024817
     2  0.940000     0.615000           0.034750
     3  0.925444     0.688750           0.032917
     4  0.888222     0.722361           0.040917
     5  0.924222     0.617696           0.033933
     6  0.874333     0.616587           0.048067
     7  0.873000     0.631360           0.052433,
             CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
     2  0.940000     0.615000           0.034750
     3  0.925444     0.688750           0.032917
     4  0.888222     0.722361           0.040917
     5  0.924222     0.617696           0.033933
     6  0.874333     0.616587           0.048067
     7  0.873000     0.631360           0.052433
     8  0.860333     0.609181           0.063267,
             CQI  UE_TX_POWER  DL_PRB_USAGE_RATE
     3  0.925444     0.688750           0.032917
     4  0.888222     0.722361           0.040917
     5  0.924222     0.617696           0.033933
     6  0.874333     0.616587           0.048067
     7  0.873000     0.631360           0.052433
     8  0.860333     0.609181           0.063267
     9  0.859556     0.638884           0.076100]



Exercise 2 : Sliding Window
^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code:: ipython3

    from ipywidgets import interact, interactive, fixed, interact_manual, interactive_output
    import ipywidgets as widgets
    from IPython.display import display
    from pprint import pprint

.. code:: ipython3

    @interact(
        case_num=widgets.IntSlider(min=1, max=10, step=1, value=2),
        window_size=widgets.IntSlider(min=2, max=10, step=1, value=7),
        kpi_num=widgets.IntSlider(min=1, max=3, step=1, value=3),
    )
    def shape_print(case_num, window_size, kpi_num):
    
        data_cnt = case_num
        input_size = window_size
        input_dim = kpi_num
    
        print(
            '=' * 51 +
            '\n (data_cnt, input_size, input_dim)  ' +
            ':' +
            f'  ({data_cnt}, {input_size}, {input_dim})\n' +
            '=' * 51 + 
            '\n',
        )
    
        
        #==== Essential ========================================================#
    
        case_list = []
        for i in range(data_cnt):
            case_list += [(
                'X ' +  '=' * 40,
                data_scaled.iloc[i:i+input_size, :input_dim],
            )]
    
        #=======================================================================#
    
    
        pprint(case_list)



.. parsed-literal::

    interactive(children=(IntSlider(value=2, description='case_num', max=10, min=1), IntSlider(value=7, descriptioâ€¦


Exercise 3 : Sliding Window ``(X & Y)``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code:: ipython3

    !jupyter nbextension enable --py widgetsnbextension


.. parsed-literal::

    Enabling notebook extension jupyter-js-widgets/extension...
          - Validating: [32mOK[0m


.. code:: ipython3

    @interact(
        case_num=widgets.IntSlider(min=1, max=10, step=1, value=2),
        window_x=widgets.IntSlider(min=2, max=5, step=1, value=5),
        window_y=widgets.IntSlider(min=1, max=5, step=1, value=2),
        input_kpi_num=widgets.IntSlider(min=1, max=3, step=1, value=3),
        output_kpi_num=widgets.IntSlider(min=1, max=3, step=1, value=3),
    )
    def shape_print(case_num, window_x, window_y, input_kpi_num, output_kpi_num):
    
        data_cnt = case_num
        input_size = window_x
        output_size = window_y
        window_size = input_size + output_size
        input_dim = input_kpi_num
        output_dim = output_kpi_num
    
        print(
            '=' * 60 +
            '\n [X]: (data_cnt, input_size,  input_dim )  ' +
            ':' +
            f'  ({data_cnt}, {input_size}, {input_dim})' +
            '\n [Y]: (data_cnt, output_size, output_dim)  ' +
            ':' +
            f'  ({data_cnt}, {output_size}, {output_dim})\n' +
            '=' * 60 + 
            '\n',
        )
        
        #==== Essential ========================================================#
    
        case_list = []
        for i in range(data_cnt):
            case_list += [(
                'X ' +  '=' * 40,
                data_scaled.iloc[i:i+input_size, :input_dim],
                'Y ' +  '-' * 40,
                data_scaled.iloc[i+input_size:i+input_size+output_size, :output_dim],
            )]
     
        #=======================================================================#
        
    
        pprint(case_list)



.. parsed-literal::

    interactive(children=(IntSlider(value=2, description='case_num', max=10, min=1), IntSlider(value=5, descriptioâ€¦


.. code:: ipython3

    def sliding_window_rnn(data, data_cnt, input_size, output_size, input_dim, output_dim):
    
        case_x_list = []
        case_y_list = []
        
        data_rownum = data.shape[0]
        max_data_cnt = data_rownum - (input_size + output_size) + 1
        
        if data_cnt is None:
            data_cnt = max_data_cnt
        else:
            data_cnt = min(data_cnt, max_data_cnt)
        
        
        #==== Essential ========================================================#
        
        for i in range(data_cnt):
            case_x_list += [data_scaled.iloc[i:i+input_size, :input_dim].values]
            case_y_list += [data_scaled.iloc[i+input_size:i+input_size+output_size, :output_dim].values]
    
        #=======================================================================#
    
    
        print(
            '=' * 60 +
            '\n [X]: (data_cnt, input_size,  input_dim )  ' +
            ':' +
            f'  ({data_cnt}, {input_size}, {input_dim})' +
            '\n [Y]: (data_cnt, output_size, output_dim)  ' +
            ':' +
            f'  ({data_cnt}, {output_size}, {output_dim})\n' +
            '=' * 60 + 
            '\n',
        )
            
        return np.stack(case_x_list), np.stack(case_y_list)
    
    
    #==== Essential ========================================================#
    
    data_x_rnn, data_y_rnn = sliding_window_rnn(
        data_scaled,
        data_cnt=data_scaled.shape[0],
        input_size=5,
        output_size=2,
        input_dim=3,
        output_dim=2,
    )
    
    #=======================================================================#
    
    
    aprint(data_x_rnn[:5], data_y_rnn[:5], name_list=['data_x_rnn[:5]', 'data_y_rnn[:5]'])


.. parsed-literal::

    ============================================================
     [X]: (data_cnt, input_size,  input_dim )  :  (4284, 5, 3)
     [Y]: (data_cnt, output_size, output_dim)  :  (4284, 2, 2)
    ============================================================
    
    =============================================================================
    |  data_x_rnn[:5]                          |   data_y_rnn[:5]               |
    |  (5, 5, 3)                               |   (5, 2, 2)                    |
    =============================================================================
    |  [[[0.93266667 0.65456731 0.02738333]    |   [[[0.92422222 0.61769608]    |
    |    [0.93322222 0.5649359  0.02481667]    |     [0.87433333 0.61658654]]   |
    |    [0.94       0.615      0.03475   ]    |                                |
    |    [0.92544444 0.68875    0.03291667]    |    [[0.87433333 0.61658654]    |
    |    [0.88822222 0.72236111 0.04091667]]   |     [0.873      0.63135965]]   |
    |                                          |                                |
    |   [[0.93322222 0.5649359  0.02481667]    |    [[0.873      0.63135965]    |
    |    [0.94       0.615      0.03475   ]    |     [0.86033333 0.60918103]]   |
    |    [0.92544444 0.68875    0.03291667]    |                                |
    |    [0.88822222 0.72236111 0.04091667]    |    [[0.86033333 0.60918103]    |
    |    [0.92422222 0.61769608 0.03393333]]   |     [0.85955556 0.63888393]]   |
    |                                          |                                |
    |   [[0.94       0.615      0.03475   ]    |    [[0.85955556 0.63888393]    |
    |    [0.92544444 0.68875    0.03291667]    |     [0.869      0.6052193 ]]]  |
    |    [0.88822222 0.72236111 0.04091667]    |                                |
    |    [0.92422222 0.61769608 0.03393333]    |                                |
    |    [0.87433333 0.61658654 0.04806667]]   |                                |
    |                                          |                                |
    |   [[0.92544444 0.68875    0.03291667]    |                                |
    |    [0.88822222 0.72236111 0.04091667]    |                                |
    |    [0.92422222 0.61769608 0.03393333]    |                                |
    |    [0.87433333 0.61658654 0.04806667]    |                                |
    |    [0.873      0.63135965 0.05243333]]   |                                |
    |                                          |                                |
    |   [[0.88822222 0.72236111 0.04091667]    |                                |
    |    [0.92422222 0.61769608 0.03393333]    |                                |
    |    [0.87433333 0.61658654 0.04806667]    |                                |
    |    [0.873      0.63135965 0.05243333]    |                                |
    |    [0.86033333 0.60918103 0.06326667]]]  |                                |
    =============================================================================


Splitting data : Train & Test
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code:: ipython3

    train_x_mlp, test_x_mlp = data_x_mlp[:4000], data_x_mlp[4000:]
    train_y_mlp, test_y_mlp = data_y_mlp[:4000], data_y_mlp[4000:]
    
    
    train_x_rnn, test_x_rnn = data_x_rnn[:4000], data_x_rnn[4000:]
    train_y_rnn, test_y_rnn = data_y_rnn[:4000], data_y_rnn[4000:]

-  Saving the data

.. code:: ipython3

    data_list = [
        train_x_mlp,
        train_y_mlp,
        train_x_rnn,
        train_y_rnn,
        test_x_mlp,
        test_y_mlp,
        test_x_rnn,
        test_y_rnn,
    ]
    
    
    data_str_list = [
        'train_x_mlp',
        'train_y_mlp',
        'train_x_rnn',
        'train_y_rnn',
        'test_x_mlp',
        'test_y_mlp',
        'test_x_rnn',
        'test_y_rnn',
    ]
    
    
    for _, __ in zip(data_list, data_str_list):
        _tmp = _.astype(np.float32)
        print(f'{__:10} : {_tmp.shape}')
        np.save(f'data/{__}.npy', _tmp)


.. parsed-literal::

    train_x_mlp : (4000, 3)
    train_y_mlp : (4000, 2)
    train_x_rnn : (4000, 5, 3)
    train_y_rnn : (4000, 2, 2)
    test_x_mlp : (289, 3)
    test_y_mlp : (289, 2)
    test_x_rnn : (284, 5, 3)
    test_y_rnn : (284, 2, 2)


-  Loading the data

.. code:: ipython3

    data_dict = load_data()
    
    train_x_mlp = data_dict['train_x_mlp']
    train_y_mlp = data_dict['train_y_mlp']
    test_x_mlp = data_dict['test_x_mlp']
    test_y_mlp = data_dict['test_y_mlp']


.. parsed-literal::

    Loading Data...
    train_x_mlp : (4000, 3)
    train_y_mlp : (4000, 2)
    train_x_rnn : (4000, 5, 3)
    train_y_rnn : (4000, 2, 2)
    test_x_mlp : (289, 3)
    test_y_mlp : (289, 2)
    test_x_rnn : (284, 5, 3)
    test_y_rnn : (284, 2, 2)
    Complete.


.. code:: ipython3

    load_data




.. parsed-literal::

    <function src.load_data.load_data()>



(ì‹¤ìŠµ) MLP in ``keras``
-----------------------

 :math:`\cdot` Case 2: í•œ ì‹œì ì˜ KPI ì—¬ëŸ¬ ê°œë¡œ ë‹¤ìŒ KPI ì˜ˆì¸¡í•˜ê¸°

 :math:`\cdot` ê³„ì‚° íë¦„ : MLP

 :math:`\cdot` ì½”ë“œ ë¦¬ë·° : MLP

`Link to MLP Input <#Sliding-Window-(MLP)>`__

.. container:: alert alert-block alert-info

   \ :math:`\divideontimes` Unit_num ì´ë€ :

   ë”¥ëŸ¬ë‹ Frameworkì—ì„œ í”íˆ ë³¼ ìˆ˜ ìˆëŠ” ``unit_num``\ ì€
   ``output_shape``\ ë¥¼ ë§í•œë‹¤.

   | ë‡Œì™€ ë‰´ëŸ°(ë‡Œì‹ ê²½)ì„ ìƒê°í•´ ë³´ì.
   | ìš°ë¦¬ê°€ ì˜í™”ë¥¼ ë³¼ ë•Œ, ë‡Œì—ì„œ ì–´ë–¤ ë‰´ëŸ°ì€ ì˜ìƒ ì´ë¯¸ì§€ë¥¼, ì–´ë–¤ ë‰´ëŸ°ì€
     ì†Œë¦¬ë¥¼ ë°›ì•„ë“¤ì¸ë‹¤.
   | í•˜ë‚˜ì˜ í™”ë©´ì€ ìˆ˜ë§ì€ ë‰´ëŸ°ë“¤ì„ í†µí•´ ê°ê°ì˜ ê²°ê³¼ë¥¼ ë‚´ë†“ê²Œ ë˜ê³ 
   | ìµœì¢…ì ìœ¼ë¡œëŠ” ì´ ê²°ê³¼ë“¤ì„ ì¢…í•©í•˜ì—¬ ì˜í™”ê°€ ì¬ë¯¸ìˆëŠ”ì§€ íŒë‹¨í•œë‹¤.

   | ``unit_num``\ ì€ ì…ë ¥ì„ ì²˜ë¦¬í•˜ëŠ” ë‰´ëŸ°ì˜ ê°œìˆ˜ì´ë©°
   | í•œ í™”ë©´ì— ëŒ€í•´ ë‰´ëŸ°ì˜ ê°œìˆ˜ë§Œí¼ ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ” ê²ƒ ì²˜ëŸ¼,
   | Inputì„ ë°”ë¼ë³´ëŠ” ``unit``\ ì˜ ê°œìˆ˜ë§Œí¼ Hidden Dimensionì˜ í¬ê¸°ë„
     ì»¤ì§€ê²Œ ëœë‹¤.

   .. raw:: html

      <center>

   â€œ``unit_num``\ ìœ¼ë¡œ ë‹¤ìŒ ë²ˆ ``Hidden Dimension``\ ì˜ í¬ê¸°ë¥¼
   ê²°ì •í•œë‹¤.â€

   .. raw:: html

      </center>

    :math:`\cdot` Input Dimension : :math:`m = 3`\ 

   :math:`\cdot` Output Dimension : :math:`l = 4`\ 

    :raw-latex:`\begin{equation}
   \begin{bmatrix} n\times m \end{bmatrix}
   \overbrace{ \begin{bmatrix} m\times l \end{bmatrix} }^{Layer} 
   = \begin{bmatrix} n\times l \end{bmatrix}
   \end{equation}`

   :math:`\space` :math:`\space`

    :raw-latex:`\begin{equation}
   \overbrace{
   \left({\begin{array}{cc}
   \color{red}a & \color{red}b & \color{red}c \\
   \color{red}d & \color{red}e & \color{red}f 
   \end{array}}\right)
   }^{input \_ dim \space = \space 3}
   \overbrace{
   \left( \begin{array}{cc}
       \color{blue}  A & \color{blue} D & \color{blue} G & \color{blue} J \\
       \color{blue} B & \color{blue} E & \color{blue} H & \color{blue} K \\
       \color{blue} C & \color{blue} F & \color{blue} I & \color{blue} L \\
   \end{array} \right) }^{unit \_ num \space = \space 4}
   = \overbrace{
   \left(\begin{array}{cc} 
       \color{red}a \color{blue}A + \color{red}b \color{blue}B + \color{red}c \color{blue}C & 
       \color{red}a \color{blue}D + \color{red}b \color{blue}E + \color{red}c \color{blue}F &
       \color{red}a \color{blue}G + \color{red}b \color{blue}H + \color{red}c \color{blue}I &
       \color{red}a \color{blue}J + \color{red}b \color{blue}K + \color{red}c \color{blue}L
       \\
       \color{red}d \color{blue}A + \color{red}e \color{blue}B + \color{red}f \color{blue}C & 
       \color{red}d \color{blue}D + \color{red}e \color{blue}E + \color{red}f \color{blue}F & 
       \color{red}d \color{blue}G + \color{red}e \color{blue}H + \color{red}f \color{blue}I &
       \color{red}d \color{blue}J + \color{red}e \color{blue}K + \color{red}f \color{blue}L
       \\
   \end{array}\right)
   }^{output \_ dim \space = \space unit \_ num \space = \space 4}
   \end{equation}`

.. code:: ipython3

    import keras
    import keras.backend as K
    from keras import Model, Sequential
    from keras.layers import Input, Dense, SimpleRNN, LSTM, GRU, TimeDistributed


.. parsed-literal::

    Using TensorFlow backend.


MLP Modeling
~~~~~~~~~~~~

.. code:: ipython3

    input_layer = Input(shape=(3, ))
    layer1 = Dense(4, input_dim=3, activation='sigmoid')
    layer2 = Dense(2, input_dim=4, activation='sigmoid')
    
    hidden_layer1 = layer1(input_layer)
    output_layer = layer2(hidden_layer1)
    
    model = Model(inputs=[input_layer], outputs=[output_layer])
    
    model.summary()


.. parsed-literal::

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_1 (InputLayer)         (None, 3)                 0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 4)                 16        
    _________________________________________________________________
    dense_2 (Dense)              (None, 2)                 10        
    =================================================================
    Total params: 26
    Trainable params: 26
    Non-trainable params: 0
    _________________________________________________________________


.. code:: ipython3

    input_layer = Input(shape=(3, ))
    layer1 = Dense(16, input_dim=3, activation='sigmoid')
    layer2 = Dense(64, input_dim=16, activation='sigmoid')
    layer3 = Dense(16, input_dim=64, activation='sigmoid')
    layer4 = Dense(8, input_dim=16, activation='sigmoid')
    layer5 = Dense(2, input_dim=8, activation='sigmoid')
    
    hidden_layer1 = layer1(input_layer)
    hidden_layer2 = layer2(hidden_layer1)
    hidden_layer3 = layer3(hidden_layer2)
    hidden_layer4 = layer4(hidden_layer3)
    output_layer = layer5(hidden_layer4)
    
    model = Model(inputs=[input_layer], outputs=[output_layer])
    
    model.summary()


.. parsed-literal::

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_4 (InputLayer)         (None, 3)                 0         
    _________________________________________________________________
    dense_13 (Dense)             (None, 16)                64        
    _________________________________________________________________
    dense_14 (Dense)             (None, 64)                1088      
    _________________________________________________________________
    dense_15 (Dense)             (None, 16)                1040      
    _________________________________________________________________
    dense_16 (Dense)             (None, 8)                 136       
    _________________________________________________________________
    dense_17 (Dense)             (None, 2)                 18        
    =================================================================
    Total params: 2,346
    Trainable params: 2,346
    Non-trainable params: 0
    _________________________________________________________________


MLP Building
~~~~~~~~~~~~

-  ``optimizer`` : ë³µìŠµ ë…¸í•˜ìš°
-  ``loss`` : ëª¨ì˜ê³ ì‚¬ ë•Œ ì‚¬ìš©í•  ì ìˆ˜ê³„ì‚°ë²• (``ì´ì ``, ``í‰ê· ``)
-  ``metrics`` : ì‹¤ì „ì—ì„œ ì‚¬ìš©í•  ì ìˆ˜ê³„ì‚°ë²• (``1 ~ 7ë“±ê¸‰``, ``A ~ F``,
   ``ìˆ˜ìš°ë¯¸ì–‘ê°€``)

.. code:: ipython3

    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

MLP Training
~~~~~~~~~~~~

.. container:: alert alert-block alert-success

   Q. ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ë³´ì.

   \ ``batch_size``\ ì™€ \ ``epoch``\ ë¥¼ ì¡°ì ˆí•´ ë³´ê³ , ``loss`` ê°’ì´
   ì¤„ì–´ë“¤ê³  ìˆëŠ” ì§€ Plotìœ¼ë¡œ í™•ì¸í•´ ë³´ì.

.. code:: ipython3

    fitted = model.fit(
        train_x_mlp,
        train_y_mlp,
        batch_size=64,
        epochs=200,
        validation_split=.2,
        verbose=1,            # [verbose] 1: progress bar, 2: one line per epoch
        shuffle=True,
    )


.. parsed-literal::

    Train on 3200 samples, validate on 800 samples
    Epoch 1/200
    3200/3200 [==============================] - 0s 50us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0029 - val_mean_absolute_error: 0.0438
    Epoch 2/200
    3200/3200 [==============================] - 0s 44us/step - loss: 0.0017 - mean_absolute_error: 0.0295 - val_loss: 0.0029 - val_mean_absolute_error: 0.0441
    Epoch 3/200
    3200/3200 [==============================] - 0s 46us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0027 - val_mean_absolute_error: 0.0423
    Epoch 4/200
    3200/3200 [==============================] - 0s 41us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0028 - val_mean_absolute_error: 0.0423
    Epoch 5/200
    3200/3200 [==============================] - 0s 41us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0027 - val_mean_absolute_error: 0.0415
    Epoch 6/200
    3200/3200 [==============================] - 0s 41us/step - loss: 0.0017 - mean_absolute_error: 0.0295 - val_loss: 0.0028 - val_mean_absolute_error: 0.0427
    Epoch 7/200
    3200/3200 [==============================] - 0s 41us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0027 - val_mean_absolute_error: 0.0415
    Epoch 8/200
    3200/3200 [==============================] - 0s 45us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0029 - val_mean_absolute_error: 0.0436
    Epoch 9/200
    3200/3200 [==============================] - 0s 44us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0030 - val_mean_absolute_error: 0.0450
    Epoch 10/200
    3200/3200 [==============================] - 0s 43us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0028 - val_mean_absolute_error: 0.0422
    Epoch 11/200
    3200/3200 [==============================] - 0s 44us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0028 - val_mean_absolute_error: 0.0418
    Epoch 12/200
    3200/3200 [==============================] - 0s 45us/step - loss: 0.0017 - mean_absolute_error: 0.0294 - val_loss: 0.0027 - val_mean_absolute_error: 0.0413
    Epoch 13/200
    3200/3200 [==============================] - 0s 46us/step - loss: 0.0017 - mean_absolute_error: 0.0295 - val_loss: 0.0028 - val_mean_absolute_error: 0.0420
    Epoch 14/200
      64/3200 [..............................] - ETA: 0s - loss: 0.0017 - mean_absolute_error: 0.03


.. raw:: html

    <b>limit_output extension: Maximum message size of 10000 exceeded with 10159 characters</b>


.. code:: ipython3

    keras_lossplot(model)




.. image:: output_59_0.png



.. container:: alert alert-block alert-danger

   | Tip:
   | 

   ``batch_size``\ ê°€ ë„ˆë¬´ ì»¤ë„ í•™ìŠµì´ ì˜ ë˜ì§€ ì•Šìœ¼ë©°, ë¬´ì‘ì • ``epoch``
   ìˆ˜ë¡œ ë°˜ë³µì„ ë§ì´ ì‹œì¼œë„ í•™ìŠµì„±ëŠ¥ì— í° ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤.

   ``batch_size``\ ë¥¼ ì‘ê²Œ í•˜ë©´ ì–´ë–¨ê¹Œ?

ìš°ë¦¬ê°€ í•œêµ­ì‚¬ ì‹œí—˜ì„ ë³¸ë‹¤ê³  ê°€ì •í•˜ì.

-  ``batch_size`` : ëª¨ì˜ê³ ì‚¬ 1íšŒë¶„ ì¶œì œ ë²”ìœ„(8=ê³ ì¡°ì„ ~ì‚¼êµ­ì‹œëŒ€,
   256=ê³ ì¡°ì„ ~ê·¼í˜„ëŒ€ì‚¬)
-  ``epoch`` : ëª¨ì˜ê³ ì‚¬ ì‘ì‹œ íšŸìˆ˜
-  ``shuffle`` : ë¬¸í•­/ë³´ê¸° ì„ê¸°
-  ëª¨ë¸ì˜ ê¹Šì´(\ ``layer``\ ì˜ ê°œìˆ˜, ``hidden space``\ ì˜ í¬ê¸°) :
   í•™ì—…ìˆ˜ì¤€, IQ

| í•™ìƒì˜ í•™ì—…ìˆ˜ì¤€ì— ë”°ë¼ ê° í•­ëª©ë“¤ì„ ì ì ˆíˆ ë°°ë¶„í•´ì•¼ ì‹¤ë ¥ì´ ì‘¥ì‘¥ ëŠ˜ë“¯ì´,
| ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ Parameter ê°’ì„ ì§€ì •í•´ ì£¼ì–´ì•¼ í•œë‹¤.

.. raw:: html

   </div>

MLP Evaluation
~~~~~~~~~~~~~~

| ìœ„ì—ì„œëŠ” ëª¨ì˜ê³ ì‚¬ ì ìˆ˜ë¡œ í•™ìŠµì˜ ì§„í–‰ìƒí™©ë§Œì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.
| ì´ì œ ì‹¤ì „ìœ¼ë¡œ ê·¸ë™ì•ˆì˜ ë…¸ë ¥ì— ëŒ€í•œ ê²°ê³¼ë¥¼ í‰ê°€í•´ ë³´ì.

.. raw:: html

   <center>

â€œTraining setìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì„ Test setìœ¼ë¡œ í‰ê°€í•œë‹¤.â€

.. raw:: html

   </center>

``model.compile``\ ì—ì„œ ì„¤ì •í•œ ``loss``\ ì™€ ``metrics`` ê°’ì´ ê°ê°
ì¶œë ¥ëœë‹¤.

+-------------+-------------------------------+----------------+
| í•­ëª©        | ë‚´ìš©                          | ì˜ˆì‹œ           |
+=============+===============================+================+
| ``loss``    | í•™ìŠµí•  ë•Œ ì‚¬ìš©í•œ ì ìˆ˜ê³„ì‚°ë²•   | ìˆ˜ëŠ¥ì ìˆ˜(ì´ì ) |
+-------------+-------------------------------+----------------+
| ``metrics`` | í‰ê°€í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ì ìˆ˜ê³„ì‚°ë²• | ìˆ˜ëŠ¥ ë“±ê¸‰ì œ    |
+-------------+-------------------------------+----------------+

Result:

::

   <finished>/<total test num> [====== <progress bar> ========] - <elapsed time>/step
   [<loss>, <metrics>]

.. code:: ipython3

    model.evaluate(test_x_mlp, test_y_mlp)


.. parsed-literal::

    289/289 [==============================] - 0s 34us/step




.. parsed-literal::

    [0.001410347225859916, 0.030117209498032566]



MLP Prediction
~~~~~~~~~~~~~~

| ì´ì œ í•™ìŠµë„ í–ˆê³ , ì‹¤ì „í‰ê°€ë„ ì¹˜ë¤˜ë‹¤.
| í•˜ì§€ë§Œ ìˆ˜ëŠ¥ì„ ì˜ í‘¸ëŠ” ê²ƒê³¼ ì¼ì„ ì˜ í•˜ëŠ” ê±´ ë‹¤ë¥¸ ì´ì•¼ê¸°ì´ë‹¤. ì§„ì •í•œ
  ì˜ë¯¸ì˜ ì‹¤ì „ì— íˆ¬ì…í•´ ë³´ì.

**X** Case 1ê°œë¥¼ ê³¨ë¼ **Y**\ ê°’ì„ ì˜ˆì¸¡í•´ ë³´ì.

.. container:: alert alert-block alert-danger

   | Tip:
   | 

   .. code:: py

      model.predict(test_x_mlp[0])

   ìœ„ì˜ ì½”ë“œëŠ” ëŒì•„ê°€ì§€ ì•ŠëŠ”ë‹¤. ì™œì¼ê¹Œ?

   ::

      ValueError: Error when checking input: expected input_2 to have shape (3,) but got array with shape (1,)

   | ê·¸ë™ì•ˆ ì‚¬ìš©í–ˆë˜ Inputì€ ``(N, 3)`` í˜•íƒœì˜ 2ì°¨ì› arrayì˜€ë‹¤.
   | 1ê°œë§Œ ì„ íƒí•´ ë²„ë¦¬ë‹ˆ ``N``\ ì— í•´ë‹¹í•˜ëŠ” ì°¨ì›ì„ ìƒëµí•˜ê³  ``3``\ ë§Œ
     ë‚¨ì€ ê²ƒì´ë‹¤.
   | 2ì°¨ì›ì„ ìœ ì§€í•˜ë©´ì„œ 1ê°œë§Œ ì„ íƒí•˜ë ¤ë©´ ì–´ë–»ê²Œ ë„£ì–´ì•¼ í• ê¹Œ?

.. code:: ipython3

    model.predict(test_x_mlp[:1])




.. parsed-literal::

    array([[0.8122951 , 0.63150454]], dtype=float32)



ì‹¤ì œ ë‚˜ì™€ì•¼ í•˜ëŠ” ê°’ì€:

.. code:: ipython3

    test_y_mlp[:1]




.. parsed-literal::

    array([[0.8277778, 0.6791667]], dtype=float32)



.. code:: ipython3

    keras_predict_plot(model, test_x_mlp, test_y_mlp, method='mlp')


.. parsed-literal::

    /opt/conda/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:457: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure
      "matplotlib is currently using a non-GUI backend, "



.. image:: output_69_1.png


(ê°œë…) MLP: Multi Layer Perceptron
----------------------------------

 ì´ì œ Layerë¥¼ í–‰ë ¬ë¡œ í‘œí˜„í•´ ë³´ì.
 :math:`\space`

.. raw:: html

   <center>

ë³µì¡í•´ ë³´ì—¬ë„ ë‹¨ì§€ í–‰ë ¬ì—°ì‚°ì˜ ì‘ìš©ì¼ ë¿!

.. raw:: html

   </center>

:math:`\space` :raw-latex:`\begin{equation}
\overbrace{ \begin{bmatrix} \color{red}3 \end{bmatrix} }^{INPUT}
\overbrace{ \begin{bmatrix} \color{red}3 \times \color{blue}4 \end{bmatrix} }^{1st \space Layer} 
\overbrace{ \begin{bmatrix} \color{blue}4 \times 2 \end{bmatrix} }^{2nd \space Layer} 
= \begin{bmatrix} 2 \end{bmatrix}
\end{equation}` :math:`\space`

:raw-latex:`\begin{equation}
\overbrace{ \begin{bmatrix} Batch \times \color{red}3 \end{bmatrix} }^{INPUT}
\overbrace{ \begin{bmatrix} \color{red}3 \times \color{blue}4 \end{bmatrix} }^{1st \space Layer} 
\overbrace{ \begin{bmatrix} \color{blue}4 \times 2 \end{bmatrix} }^{2nd \space Layer} 
= \begin{bmatrix} Batch \times 2 \end{bmatrix}
\end{equation}`
:math:`\space`

.. container:: alert alert-block alert-warning

   | \ :math:`\divideontimes` í–‰ë ¬ê³¼ ì‚¬ìƒ: :math:`[m \times n]`\ ì€
     :math:`n` ì°¨ì› ê³µê°„ì—ì„œ :math:`m` ì°¨ì› ê³µê°„ìœ¼ë¡œì˜ mapping (
     :math:`L:N \rightarrow M` )
   | :math:`\space` :math:`\space` \ 

     .. math:: \underbrace{Y}_{ [m] } = \underbrace{A}_{ [m \times n] } \underbrace{X}_{ [n] }

     \ 

:math:`m` ì°¨ì›ì—ì„œ :math:`n` ì°¨ì›ìœ¼ë¡œì˜ ì‚¬ìƒì€, \ 

.. math:: \underbrace{Y}_{ [n] } = \underbrace{X}_{ [m] } \underbrace{A}_{ [m \times n] }

\ 

.. container::

   \* :math:`y=ax`\ ì˜ ìˆœì„œì™€ ë‹¤ë¥¸ ì ì— ì£¼ì˜.

Input
~~~~~

|  :math:`m` ì°¨ì›ì—ì„œ :math:`n` ì°¨ì›ìœ¼ë¡œì˜ ì‚¬ìƒ = í–‰ë ¬ = Layerì˜ ì˜ë¯¸ë¥¼
  ì•Œì•˜ìœ¼ë‹ˆ, ì´ì œëŠ” ``Input`` :math:`m` ê³¼ ``Output`` :math:`n` ì— ëŒ€í•´
  ì•Œì•„ë³´ì. ìš°ë¦¬ì˜ ì²« Layer \ :math:`A`\  ëŠ” í•œ ë²ˆì—
  :math:`\color{red}3`\ ì°¨ì›ì˜ Data :math:`X`\ ë¥¼ ì†Œí™”í•˜ëŠ” í–‰ë ¬ì´ë‹¤.
| í•˜ì§€ë§Œ, :math:`[2 \times \color{red}3]` ì²˜ëŸ¼ í–‰ë ¬ì˜ í˜•íƒœë¡œ í•œ ë²ˆì—
  ë³‘ë ¬ì²˜ë¦¬ í•  ìˆ˜ë„ ìˆë‹¤.

ì´ ë•Œì˜ Data ê°œìˆ˜ëŠ” ``Output`` ê¹Œì§€ ìœ ì§€ëœë‹¤.

| :math:`\space` :raw-latex:`\begin{equation}
  \overbrace{ \begin{bmatrix} Batch \times \color{red}3 \end{bmatrix} }^{INPUT}
  \begin{bmatrix} \color{red}3 \times \color{blue}4 \end{bmatrix}
  \begin{bmatrix} \color{blue}4 \times 2 \end{bmatrix}
  = \overbrace{ \begin{bmatrix} Batch \times 2 \end{bmatrix} }^{OUTPUT}
  \end{equation}`
| :math:`\space`

\ 

.. math:: \underbrace{Y}_{ [B \times \color{blue}n] } = \underbrace{X}_{ [B \times \color{red}m] } \underbrace{A}_{ [\color{red}m \times \color{blue}n] }

\ 

 ì´ì œ ìš°ë¦¬ëŠ” ì—´ë²¡í„° 1ê°œì”©ì„ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, Batchë¡œ ì—¬ëŸ¬ Inputì„
í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤

.. container:: alert alert-block alert-warning

   \ :math:`\divideontimes` Inputì˜ í˜•íƒœ: :math:`[2 \times 3]`\ ì€
   :math:`3`\ ì°¨ì›ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„° :math:`2`\ ê°œë¥¼ ì˜ë¯¸í•œë‹¤. \ 

   .. math:: [2 \times 3]=[Data \space ê°œìˆ˜, Data \space ì°¨ì›]

   \ 

.. code:: ipython3

    arr_x = train_x_mlp[:2].round(2)
    arr_x1 = train_x_mlp[:1].round(2)
    
    aprint(arr_x, arr_x1, name_list=['arr_x', 'arr_x_one'])


.. parsed-literal::

    ================================================
    |  arr_x               |   arr_x_one           |
    |  (2, 3)              |   (1, 3)              |
    ================================================
    |  [[0.93 0.65 0.03]   |   [[0.93 0.65 0.03]]  |
    |   [0.93 0.56 0.02]]  |                       |
    ================================================


Output
~~~~~~

.. code:: ipython3

    arr_y = train_y_mlp[:2].round(2)
    arr_y1 = train_y_mlp[:1].round(2)
    
    aprint(arr_y, arr_y1, name_list=['arr_y', 'arr_y_one'])


.. parsed-literal::

    ======================================
    |  arr_y          |   arr_y_one      |
    |  (2, 2)         |   (1, 2)         |
    ======================================
    |  [[0.93 0.56]   |   [[0.93 0.56]]  |
    |   [0.94 0.62]]  |                  |
    ======================================


Layers
~~~~~~

|  ì´ì œ Dataë„ í–‰ë ¬ë¡œ í‘œí˜„í•´ ë³´ì.
|  ì²«ì§¸ LayerëŠ” :math:`\color{red}3`\ ì°¨ì›ì˜ Data :math:`X`\ ë¥¼
  :math:`\color{blue}4`\ ì°¨ì›ìœ¼ë¡œ í™•ì¥í•˜ê³ ,
| ë‘˜ì§¸ LayerëŠ” :math:`\color{blue}4`\ ì°¨ì›ì„
  :math:`\color{black}2`\ ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ê²Œ ëœë‹¤.

| :math:`\space` :raw-latex:`\begin{equation}
  \begin{bmatrix} Batch \times \color{red}3 \end{bmatrix}
  \overbrace{ \begin{bmatrix} \color{red}3 \times \color{blue}4 \end{bmatrix} }^{1st \space Layer}
  \overbrace{ \begin{bmatrix} \color{blue}4 \times 2 \end{bmatrix} }^{2nd \space Layer}
  = \begin{bmatrix} Batch \times 2 \end{bmatrix}
  \end{equation}`
| :math:`\space`

\ 

.. math:: \underbrace{Y}_{ [B \times \color{blue}n] } = \underbrace{X}_{ [B \times \color{red}m] } \underbrace{A}_{ [\color{red}m \times \color{blue}n] }

\ 

|  ì´ ê¸°ë³¸ Layerì˜ í˜•íƒœë¥¼ ``Fully Connected Layer`` ë˜ëŠ”
  ``Dense Layer``\ ë¼ê³  ë¶€ë¥¸ë‹¤.
| Layerë¥¼ êµ¬ì„±í•˜ê¸° ìœ„í•´ì„œëŠ” :math:`\color{red}m`\ ê³¼
  :math:`\color{blue}n`\ ì„ ì •í•´ ì£¼ì–´ì•¼ í•œë‹¤.
| ê°’ì„ ì •í•˜ê³  ë‚˜ë©´, ì„ì˜ì˜ ì´ˆê¸°ê°’ìœ¼ë¡œ í–‰ë ¬ì„ ë§Œë“¤ì–´ Layerë¥¼ ì²˜ìŒ
  êµ¬ì„±í•˜ê²Œ ëœë‹¤.

ì´ ë•Œ, :math:`\color{blue}n`\ ì— í•´ë‹¹í•˜ëŠ” $
:raw-latex:`\color{blue}`4\ :math:`ì°¨ì›ì„ Hidden Layerì˜ ì°¨ì›ì´ë¼ê³  í•œë‹¤. ì¦‰ LayerëŠ” <b>`\ m$
ì°¨ì› Inputìœ¼ë¡œ :math:`n` ì°¨ì› Outputì„ ë§Œë“¤ì–´ ë‚´ëŠ” í–‰ë ¬ì´ë¼ê³  í•  ìˆ˜
ìˆë‹¤. ì´ì œ ìš°ë¦¬ëŠ”, ì ì´ ì•„ë‹Œ ì„ ì„ Layerë¡œ ë³¼ ìˆ˜ ìˆë‹¤.

.. container:: alert alert-block alert-warning

   \ :math:`\divideontimes` Layerì˜ í˜•íƒœ: \ :math:`m` ì°¨ì› Inputìœ¼ë¡œ
   :math:`n` ì°¨ì› Outputì„ ë§Œë“¤ì–´ ë‚´ëŠ” í–‰ë ¬

    \ 

   .. math:: [3 \times 4]=[Input \space Data \space ì°¨ì›, Hidden \space ì°¨ì›]

   \ 

ê¸°ë³¸ LayerëŠ” ``Fully Connected Layer`` ë˜ëŠ” ``Dense Layer``\ ì´ë©°,
``input``\ ê³¼ ``output shape``\ ë¥¼ ë³€ìˆ˜ë¡œ ë°›ì•„ ìƒì„±.

-  Input Shape : :math:`m`\ 
-  Output Shape : :math:`l`\ 

 :raw-latex:`\begin{equation}
\begin{bmatrix} n\times m \end{bmatrix}
\overbrace{ \begin{bmatrix} m\times l \end{bmatrix} }^{Layer} 
= \begin{bmatrix} n\times l \end{bmatrix}
\end{equation}`

:math:`\space` :math:`\space`

 :raw-latex:`\begin{equation}
\left({\begin{array}{cc} \color{red}a & \color{red}b & \color{red}c\\\color{red}d & \color{red}e & \color{red}f \end{array}}\right)
\overbrace{ \left( \begin{array}{cc} \color{blue}  A & \color{blue} D & \color{blue} G\\ \color{blue} B & \color{blue} E & \color{blue} H \\ \color{blue} C & \color{blue} F & \color{blue} I \end{array} \right) }^{Layer}
= \left(\begin{array}{cc} 
\color{red}a \color{blue}A + \color{red}b \color{blue}B + \color{red}c \color{blue}C & 
\color{red}a \color{blue}D + \color{red}b \color{blue}E + \color{red}c \color{blue}F &
\color{red}a \color{blue}G + \color{red}b \color{blue}H + \color{red}c \color{blue}I
\\
\color{red}d \color{blue}A + \color{red}e \color{blue}B + \color{red}f \color{blue}C & 
\color{red}d \color{blue}D + \color{red}e \color{blue}E + \color{red}f \color{blue}F & 
\color{red}d \color{blue}G + \color{red}e \color{blue}H + \color{red}f \color{blue}I
\\
\end{array}\right)
\end{equation}`

Dense Layer (Fully Connected Layer)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code:: ipython3

    def dense_layer(
        input_x,
        output_dim=None,
        name=None,
        seed=1,
        ):
        input_dim = input_x.shape[-1]
        np.random.seed(seed)
        
        
        #==== Essential ========================================================#
    
        weight = np.random.random((input_dim, output_dim)).round(2)
    
        output = input_x @ weight
        
        #=======================================================================#
        
        
        print(name)
        aprint(input_x, weight, output, name_list=['Input', 'Weight', 'Output'])
    
        return output, weight

.. code:: ipython3

    dense_layer(arr_x, output_dim=4)


.. parsed-literal::

    None
    ==========================================================================================================
    |  Input               |   Weight                   |   Output                                           |
    |  (2, 3)              |   (3, 4)                   |   (2, 4)                                           |
    ==========================================================================================================
    |  [[0.93 0.65 0.03]   |   [[0.42 0.72 0.   0.3 ]   |   [[0.5001     0.7443     0.1361     0.52719999]   |
    |   [0.93 0.56 0.02]]  |    [0.15 0.09 0.19 0.35]   |    [0.4826     0.73080001 0.1148     0.4888    ]]  |
    |                      |    [0.4  0.54 0.42 0.69]]  |                                                    |
    ==========================================================================================================




.. parsed-literal::

    (array([[0.5001    , 0.7443    , 0.1361    , 0.52719999],
            [0.4826    , 0.73080001, 0.1148    , 0.4888    ]]),
     array([[0.42, 0.72, 0.  , 0.3 ],
            [0.15, 0.09, 0.19, 0.35],
            [0.4 , 0.54, 0.42, 0.69]]))



Initialization
^^^^^^^^^^^^^^

| Layerë¥¼ ìƒì„±í•œë‹¤ëŠ” ê²ƒì€ í–‰ë ¬ì„ ìƒì„±í•œë‹¤ëŠ” ê²ƒê³¼ ê°™ë‹¤.
| ì´ ë•Œ ì„ì˜ì˜ ê°’ì„ ì´ìš©í•´ í–‰ë ¬ì„ ìƒì„±í•˜ì—¬ì•¼ í•˜ëŠ”ë°, ì´ ì´ˆê¸°ê°’ì„
  ë¶€ì—¬í•˜ëŠ” ê³¼ì •ì„ ``Initialization`` ì´ë¼ê³  í•¨.

.. code:: ipython3

    np.random.seed(1)
    np.random.random((3, 4)).round(2)




.. parsed-literal::

    array([[0.42, 0.72, 0.  , 0.3 ],
           [0.15, 0.09, 0.19, 0.35],
           [0.4 , 0.54, 0.42, 0.69]])



Activation Function
~~~~~~~~~~~~~~~~~~~

Layer ê°„ì˜ ì„ í˜• ê²°í•©ì€ ì„ í˜• í•¨ìˆ˜(1ì°¨ í•¨ìˆ˜)ì˜ í˜•íƒœë¡œë§Œ ì¡´ì¬í•œë‹¤.

:raw-latex:`\begin{equation}
y = f(x) = ax,\\  
z = g(y) = by,\\
 \\
z = g(f(x)) = b(ax) = abx
\end{equation}`

:math:`c = ab` ë¼ê³  í•˜ë©´ \ 

.. math:: z = h(x) = cx

\ 

| ê°€ ë˜ì–´ ì‚¬ì‹¤ìƒ 2ê°œë¥¼ ê²¹ì¹œ íš¨ê³¼ê°€ ì—†ì–´ì§„ë‹¤.
| ê·¸ëŸ¬ë¯€ë¡œ, Layer ì‚¬ì´ì— ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ë¼ì›Œ ë„£ì–´ :math:`y=f(x)` ë¥¼
  ë¹„ì„ í˜• ê´€ê³„ë¡œ ë§Œë“¤ê²Œ ëœë‹¤.

ë˜í•œ, Output ê°’ì˜ ë²”ìœ„ë¥¼ :math:`[0, 1]` ì‚¬ì´ë¡œ ì œí•œí•˜ëŠ” íš¨ê³¼ë„ ìˆë‹¤.
(``sigmoid``)

Activations & Its Derivatives
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code:: ipython3

    examples.activation_plot()


.. parsed-literal::

    /opt/conda/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:457: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure
      "matplotlib is currently using a non-GUI backend, "



.. image:: output_86_1.png


| ``sigmoid`` í•¨ìˆ˜ëŠ” ``tanh``\ ì— ë¹„í•´ ê¸°ìš¸ê¸°ê°€ ì‘ë‹¤.
| ë¯¸ë¶„ê°’ì˜ ìµœëŒ€ì¹˜ê°€ 0.25ì´ë©°, ë¯¸ë¶„ê°’ì´ 1ì¸ ``tanh`` í•¨ìˆ˜ì— ë¹„í•´
  :math:`\frac {1}{4}` ìˆ˜ì¤€ì´ë‹¤.

 ì´ ê¸°ìš¸ê¸°ëŠ” :math:`x`\ ê°’ì˜ ì°¨ì´ì— ë¹„í•´ :math:`y`\ ê°’ì˜ ì°¨ì´ê°€ ì‘ë‹¤ëŠ”
ë§ì´ë©°, í•™ìŠµ ì§„ë„ê°€ ëŠë¦° ì›ì¸ì´ ëœë‹¤.

ë•Œë¬¸ì—, ë¯¸ë¶„ê°’ì´ ì»¤ì„œ í•™ìŠµ Impactê°€ í° ``tanh``\ ë‚˜ ``relu``\ ê°€
ë“±ì¥í•˜ê²Œ ëœë‹¤.

.. code:: ipython3

    examples.d_activation_plot()


.. parsed-literal::

    /opt/conda/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:457: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure
      "matplotlib is currently using a non-GUI backend, "



.. image:: output_88_1.png


Biases
^^^^^^

.. container:: alert alert-block alert-success

   | \ :math:`\divideontimes` ``bias``\ ê°€ ê°–ëŠ” ì˜ë¯¸ 
   | 

   | ``weight``\ ê°€ :math:`x` ê°„ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•œë‹¤ë©´, ``bias``\ ëŠ”
     ì˜ì ì¡°ì ˆì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.
   | ``bias``\ ê°€ ì—†ìœ¼ë©´, :math:`x`\ ì˜ ì°¨ì›ì— ê´€ê³„ì—†ì´ ëª¨ë“  í•¨ìˆ˜ëŠ” ì›ì 
     \ :math:`(0, 0)`\  ì„ ì§€ë‚˜ê²Œ ë˜ì–´ ë°ì´í„° í•™ìŠµì— ì§€ì¥ì„ ì´ˆë˜í•  ìˆ˜
     ìˆë‹¤.

.. figure:: images/bias_1.png
   :alt: mifeel_gunfeel

   mifeel_gunfeel

--------------

-  :math:`y = 2x`

.. code:: ipython3

    examples.draw_without_bias()



.. image:: output_91_0.png


--------------

-  :math:`y=2x-4`

.. code:: ipython3

    examples.draw_with_bias()



.. image:: output_93_0.png


Complete Network
~~~~~~~~~~~~~~~~

``Input``\ ê³¼ ``Layer``, ``Activation``\ ì„ í™œìš©í•˜ì—¬ ê°„ë‹¨í•œ
``Multi Layer Perceptron`` ëª¨ë¸ì„ ìƒì„±í•´ ë³´ì.

.. code:: ipython3

    # Input Layer
    input_layer = arr_x1
    
    
    # Hidden Layer
    hidden_layer_1, w1 = dense_layer(
        input_layer,
        output_dim=4,
        seed=4,
        name='fc1_layer',
    )
    activated_1 = sigmoid(hidden_layer_1)
    
    
    # Output Layer
    hidden_layer_2, w2 = dense_layer(
        hidden_layer_1,
        output_dim=2,
        seed=3,
        name='fc2_layer',
    )
    activated_2 = sigmoid(hidden_layer_2)


.. parsed-literal::

    fc1_layer
    ==========================================================================================================
    |  Input               |   Weight                   |   Output                                           |
    |  (1, 3)              |   (3, 4)                   |   (1, 4)                                           |
    ==========================================================================================================
    |  [[0.93 0.65 0.03]]  |   [[0.97 0.55 0.97 0.71]   |   [[1.36459999 0.6674     1.56249998 0.6728    ]]  |
    |                      |    [0.7  0.22 0.98 0.01]   |                                                    |
    |                      |    [0.25 0.43 0.78 0.2 ]]  |                                                    |
    ==========================================================================================================
    sigmoid
    ==========================================================================================================
    |  Raw                                              |   Activated                                        |
    |  (1, 4)                                           |   (1, 4)                                           |
    ==========================================================================================================
    |  [[1.36459999 0.6674     1.56249998 0.6728    ]]  |   [[0.7965063  0.66092073 0.82671179 0.66212984]]  |
    ==========================================================================================================
    fc2_layer
    =======================================================================================================
    |  Input                                            |   Weight         |   Output                     |
    |  (1, 4)                                           |   (4, 2)         |   (1, 2)                     |
    =======================================================================================================
    |  [[1.36459999 0.6674     1.56249998 0.6728    ]]  |   [[0.55 0.71]   |   [[2.42216498 2.85677798]]  |
    |                                                   |    [0.29 0.51]   |                              |
    |                                                   |    [0.89 0.9 ]   |                              |
    |                                                   |    [0.13 0.21]]  |                              |
    =======================================================================================================
    sigmoid
    ==============================================================
    |  Raw                        |   Activated                  |
    |  (1, 2)                     |   (1, 2)                     |
    ==============================================================
    |  [[2.42216498 2.85677798]]  |   [[0.91850195 0.94566799]]  |
    ==============================================================


.. code:: ipython3

    hidden_layer_2




.. parsed-literal::

    array([[2.42216498, 2.85677798]])



.. code:: ipython3

    activated_2




.. parsed-literal::

    array([[0.91850195, 0.94566799]])



Batch
~~~~~

 ì´ì œ Batchì— ëŒ€í•´ ì•Œì•„ë³´ì. BatchëŠ” í•œë²ˆ í•™ìŠµì— ì´ìš©í•˜ëŠ” ìƒ˜í”Œ ë‹¨ìœ„ë¥¼
ë§í•œë‹¤. ì „ì²´ Inputì˜ ê°œìˆ˜ê°€ 10ì´ê³  ``batch_size``\ ê°€ 2ì¼ ê²½ìš°,
:math:`\space\space`\  :math:`\leftarrow` input_size: 10, batch_size: 2
ë¶€ë¶„ì§‘í•©ì¸ 2ê°œì”©ì„ êº¼ë‚´ì–´ 1ë²ˆ í•™ìŠµì— í™œìš©í•˜ê²Œ ë˜ê³ 
:math:`\space\space`\  :math:`\leftarrow` step_num: 1 (ëˆ„ì í•™ìŠµíšŸìˆ˜) ì´
5ë²ˆ í•™ìŠµí•˜ë©´ ì „ì²´ Inputì„ ë‹¤ í•™ìŠµí•˜ê²Œ ëœë‹¤. :math:`\space\space`\ 
:math:`\leftarrow` batch_nu: 5 (í•™ìŠµíšŸìˆ˜) ì´ë ‡ê²Œ Inputì„ í•œë²ˆ ë‹¤ ë³´ëŠ”
ê²½ìš°ë¥¼ 1 epochì´ë¼ í•˜ë©°, :math:`\space\space`\  :math:`\leftarrow`
epoch_num: 1

| 10 epochë¡œ í•™ìŠµí•˜ëŠ” ê²½ìš°
| ``batch_num``\ ì€ 5, ``step_num``\ ì€ 50ì´ ëœë‹¤.

|  :math:`\space` :math:`\space 1 \space Epoch,`
  :raw-latex:`\begin{equation}
  \begin{bmatrix} 1st \space Batch \times \color{red}3 \end{bmatrix}
  \overbrace{ \begin{bmatrix} \color{red}3 \times \color{blue}4 \end{bmatrix} }^{1st Layer}
  \overbrace{ \begin{bmatrix} \color{blue}4 \times 2 \end{bmatrix} }^{2nd Layer}
  = \begin{bmatrix} 1st \space Batch \times 2 \end{bmatrix}
  \Rightarrow 1st \space Backpropagation
  \end{equation}`
| :raw-latex:`\begin{equation}
  \begin{bmatrix} 2nd \space Batch \times \color{red}3 \end{bmatrix}
  \overbrace{ \begin{bmatrix} \color{red}3 \times \color{blue}4 \end{bmatrix} }^{1st Layer}
  \overbrace{ \begin{bmatrix} \color{blue}4 \times 2 \end{bmatrix} }^{2nd Layer}
  = \begin{bmatrix} 2nd \space Batch \times 2 \end{bmatrix}
  \Rightarrow 2nd \space Backpropagation
  \end{equation}`
| 

  .. math:: \vdots

   :raw-latex:`\begin{equation}
  \begin{bmatrix} 5th \space Batch \times \color{red}3 \end{bmatrix}
  \overbrace{ \begin{bmatrix} \color{red}3 \times \color{blue}4 \end{bmatrix} }^{1st Layer}
  \overbrace{ \begin{bmatrix} \color{blue}4 \times 2 \end{bmatrix} }^{2nd Layer}
  = \begin{bmatrix} 5th \space Batch \times 2 \end{bmatrix}
  \Rightarrow 5th \space Backpropagation
  \end{equation}` :math:`\space`

í•™ìŠµí•  ë•ŒëŠ” ì£¼ì–´ì§„ ``input_size``\ ì— ëŒ€í•´ ``batch_size``\ ì™€
``epoch_num``\ ì„ ì§€ì •í•œë‹¤. ì´ì œ, ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ Inputì„ ì–´ë–»ê²Œ
ì½ì–´ë“¤ì´ëŠ” ì§€ ì•Œ ìˆ˜ ìˆë‹¤.

.. container:: alert alert-block alert-warning

   \ :math:`\divideontimes` Batchë€: 1ë²ˆ í•™ìŠµì— í™œìš©í•˜ëŠ” Inputì˜
   ë¶€ë¶„ì§‘í•© ë‹¨ìœ„

   .. math:: Batch \space num = \frac {Input \space size}{Batch \space size}

   .. math:: 1 \space Epoch = Batch \space size \times Batch \space num

   | 

     .. math::  Step \space num = Epoch \space num \times Batch \space num
   | 

.. code:: ipython3

    # Input Layer
    input_layer = arr_x1
    
    
    # Hidden Layer
    hidden_layer_1, w1 = dense_layer(
        input_layer,
        output_dim=4,
        seed=4,
        name='fc1_layer',
    )
    activated_1 = sigmoid(hidden_layer_1)
    
    
    # Output Layer
    hidden_layer_2, w2 = dense_layer(
        hidden_layer_1,
        output_dim=2,
        seed=3,
        name='fc2_layer',
    )
    activated_2 = sigmoid(hidden_layer_2)


.. parsed-literal::

    fc1_layer
    ==========================================================================================================
    |  Input               |   Weight                   |   Output                                           |
    |  (1, 3)              |   (3, 4)                   |   (1, 4)                                           |
    ==========================================================================================================
    |  [[0.93 0.65 0.03]]  |   [[0.97 0.55 0.97 0.71]   |   [[1.36459999 0.6674     1.56249998 0.6728    ]]  |
    |                      |    [0.7  0.22 0.98 0.01]   |                                                    |
    |                      |    [0.25 0.43 0.78 0.2 ]]  |                                                    |
    ==========================================================================================================
    sigmoid
    ==========================================================================================================
    |  Raw                                              |   Activated                                        |
    |  (1, 4)                                           |   (1, 4)                                           |
    ==========================================================================================================
    |  [[1.36459999 0.6674     1.56249998 0.6728    ]]  |   [[0.7965063  0.66092073 0.82671179 0.66212984]]  |
    ==========================================================================================================
    fc2_layer
    =======================================================================================================
    |  Input                                            |   Weight         |   Output                     |
    |  (1, 4)                                           |   (4, 2)         |   (1, 2)                     |
    =======================================================================================================
    |  [[1.36459999 0.6674     1.56249998 0.6728    ]]  |   [[0.55 0.71]   |   [[2.42216498 2.85677798]]  |
    |                                                   |    [0.29 0.51]   |                              |
    |                                                   |    [0.89 0.9 ]   |                              |
    |                                                   |    [0.13 0.21]]  |                              |
    =======================================================================================================
    sigmoid
    ==============================================================
    |  Raw                        |   Activated                  |
    |  (1, 2)                     |   (1, 2)                     |
    ==============================================================
    |  [[2.42216498 2.85677798]]  |   [[0.91850195 0.94566799]]  |
    ==============================================================


Loss Function
~~~~~~~~~~~~~

``Cost Function`` í˜¹ì€ ``Objective Function``\ ìœ¼ë¡œë„ ë¶ˆë¦°ë‹¤.
ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ ``Error`` ë˜ëŠ” ``Cost``\ ë¼ê³  ë¶€ë¥´ëŠ”ë° ì´
``Error``\ ì˜ ì¸¡ì •ë°©ë²•ì´ ê³§ ``Loss Function``\ ì´ë‹¤.
â€™ëª¨ë¸ì´ í•™ìŠµí•œë‹¤â€™ëŠ” ì˜ë¯¸ëŠ”, ì´ ê°’ì„ ìµœì†Œí™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ :math:`w` ì™€
:math:`b` ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤ëŠ” ëœ»ì´ë‹¤.

.. container::

   \* ``accuracy``\ ë¥¼ ëª©ì í•¨ìˆ˜ë¡œ ì‚¬ìš©í•  ê²½ìš°, ì—…ë°ì´íŠ¸í•¨ì— ë”°ë¼ ê°’ì´
   ë¶ˆì—°ì†ì ìœ¼ë¡œ ë³€í•˜ê¸° ë•Œë¬¸ì— ë¶€ì í•©.

Mean Square Error
^^^^^^^^^^^^^^^^^

| ì œì¼ ê¸°ë³¸ì ì¸ ``Loss Function``\ ì´ë©°,
| ê°’ì˜ ì°¨ì´ë¥¼ ì œê³±í•˜ì—¬ ê·¸ í¬ê¸° ê°’ì˜ í‰ê· ì„ ê³„ì‚°í•œë‹¤.

 \ 

.. math:: loss = \mathcal{L}(\hat{y}, y) = (\hat y^{(i)} - y^{(i)})^2

\  \ 

.. math:: MSE = \frac {1}{m} \sum_{i=1}^{m}(\hat y_{i} - y_{i})^2 

\ 

.. code:: ipython3

    def mean_squared_error(logits, real):
        
        
        #==== Essential ========================================================#
        
        err = ((logits - real) ** 2).mean()
        
        #=======================================================================#
    
    
        return err

.. code:: ipython3

    mean_squared_error(activated_2, arr_y1)




.. parsed-literal::

    0.0744360007255992



Training
~~~~~~~~

Gradient Descent
^^^^^^^^^^^^^^^^

``Gradient``\ ëŠ” â€˜ê²½ì‚¬ë„â€™ ë¼ëŠ” ëœ»ì´ë©°, ``Gradient Descent``\ ë¥¼ ì‚°ì„
ë‚´ë ¤ê°€ëŠ” ê³¼ì •ì— ë¹„ìœ í•  ìˆ˜ ìˆë‹¤.

ì‚° ì†ì—ì„œ ê¸¸ì„ ìƒì—ˆë‹¤ê³  ìƒê°í•´ ë³´ì.

1. | ê¸¸ì°¾ê¸°
   | ì‚°ì„ ë¹ ì ¸ë‚˜ì˜¤ê¸° ìœ„í•´ì„œëŠ” ì‚°ì˜ ê²½ì‚¬ì™€ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ë‚´ë ¤ì™€ì•¼ í•œë‹¤.
   | ì´ë ‡ê²Œ ê¸¸ì„ ì°¾ëŠ” ë°©ë²•ì´ â€˜Gradient Descentâ€™ ì´ë‹¤.

2. | ì¤‘ê°„ ê³¨ì§œê¸°
   | ì‹œì•¼ê°€ ì¢ê±°ë‚˜ ì§€ë„ê°€ ì¡°ê¸ˆë°–ì— ì—†ì„ ê²½ìš°,
   | ê²½ì‚¬ì— ì˜ì¡´í•´ì„œ ë‚´ë ¤ê°ˆ ìˆ˜ëŠ” ìˆì§€ë§Œ ì´ëŒ€ë¡œ ê°€ë©´ ëª©ì ì§€ê°€ ë‚˜ì˜¬ ì§€
     í™•ì‹ í•  ìˆ˜ ì—†ë‹¤.
   | ê·€í‰ì´ ê³¨ì§œê¸°ì— ë„ì°©í•´ì„œ ì˜ì›íˆ í—¤ë§¬ ìˆ˜ë„ ìˆë‹¤.
   | ì´ ë•Œ ìš°ë¦¬ëŠ” â€˜êµ­ì†Œ ìµœì í•´(Local Minima)â€™ë¥¼ ì°¾ì•˜ë‹¤ê³  í•œë‹¤.

3. | ìµœì¢… ëª©ì ì§€
   | ì‹œì•¼ê°€ ë„“ê±°ë‚˜ ì‚°ê¸¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì˜ ì•Œê³  ìˆëŠ” ê²½ìš°, ë˜ëŠ” ìš´ì´
     ì¢‹ìœ¼ë©´
   | ê°€ì•¼í•  ê³³ì„ ì˜ ì•Œê³  ë•Œë¡œëŠ” ë‹¤ì‹œ ë´‰ìš°ë¦¬ë¥¼ ì˜¤ë¥´ê¸°ë„ í•˜ë©´ì„œ ëª©ì ì§€ì—
     ë„ì°©í•  ìˆ˜ ìˆë‹¤.
   | ì´ ë•Œ ìš°ë¦¬ëŠ” â€˜ì „ì—­ ìµœì í•´(Global Minimum)â€™ë¥¼ ì°¾ì•˜ë‹¤ê³  í•œë‹¤.

4. | ë³´í­
   | ë³´í­ì´ ë„“ìœ¼ë©´, ì„±í¼ì„±í¼ ê°ˆ ìˆ˜ëŠ” ìˆê² ì§€ë§Œ ë„ˆë¬´ ë¹¨ë¼ì„œ ê¸¸ì„ ì§€ë‚˜ì¹  ìˆ˜
     ìˆë‹¤.
   | ë³´í­ì´ ì¢ìœ¼ë©´, ëª©ì ì§€ë¥¼ ì˜ ì•Œì•„ë„ ë„ˆë¬´ ëŠë ¤ì„œ ê°€ë‹¤ ì§€ì¹  ìˆ˜ ìˆë‹¤.
   | ê°€ì•¼í•  ê³³ì„ ì˜ ì•Œê³  ë•Œë¡œëŠ” ë‹¤ì‹œ ë´‰ìš°ë¦¬ë¥¼ ì˜¤ë¥´ê¸°ë„ í•˜ë©´ì„œ ëª©ì ì§€ì—
     ë„ì°©í•  ìˆ˜ ìˆë‹¤.
   | ì´ ë³´í­ì´ â€˜í•™ìŠµë¥ (Learning Rate)â€™ì´ë‹¤.

.. code:: ipython3

    examples.gradient_plot_a()
    examples.gradient_plot_b()


.. parsed-literal::

    /opt/conda/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:457: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure
      "matplotlib is currently using a non-GUI backend, "
    /opt/conda/envs/tf-py36/lib/python3.6/site-packages/matplotlib/figure.py:457: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure
      "matplotlib is currently using a non-GUI backend, "



.. image:: output_109_1.png



.. image:: output_109_2.png


Forward-propagation
^^^^^^^^^^^^^^^^^^^

-  Dense Layer

-  Activation Layer

Forward-propagation
'''''''''''''''''''

.. code:: ipython3

    from src.layers import dense_layer, sigmoid, tanh
    
    
    def forward_propagation(
        input_x,
        weight_dict=None,
        print_ok=True,
        ):
    
        if weight_dict is None:
            weight_dict = {
                'w1': None,
                'b1': None,
                'w2': None,
                'b2': None,
            }
        
        w1 = weight_dict['w1']
        b1 = weight_dict['b1']
        w2 = weight_dict['w2']
        b2 = weight_dict['b2']
    
        
        #==== Essential ========================================================#
    
        ##### LAYER (Forward-propagation) #####
    
        # Input Layer
        input_layer = input_x
    
        # Hidden Layer
        hidden_layer_1, w1, b1 = dense_layer(
            input_layer,
            output_dim=4,
            weight=w1,
            bias=b1,
            seed=4,
            name='fc1_layer',
            print_ok=print_ok,
        )
        activated_1 = sigmoid(hidden_layer_1, print_ok=print_ok)
    
        # Output Layer
        hidden_layer_2, w2, b2 = dense_layer(
            hidden_layer_1,
            output_dim=2,
            weight=w2,
            bias=b2,
            seed=3,
            name='fc2_layer',
            print_ok=print_ok,
        )
        activated_2 = sigmoid(hidden_layer_2, print_ok=print_ok)
        
        #=======================================================================#
        
        
        if print_ok:
            print(f'\n= Input =\n{input_layer}')
            print(f'\n= 1 Layer =\n{hidden_layer_1}\n{activated_1}')
            print(f'\n= 2 Layer =\n{hidden_layer_2}')
            print(f'\n= Output =\n{activated_2}')
    
    
        weight_dict['w1'] = w1
        weight_dict['b1'] = b1
        weight_dict['l1'] = hidden_layer_1
        weight_dict['a1'] = activated_1
        
        
        weight_dict['w2'] = w2
        weight_dict['b2'] = b2
        weight_dict['l2'] = hidden_layer_2
        weight_dict['a2'] = activated_2
    
        return activated_2, weight_dict

.. code:: ipython3

    output, w_dict = forward_propagation(arr_x1)


.. parsed-literal::

    fc1_layer:	(1, 3) -> (1, 4)
    sigmoid
    ==========================================================================================================
    |  Raw                                              |   Activated                                        |
    |  (1, 4)                                           |   (1, 4)                                           |
    ==========================================================================================================
    |  [[2.22459999 1.6474     1.72249998 1.2728    ]]  |   [[0.90243695 0.83853934 0.84845057 0.78122169]]  |
    ==========================================================================================================
    fc2_layer:	(1, 4) -> (1, 2)
    sigmoid
    ==============================================================
    |  Raw                        |   Activated                  |
    |  (1, 2)                     |   (1, 2)                     |
    ==============================================================
    |  [[3.44976498 4.67717798]]  |   [[0.96922413 0.99078055]]  |
    ==============================================================
    
    = Input =
    [[0.93 0.65 0.03]]
    
    = 1 Layer =
    [[2.22459999 1.6474     1.72249998 1.2728    ]]
    [[0.90243695 0.83853934 0.84845057 0.78122169]]
    
    = 2 Layer =
    [[3.44976498 4.67717798]]
    
    = Output =
    [[0.96922413 0.99078055]]


Backpropagation
^^^^^^^^^^^^^^^

.. container:: alert alert-block alert-warning

   \ :math:`\divideontimes` ì—­ì „íŒŒ(Backpropagation):

   .. raw:: html

      <center>

   ë§ˆì§€ë§‰ Outputê³¼ ì‹¤ì œ ê°’ê³¼ì˜ ì°¨ì´( :math:`Loss` )ë¡œ ë°°ìš´ ê²½í—˜ì„
   ë‚´ë¶€ :math:`Weight`\ ì— ê³¨ê³ ë£¨ Feedbackí•˜ëŠ” ê³¼ì •.

   .. raw:: html

      </center>

.. container:: alert alert-block alert-success

   \ :math:`\divideontimes` Chain Rule: í•©ì„±í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜ì— ëŒ€í•œ
   ê³µì‹ì´ë‹¤.

   .. math::


      z = g(f((x)) \space ì¼ \space ë•Œ \\
      \space \\
      \frac{\partial z}{\partial x} = \frac{\partial z}{\partial y}\frac{\partial y}{\partial x}

   | ëª¨ë¸ì€ Layerë“¤ ê°„ì˜ í•©ì„±ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.
   | ì¦‰, ëª¨ë¸ ì „ì²´ë¥¼ ë¯¸ë¶„í•œ ê°’ì€ Layer ê°ê° ë¯¸ë¶„ê°’ì˜ ê³±ê³¼ ê°™ë‹¤.

   .. math::


      \frac{\partial z}{\partial w} = \frac{\partial z}{\partial y}\frac{\partial y}{\partial x}\frac{\partial x}{\partial w}

 \ 

.. math::  \frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}

\ 

| :math:`J`\ ëŠ” :math:`Error`\ ë¥¼, :math:`\theta`\ ëŠ” :math:`Weight`\ ë¥¼
  ì˜ë¯¸í•œë‹¤.
| ì¦‰, \ :math:`\frac{\partial J}{\partial \theta}`\ ëŠ” :math:`Weight`
  ë³€í™”ëŸ‰ ëŒ€ë¹„ :math:`Error` ê°’ì˜ ë³€í™”ëŸ‰ì„ ì˜ë¯¸í•œë‹¤.

.. math::


   J = MSE = \frac{1}{m}\sum\limits_{i=1}^m {(\hat{y} - y)^2}

.. raw:: html

   <div class="alert alert-block alert-success">

\ :math:`\divideontimes` Derivative of Sigmoid ë¹„ì„ í˜•í•¨ìˆ˜ë¡œ â€˜sigmoidâ€™ê°€
ìì£¼ ì“°ì´ëŠ” ì´ìœ  ì¤‘ í•˜ë‚˜ëŠ” ë„í•¨ìˆ˜ê°€ ê°„ë‹¨í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

.. raw:: latex

   \begin{array}{ll} \hline
   Name
   &
   Formula
   &
   Derivative
   \\ \hline
   sigmoid
   &
   \sigma(x) = \dfrac{1}{1+e^{-x}}
   &
   \sigma(x)(1 - \sigma(x))
   \\ \hline
   tanh
   &
   2\sigma(2x) - 1
   &
   1-\tanh(x)^2
   \\ \hline
   \end{array}

.. code:: ipython3

    def d_sigmoid(y):
        return y * (1 - y)

Backpropagation (Scratch)
'''''''''''''''''''''''''

.. code:: ipython3

    from src.layers import d_sigmoid, d_tanh
    
    
    def backward_propagation_scratch(
        input_x,
        input_y,
        weight_dict=None,
        learning_rate=5e-2,
        print_ok=True,
        ):
        
        m = input_dim = input_x.shape[-1]
    
        w1 = weight_dict['w1']
        b1 = weight_dict['b1']
        l1 = weight_dict['l1']
        a1 = weight_dict['a1']
        
        w2 = weight_dict['w2']
        b2 = weight_dict['b2']
        l2 = weight_dict['l2']
        a2 = weight_dict['a2']
        
        
        #==== Essential ========================================================#
    
        ##### LAYER (Back-propagation) #####
    
        # Calculate Gradients
        err = a2 - input_y
        d_a2 = err / d_sigmoid(a2)
        d_l2 = a2 - input_y
        d_b2 = 1./m * np.sum(d_l2.T, axis=1, keepdims=False)
        d_o2 = d_l2
    
        d_w2 = 1./m * (a1.T @ d_o2)
        d_a1 = d_o2 @ w2.T
        d_l1 = d_sigmoid(a1)
        d_b1 = 1./m * np.sum(d_l1.T, axis=1, keepdims=False)
        d_o1 = d_l1
        d_w1 = 1./m * (input_x.T @ d_o1)
    
        
        # Update
        new_w1 = w1 - (learning_rate * d_w1)
        new_b1 = b1 - (learning_rate * d_b1)
        new_w2 = w2 - (learning_rate * d_w2)
        new_b2 = b2 - (learning_rate * d_b2)
    
        #=======================================================================#
    
        
        if print_ok:
            print('\n\nResult: Backpropagation')
            aprint(w2, new_w2, name_list=['w2', 'new_w2'])
            aprint(b2, new_b2, name_list=['b2', 'new_b2'])
            aprint(w1, new_w1, name_list=['w1', 'new_w1'])
            aprint(b1, new_b1, name_list=['b1', 'new_b1'])
    
        w1 = new_w1
        b1 = new_b1
        w2 = new_w2
        b2 = new_b2
    
    
        gradient_dict = {
            'd_w1': d_w1,
            'd_b1': d_b1,
            'd_l1': d_l1,
            'd_a1': d_a1,
            
            'd_w2': d_w2,
            'd_b2': d_b2,
            'd_l2': d_l2,
            'd_a2': d_a2,
        }
    
        weight_dict['w1'] = new_w1
        weight_dict['b1'] = new_b1
        
        weight_dict['w2'] = new_w2
        weight_dict['b2'] = new_b2
        
        return gradient_dict, weight_dict

.. code:: ipython3

    output, w_dict = forward_propagation(arr_x1)
    grad_dict, new_w_dict = backward_propagation_scratch(arr_x1, arr_y1, weight_dict=w_dict)


.. parsed-literal::

    fc1_layer:	(1, 3) -> (1, 4)
    sigmoid
    ==========================================================================================================
    |  Raw                                              |   Activated                                        |
    |  (1, 4)                                           |   (1, 4)                                           |
    ==========================================================================================================
    |  [[2.22459999 1.6474     1.72249998 1.2728    ]]  |   [[0.90243695 0.83853934 0.84845057 0.78122169]]  |
    ==========================================================================================================
    fc2_layer:	(1, 4) -> (1, 2)
    sigmoid
    ==============================================================
    |  Raw                        |   Activated                  |
    |  (1, 2)                     |   (1, 2)                     |
    ==============================================================
    |  [[3.44976498 4.67717798]]  |   [[0.96922413 0.99078055]]  |
    ==============================================================
    
    = Input =
    [[0.93 0.65 0.03]]
    
    = 1 Layer =
    [[2.22459999 1.6474     1.72249998 1.2728    ]]
    [[0.90243695 0.83853934 0.84845057 0.78122169]]
    
    = 2 Layer =
    [[3.44976498 4.67717798]]
    
    = Output =
    [[0.96922413 0.99078055]]
    
    
    Result: Backpropagation
    ==================================================
    |  w2             |   new_w2                     |
    |  (4, 2)         |   (4, 2)                     |
    ==================================================
    |  [[0.55 0.71]   |   [[0.54941005 0.7035208 ]   |
    |   [0.29 0.51]   |    [0.28945182 0.50397956]   |
    |   [0.89 0.9 ]   |    [0.88944534 0.8939084 ]   |
    |   [0.13 0.21]]  |    [0.12948929 0.20439108]]  |
    ==================================================
    ==============================================
    |  b2           |   new_b2                   |
    |  (2,)         |   (2,)                     |
    ==============================================
    |  [0.05 0.44]  |   [0.04934626 0.43282032]  |
    ==============================================
    ==================================================================================
    |  w1                       |   new_w1                                           |
    |  (3, 4)                   |   (3, 4)                                           |
    ==================================================================================
    |  [[0.97 0.55 0.97 0.71]   |   [[0.96681802 0.54673363 0.9667465  0.70666091]   |
    |   [0.7  0.22 0.98 0.01]   |    [0.69777604 0.21771705 0.97772605 0.00766623]   |
    |   [0.25 0.43 0.78 0.2 ]]  |    [0.24989736 0.42989463 0.77989505 0.19989229]]  |
    ==================================================================================
    ==============================================================================
    |  b1                     |   new_b1                                         |
    |  (4,)                   |   (4,)                                           |
    ==============================================================================
    |  [0.86 0.98 0.16 0.6 ]  |   [0.85657852 0.97648777 0.15650161 0.59640958]  |
    ==============================================================================


Optimization
^^^^^^^^^^^^

.. container:: alert alert-block alert-warning

   \ :math:`\divideontimes` Optimizer: ì§€ë¦„ê¸¸ì„ ì˜ ì°¾ëŠ” ë°©ë²•

   .. raw:: html

      <center>

    ëˆˆì•ì˜ ì´ë“(Local Minima)ì— ë¹ ì§€ì§€ ì•Šê³ , ìµœì ì˜ ë‹µ(Global Minima)ì„
   ì˜ ì°¾ì•„ì•¼ í•œë‹¤!

   .. raw:: html

      </center>

| í•¨ìˆ˜ì˜ ê·¹ëŒ€ê°’ ë˜ëŠ” ê·¹ì†Œê°’ì„ êµ¬í•˜ê¸° ìœ„í•´ í˜„ì¬ ìœ„ì¹˜ì—ì„œ í•¨ìˆ˜ê°’ì˜ ë³€í™”ê°€
  ê°€ì¥ í° ë°©í–¥ìœ¼ë¡œ ì´ë™í•œë‹¤.
| í•¨ìˆ˜ê°’ì˜ ë³€í™”ê°€ ê°€ì¥ í° ë°©í–¥ì„ êµ¬í•  ìˆ˜ë§Œ ìˆë‹¤ë©´ ë‹¤ì–‘í•œ ë¬¸ì œì— ë˜‘ê°™ì€
  ê°œë…ì„ ì ìš©í•  ìˆ˜ ìˆë‹¤.

Exercise : Backprop + Optimizer
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

ì´ì œ ë‹¤ì–‘í•œ Optimizerë¥¼ ì ìš©í•´ì„œ Backpropagationì„ ìˆ˜í–‰í•´ ë³´ì.

.. code:: ipython3

    from src.optimizer import sgd, sgd_momentum, adagrad, rmsprop, adam
    
    
    def backward_propagation(
        input_x,
        input_y,
        weight_dict=None,
        param_dict=None,
        loss_func=mean_squared_error,
        optimizer=sgd,
        print_ok=True,
        ):
    
        if param_dict is None:
            param_dict = {}
        learning_rate = param_dict['learning_rate']
        step_num = param_dict['step_num']
        
        m = input_dim = input_x.shape[-1]
    
        w1 = weight_dict['w1']
        b1 = weight_dict['b1']
        l1 = weight_dict['l1']
        a1 = weight_dict['a1']
        
        w2 = weight_dict['w2']
        b2 = weight_dict['b2']
        l2 = weight_dict['l2']
        a2 = weight_dict['a2']
        
        #==== Essential 1 ======================================================#
    
        ##### LAYER (Back-propagation) #####
    
        # Calculate Gradients
        err = loss_func(a2, input_y)
        d_a2 = err / d_sigmoid(a2)
        d_l2 = a2 - input_y
        d_b2 = 1./m * np.sum(d_l2.T, axis=1, keepdims=False)
        d_o2 = d_l2
    
        d_w2 = 1./m * (a1.T @ d_o2)
        d_a1 = d_o2 @ w2.T
        d_l1 = d_sigmoid(a1)
        d_b1 = 1./m * np.sum(d_l1.T, axis=1, keepdims=False)
        d_o1 = d_l1
        d_w1 = 1./m * (input_x.T @ d_o1)
    
        #=======================================================================#
    
        delta_dict = {
            'w2': d_w2,
            'b2': d_b2,
            'w1': d_w1,
            'b1': d_b1,
        }
    
        step_num += 1
    
        for w_name in delta_dict:
            param_dict[w_name] = {}
            param_dict[w_name]['learning_rate'] = learning_rate
            param_dict[w_name]['step_num'] = step_num
    
    
        #==== Essential 2 ======================================================#
    
        # Update
        new_w_list = []
        for w_name in delta_dict:
            w, dw = w_dict[w_name], delta_dict[w_name]
    
            new_w, param_dict[w_name] = optimizer(w, dw, param_dict=param_dict[w_name])
            new_w_list += [new_w]
    
        new_w2, new_b2, new_w1, new_b1 = new_w_list
    
        #=======================================================================#
        
        
        if print_ok:
            print(f"\n\nResult: Backpropagation {step_num}, Optimizer = '{optimizer.__name__}'")
            aprint(w2, new_w2, name_list=['w2', 'new_w2'])
            aprint(b2, new_b2, name_list=['b2', 'new_b2'])
            aprint(w1, new_w1, name_list=['w1', 'new_w1'])
            aprint(b1, new_b1, name_list=['b1', 'new_b1'])
    
        weight_dict['w1'] = new_w1
        weight_dict['b1'] = new_b1
        
        weight_dict['w2'] = new_w2
        weight_dict['b2'] = new_b2
        
        param_dict['step_num'] = step_num
        param_dict['learning_rate'] = learning_rate
        
    
        return weight_dict, param_dict

Exercise : Training
^^^^^^^^^^^^^^^^^^^

.. code:: ipython3

    #--------------------#
    
    """Parameter Setting.
    """
    
    # OPTIMIZER = sgd
    # OPTIMIZER = sgd_momentum
    OPTIMIZER = adagrad
    # OPTIMIZER = rmsprop
    # OPTIMIZER = adam
    
    MAX_STEP = 100
    LEARNING_RATE = 0.005
    
    #--------------------#
    
    
    print_ok = False
    p_dict = {}
    p_dict['learning_rate'] = LEARNING_RATE
    p_dict['step_num'] = 0
    w_dict = None
    output_list = []
    
    
    #==== Essential ========================================================#
    
    for _ in range(MAX_STEP):
        output, w_dict = forward_propagation(
            arr_x1,
            weight_dict=w_dict,
            print_ok=print_ok,
        )
        w_dict, p_dict = backward_propagation(
            arr_x1,
            arr_y1,
            loss_func=mean_squared_error,
            weight_dict=w_dict,
            param_dict=p_dict,
            optimizer=OPTIMIZER,
            print_ok=print_ok,
        )
        output_list += [output]
        
    #=======================================================================#
    
    
        if _ % (MAX_STEP // 10) >= 9:
            print('Step_num :', p_dict['step_num'])
            aprint(
                output_list[0],
                output,
                arr_y1,
                name_list=['1st OUTPUT', 'LAST OUTPUT', 'GROUND TRUTH'],
                decimals=3,
            )
            print('')
    
    last_output, _ = forward_propagation(
        arr_x1,
        weight_dict=w_dict,
        print_ok=False,
    )


.. parsed-literal::

    Step_num : 10
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.948 0.983]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 20
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.927 0.969]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 30
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.93  0.947]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 40
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.93  0.913]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 50
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.928 0.868]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 60
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.926 0.813]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 70
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.92  0.752]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 80
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.909 0.689]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 90
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.892 0.631]]  |   [[0.93 0.56]]  |
    =============================================================
    
    Step_num : 100
    =============================================================
    |  1st OUTPUT       |   LAST OUTPUT      |   GROUND TRUTH   |
    |  (1, 2)           |   (1, 2)           |   (1, 2)         |
    =============================================================
    |  [[0.969 0.991]]  |   [[0.866 0.582]]  |   [[0.93 0.56]]  |
    =============================================================
    


MLP Summary
~~~~~~~~~~~

RNNìœ¼ë¡œ ë„˜ì–´ê°€ê¸° ì „ì—, MLP ì‘ë™ í”„ë¡œì„¸ìŠ¤ë¥¼ í•œë²ˆ ë” í™•ì¸í•´ ë³´ì.

 :math:`\cdot` ê³„ì‚° íë¦„ : MLP

References
==========

| https://wiseodd.github.io/techblog/2016/06/22/nn-optimization/
| https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21
